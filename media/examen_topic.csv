question,A,B,C,D,E,Unnamed: 6,RIGHT ANSWER,QESTION NUMBER,Explication
Your chemical company needs to manually check documentation for customer order. You use a pull subscription in Pub/Sub so that sales agents get details from the order. You must ensure that you do not process orders twice with different sales agents and that you do not add more complexity to this workflow. What should you do?,Use a Deduplicate PTransform in Dataflow before sending the messages to the sales agents.,Create a transactional database that monitors the pending messages.,Use Pub/Sub exactly-once delivery in your pull subscription.,Create a new Pub/Sub push subscription to monitor the orders processed in the agent's system.,,,C,311,
"You need to look at BigQuery data from a specific table multiple times a day. The underlying table you are querying is several petabytes in size, but you want to filter your data and provide simple aggregations to downstream users. You want to run queries faster and get up-to-date insights quicker. What should you do?",Run a scheduled query to pull the necessary data at specific intervals dally.,Use a cached query to accelerate time to results.,Limit the query columns being pulled in the final result.,Create a materialized view based off of the query being run.,,,D,310,
You work for an airline and you need to store weather data in a BigQuery table. Weather data will be used as input to a machine learning model. The model only uses the last 30 days of weather data. You want to avoid storing unnecessary data and minimize costs. What should you do?,Create a BigQuery table where each record has an ingestion timestamp. Run a scheduled query to delete all the rows with an ingestion timestamp older than 30 days.,Create a BigQuery table partitioned by datetime value of the weather date. Set up partition expiration to 30 days.,Create a BigQuery table partitioned by ingestion time. Set up partition expiration to 30 days.,Create a BigQuery table with a datetime column for the day the weather data refers to. Run a scheduled query to delete rows with a datetime value older than 30 days.,,,B,309,
"You are migrating a large number of files from a public HTTPS endpoint to Cloud Storage. The files are protected from unauthorized access using signed URLs. You created a TSV file that contains the list of object URLs and started a transfer job by using Storage Transfer Service. You notice that the job has run for a long time and eventually failed. Checking the logs of the transfer job reveals that the job was running fine until one point, and then it failed due to HTTP 403 errors on the remaining files. You verified that there were no changes to the source system. You need to fix the problem to resume the migration process. What should you do?","Set up Cloud Storage FUSE, and mount the Cloud Storage bucket on a Compute Engine instance. Remove the completed files from the TSV file. Use a shell script to iterate through the TSV file and download the remaining URLs to the FUSE mount point.",Renew the TLS certificate of the HTTPS endpoint. Remove the completed files from the TSV file and rerun the Storage Transfer Service job.,Create a new TSV file for the remaining files by generating signed URLs with a longer validity period. Split the TSV file into multiple smaller files and submit them as separate Storage Transfer Service jobs in parallel.,Update the file checksums in the TSV file from using MD5 to SHA256. Remove the completed files from the TSV file and rerun the Storage Transfer Service job.,,,C,308,"- It addresses the likely issue: that the signed URLs have expired or are otherwise invalid. By creating a new TSV file with freshly generated signed URLs (with a longer validity period), you're ensuring that the Storage Transfer Service has valid authorization to access the files. - Splitting the TSV file and running parallel jobs might help in managing the workload more efficiently and overcoming any limitations related to the number of files or transfer speed."
You need to connect multiple applications with dynamic public IP addresses to a Cloud SQL instance. You configured users with strong passwords and enforced the SSL connection to your Cloud SQL instance. You want to use Cloud SQL public IP and ensure that you have secured connections. What should you do?,Add CIDR 0.0.0.0/0 network to Authorized Network. Use Identity and Access Management (IAM) to add users.,Add all application networks to Authorized Network and regularly update them.,Leave the Authorized Network empty. Use Cloud SQL Auth proxy on all applications.,Add CIDR 0.0.0.0/0 network to Authorized Network. Use Cloud SQL Auth proxy on all applications.,,,C,307,"- Using the Cloud SQL Auth proxy is a recommended method for secure connections, especially when dealing with dynamic IP addresses. - The Auth proxy provides secure access to your Cloud SQL instance without the need for Authorized Networks or managing IP addresses. - It works by encapsulating database traffic and forwarding it through a secure tunnel, using Google's IAM for authentication. - Leaving the Authorized Networks empty means you're not allowing any direct connections based on IP addresses, relying entirely on the Auth proxy for secure connectivity. This is a secure and flexible solution, especially for applications with dynamic IPs."
You are preparing an organization-wide dataset. You need to preprocess customer data stored in a restricted bucket in Cloud Storage. The data will be used to create consumer analyses. You need to comply with data privacy requirements.,Use Dataflow and the Cloud Data Loss Prevention API to mask sensitive data. Write the processed data in BigQuery.,Use customer-managed encryption keys (CMEK) to directly encrypt the data in Cloud Storage. Use federated queries from BigQuery. Share the encryption key by following the principle of least privilege.,Use the Cloud Data Loss Prevention API and Dataflow to detect and remove sensitive fields from the data in Cloud Storage. Write the filtered data in BigQuery.,Use Dataflow and Cloud KMS to encrypt sensitive fields and write the encrypted data in BigQuery. Share the encryption key by following the principle of least privilege.,,,A,306,"- Prioritizes Data Privacy: It protects sensitive information by masking it, reducing the risk of exposure in case of unauthorized access or accidental leaks. - Reduces Data Sensitivity: Masking renders sensitive data unusable for attackers, even if they gain access to it. - Preserves Data Utility: Masked data can still be used for consumer analyses, as patterns and relationships are often preserved, allowing meaningful insights to be derived."
"Your organization uses a multi-cloud data storage strategy, storing data in Cloud Storage, and data in Amazon Web Services’ (AWS) S3 storage buckets. All data resides in US regions. You want to query up-to-date data by using BigQuery, regardless of which cloud the data is stored in. You need to allow users to query the tables from BigQuery without giving direct access to the data in the storage buckets. What should you do?",Setup a BigQuery Omni connection to the AWS S3 bucket data. Create BigLake tables over the Cloud Storage and S3 data and query the data using BigQuery directly.,Set up a BigQuery Omni connection to the AWS S3 bucket data. Create external tables over the Cloud Storage and S3 data and query the data using BigQuery directly.,Use the Storage Transfer Service to copy data from the AWS S3 buckets to Cloud Storage buckets. Create BigLake tables over the Cloud Storage data and query the data using BigQuery directly.,Use the Storage Transfer Service to copy data from the AWS S3 buckets to Cloud Storage buckets. Create external tables over the Cloud Storage data and query the data using BigQuery directly.,,,A,305,"- BigQuery Omni: This is an extension of BigQuery that allows you to analyze data across Google Cloud, AWS, and Azure without having to manage the infrastructure or move data across clouds. It's suitable for querying data stored in AWS S3 buckets directly. - BigLake: Allows you to create a logical abstraction (table) over data stored in Cloud Storage and S3, so you can query data using BigQuery without moving it. - Unified Querying: By setting up BigQuery Omni to connect to AWS S3 and creating BigLake tables over both Cloud Storage and S3 data, you can query all data using BigQuery directly."
"You have a table that contains millions of rows of sales data, partitioned by date. Various applications and users query this data many times a minute. The query requires aggregating values by using AVG, MAX, and SUM, and does not require joining to other tables. The required aggregations are only computed over the past year of data, though you need to retain full historical data in the base tables. You want to ensure that the query results always include the latest data from the tables, while also reducing computation cost, maintenance overhead, and duration. What should you do?",Create a materialized view to aggregate the base table data. Include a filter clause to specify the last one year of partitions.,Create a materialized view to aggregate the base table data. Configure a partition expiration on the base table to retain only the last one year of partitions.,Create a view to aggregate the base table data. Include a filter clause to specify the last year of partitions.,Create a new table that aggregates the base table data. Include a filter clause to specify the last year of partitions. Set up a scheduled query to recreate the new table every hour.,,,A,304,"- Materialized View: Materialized views in BigQuery are precomputed views that periodically cache the result of a query for increased performance and efficiency. They are especially beneficial for heavy and repetitive aggregation queries. - Filter for Recent Data: Including a clause to focus on the last year of partitions ensures that the materialized view is only storing and updating the relevant data, optimizing storage and refresh time. - Always Up-to-date: Materialized views are maintained by BigQuery and automatically updated at regular intervals, ensuring they include the latest data up to a certain freshness point."
You are managing a Dataplex environment with raw and curated zones. A data engineering team is uploading JSON and CSV files to a bucket asset in the curated zone but the files are not being automatically discovered by Dataplex. What should you do to ensure that the files are discovered by Dataplex?,Move the JSON and CSV files to the raw zone.,Enable auto-discovery of files for the curated zone.,Use the bg command-line tool to load the JSON and CSV files into BigQuery tables.,Grant object level access to the CSV and JSON files in Cloud Storage.,,,A,303,"Should be A. Curated zone need Parquet, Avro, ORC format not CSV or JSON. Check the ref - https://cloud.google.com/dataplex/docs/add-zone#curated-zones"
"You work for a farming company. You have one BigQuery table named sensors, which is about 500 MB and contains the list of your 5000 sensors, with columns for id, name, and location. This table is updated every hour. Each sensor generates one metric every 30 seconds along with a timestamp, which you want to store in BigQuery. You want to run an analytical query on the data once a week for monitoring purposes. You also want to minimize costs. What data model should you use?","1. Create a metrics column in the sensors table.
2. Set RECORD type and REPEATED mode for the metrics column.
3. Use an UPDATE statement every 30 seconds to add new metrics.","1. Create a metrics column in the sensors table.
2. Set RECORD type and REPEATED mode for the metrics column.
3. Use an INSERT statement every 30 seconds to add new metrics.","1. Create a metrics table partitioned by timestamp.
2. Create a sensorId column in the metrics table, that points to the id column in the sensors table.
3. Use an INSERT statement every 30 seconds to append new metrics to the metrics table.
4. Join the two tables, if needed, when running the analytical query","1. Create a metrics table partitioned by timestamp.
2. Create a sensorId column in the metrics table, which points to the id column in the sensors table.
3. Use an UPDATE statement every 30 seconds to append new metrics to the metrics table.
4. Join the two tables, if needed, when running the analytical query.",,,C,302,"Partitioned Metrics Table: Creating a separate metrics table partitioned by timestamp is a standard practice for time-series data like sensor readings. Partitioning by timestamp allows for more efficient querying, especially when you're only interested in a specific time range (like weekly monitoring).
Reference to Sensors Table: Including a sensorId column that references the id column in the sensors table allows you to maintain a relationship between the metrics and the sensors without duplicating sensor information.
INSERT Every 30 Seconds: Using an INSERT statement every 30 seconds to the partitioned metrics table is a standard approach for time-series data ingestion in BigQuery. It allows for efficient data storage and querying.
Join for Analysis: When you need to analyze the data, you can join the metrics table with the sensors table based on the sensorId, allowing for comprehensive analysis with sensor details."
"You are architecting a data transformation solution for BigQuery. Your developers are proficient with SQL and want to use the ELT development technique. In addition, your developers need an intuitive coding environment and the ability to manage SQL as code. You need to identify a solution for your developers to build these pipelines. What should you do?","Use Dataform to build, manage, and schedule SQL pipelines.","Use Dataflow jobs to read data from Pub/Sub, transform the data, and load the data to BigQuer",Use Data Fusion to build and execute ETL pipelines.,Use Cloud Composer to load data and run SQL pipelines by using the BigQuery job operators.,,,A,301,"- Aligns with ELT Approach: Dataform is designed for ELT (Extract, Load, Transform) pipelines, directly executing SQL transformations within BigQuery, matching the developers' preference. -SQL as Code: It enables developers to write and manage SQL transformations as code, promoting version control, collaboration, and testing. - Intuitive Coding Environment: Dataform provides a user-friendly interface and familiar SQL syntax, making it easy for SQL-proficient developers to adopt. - Scheduling and Orchestration: It includes built-in scheduling capabilities to automate pipeline execution, simplifying pipeline management."
"You currently have transactional data stored on-premises in a PostgreSQL database. To modernize your data environment, you want to run transactional workloads and support analytics needs with a single database. You need to move to Google Cloud without changing database management systems, and minimize cost and complexity. What should you do?",Migrate and modernize your database with Cloud Spanner.,Migrate your workloads to AlloyDB for PostgreSQL.,Migrate to BigQuery to optimize analytics.,Migrate your PostgreSQL database to Cloud SQL for PostgreSQL.,,,D,300,"Minimize cost. https://cloud.google.com/alloydb?hl=en AlloyDB offers superior performance, 4x faster than standard PostgreSQL for transactional workloads. That does not come without cost."
"You have an upstream process that writes data to Cloud Storage. This data is then read by an Apache Spark job that runs on Dataproc. These jobs are run in the us-central1 region, but the data could be stored anywhere in the United States. You need to have a recovery process in place in case of a catastrophic single region failure. You need an approach with a maximum of 15 minutes of data loss (RPO=15 mins). You want to ensure that there is minimal latency when reading the data. What should you do?","1. Create two regional Cloud Storage buckets, one in the us-central1 region and one in the us-south1 region.
2. Have the upstream process write data to the us-central1 bucket. Use the Storage Transfer Service to copy data hourly from the us-central1 bucket to the us-south1 bucket.
3. Run the Dataproc cluster in a zone in the us-central1 region, reading from the bucket in that region.
4. In case of regional failure, redeploy your Dataproc clusters to the us-south1 region and read from the bucket in that region instead.","1. Create a Cloud Storage bucket in the US multi-region.
2. Run the Dataproc cluster in a zone in the us-central1 region, reading data from the US multi-region bucket.
3. In case of a regional failure, redeploy the Dataproc cluster to the us-central2 region and continue reading from the same bucket.","1. Create a dual-region Cloud Storage bucket in the us-central1 and us-south1 regions.
2. Enable turbo replication.
3. Run the Dataproc cluster in a zone in the us-central1 region, reading from the bucket in the us-south1 region.
4. In case of a regional failure, redeploy your Dataproc cluster to the us-south1 region and continue reading from the same bucket","1. Create a dual-region Cloud Storage bucket in the us-central1 and us-south1 regions.
2. Enable turbo replication.
3. Run the Dataproc cluster in a zone in the us-central1 region, reading from the bucket in the same region.
4. In case of a regional failure, redeploy the Dataproc clusters to the us-south1 region and read from the same bucket.",,,D,299,"- Rapid Replication: Turbo replication ensures near-real-time data synchronization between regions, achieving an RPO of 15 minutes or less. - Minimal Latency: Dataproc clusters can read from the bucket in the same region, minimizing data transfer latency and optimizing performance. - Disaster Recovery: In case of regional failure, Dataproc clusters can seamlessly redeploy to the other region and continue reading from the same bucket, ensuring business continuity."
"One of your encryption keys stored in Cloud Key Management Service (Cloud KMS) was exposed. You need to re- encrypt all of your CMEK-protected Cloud Storage data that used that key, and then delete the compromised key. You also want to reduce the risk of objects getting written without customer-managed encryption key (CMEK) protection in the future. What should you do?",Rotate the Cloud KMS key version. Continue to use the same Cloud Storage bucket.,Create a new Cloud KMS key. Set the default CMEK key on the existing Cloud Storage bucket to the new one.,Create a new Cloud KMS key. Create a new Cloud Storage bucket. Copy all objects from the old bucket to the new one bucket while specifying the new Cloud KMS key in the copy command.,Create a new Cloud KMS key. Create a new Cloud Storage bucket configured to use the new key as the default CMEK key. Copy all objects from the old bucket to the new bucket without specifying a key.,,,D,298,"- New Key Creation: A new Cloud KMS key ensures a secure replacement for the compromised one. - New Bucket: A separate bucket prevents potential conflicts with existing objects and configurations. - Default CMEK: Setting the new key as default enforces encryption for all objects in the bucket, reducing the risk of unencrypted data. - Copy Without Key Specification: Copying objects without specifying a key leverages the default key, simplifying the process and ensuring consistent encryption. - Old Key Deletion: After copying, the compromised key can be safely deleted."
You migrated your on-premises Apache Hadoop Distributed File System (HDFS) data lake to Cloud Storage. The data scientist team needs to process the data by using Apache Spark and SQL. Security policies need to be enforced at the column level. You need a cost-effective solution that can scale into a data mesh. What should you do?,"1. Deploy a long-living Dataproc cluster with Apache Hive and Ranger enabled.
2. Configure Ranger for column level security.
3. Process with Dataproc Spark or Hive SQL.","1. Define a BigLake table.
2. Create a taxonomy of policy tags in Data Catalog.
3. Add policy tags to columns.
4. Process with the Spark-BigQuery connector or BigQuery SQL.","1. Load the data to BigQuery tables.
2. Create a taxonomy of policy tags in Data Catalog.
3. Add policy tags to columns.
4. Process with the Spark-BigQuery connector or BigQuery SQL.","1. Apply an Identity and Access Management (IAM) policy at the file level in Cloud Storage.
2. Define a BigQuery external table for SQL processing.
3. Use Dataproc Spark to process the Cloud Storage files.",,,B,297,"- BigLake Integration: BigLake allows you to define tables on top of data in Cloud Storage, providing a bridge between data lake storage and BigQuery's powerful analytics capabilities. This approach is cost-effective and scalable. - Data Catalog for Governance: Creating a taxonomy of policy tags in Google Cloud's Data Catalog and applying these tags to specific columns in your BigLake tables enables fine-grained, column-level access control. - Processing with Spark and SQL: The Spark-BigQuery connector allows data scientists to process data using Apache Spark directly against BigQuery (and BigLake tables). This supports both Spark and SQL processing needs. - Scalability into a Data Mesh: BigLake and Data Catalog are designed to scale and support the data mesh architecture, which involves decentralized data ownership and governance."
"Your infrastructure team has set up an interconnect link between Google Cloud and the on-premises network. You are designing a high-throughput streaming pipeline to ingest data in streaming from an Apache Kafka cluster hosted on- premises. You want to store the data in BigQuery, with as minimal latency as possible. What should you do?","Setup a Kafka Connect bridge between Kafka and Pub/Sub. Use a Google-provided Dataflow template to read the data from Pub/Sub, and write the data to BigQuery.","Use a proxy host in the VPC in Google Cloud connecting to Kafka. Write a Dataflow pipeline, read data from the proxy host, and write the data to BigQuery.","Use Dataflow, write a pipeline that reads the data from Kafka, and writes the data to BigQuery.","Setup a Kafka Connect bridge between Kafka and Pub/Sub. Write a Dataflow pipeline, read the data from Pub/Sub, and write the data to BigQuery.",,,C,296,"Latency: Option C, with direct integration between Kafka and Dataflow, offers lower latency by eliminating intermediate steps. Flexibility: Custom Dataflow pipelines (Option C) provide more control over data processing and optimization compared to using a pre-built template."
You are designing the architecture to process your data from Cloud Storage to BigQuery by using Dataflow. The network team provided you with the Shared VPC network and subnetwork to be used by your pipelines. You need to enable the deployment of the pipeline on the Shared VPC network. What should you do?,Assign the compute.networkUser role to the Dataflow service agent.,Assign the compute.networkUser role to the service account that executes the Dataflow pipeline.,Assign the dataflow.admin role to the Dataflow service agent.,Assign the dataflow.admin role to the service account that executes the Dataflow pipeline.,,,A,295,"- Dataflow service agent is the one responsible for setting up and managing the network resources that Dataflow requires. - By granting the compute.networkUser role to this service agent, we are enabling it to provision the necessary network resources within the Shared VPC for your Dataflow job."
"You work for a large ecommerce company. You are using Pub/Sub to ingest the clickstream data to Google Cloud for analytics. You observe that when a new subscriber connects to an existing topic to analyze data, they are unable to subscribe to older data. For an upcoming yearly sale event in two months, you need a solution that, once implemented, will enable any new subscriber to read the last 30 days of data. What should you do?","Create a new topic, and publish the last 30 days of data each time a new subscriber connects to an existing topic.",Set the topic retention policy to 30 days.,Set the subscriber retention policy to 30 days.,"Ask the source system to re-push the data to Pub/Sub, and subscribe to it.",,,B,294,"- Topic Retention Policy: This policy determines how long messages are retained by Pub/Sub after they are published, even if they have not been acknowledged (consumed) by any subscriber. - 30 Days Retention: By setting the retention policy of the topic to 30 days, all messages published to this topic will be available for consumption for 30 days. This means any new subscriber connecting to the topic can access and analyze data from the past 30 days."
"Your organization is modernizing their IT services and migrating to Google Cloud. You need to organize the data that will be stored in Cloud Storage and BigQuery. You need to enable a data mesh approach to share the data between sales, product design, and marketing departments. What should you do?","1. Create a project for storage of the data for each of your departments.
2. Enable each department to create Cloud Storage buckets and BigQuery datasets.
3. Create user groups for authorized readers for each bucket and dataset.
4. Enable the IT team to administer the user groups to add or remove users as the departments’ request.","1. Create multiple projects for storage of the data for each of your departments’ applications.
2. Enable each department to create Cloud Storage buckets and BigQuery datasets.
3. Publish the data that each department shared in Analytics Hub.
4. Enable all departments to discover and subscribe to the data they need in Analytics Hub.","1. Create a project for storage of the data for your organization.
2. Create a central Cloud Storage bucket with three folders to store the files for each department.
3. Create a central BigQuery dataset with tables prefixed with the department name.
4. Give viewer rights for the storage project for the users of your departments.","1. Create multiple projects for storage of the data for each of your departments’ applications.
2. Enable each department to create Cloud Storage buckets and BigQuery datasets.
3. In Dataplex, map each department to a data lake and the Cloud Storage buckets, and map the BigQuery datasets to zones.
4. Enable each department to own and share the data of their data lakes",,,D,293,"- Decentralized ownership: Each department controls its data lake, aligning with the core principle of data ownership in a data mesh. - Self-service data access: Departments can create and manage their own Cloud Storage buckets and BigQuery datasets within their data lakes, enabling self-service data access. - Interdepartmental sharing: Dataplex facilitates data sharing by enabling departments to publish their data products from their data lakes, making it easily discoverable and usable by other departments."
"You have terabytes of customer behavioral data streaming from Google Analytics into BigQuery daily. Your customers’ information, such as their preferences, is hosted on a Cloud SQL for MySQL database. Your CRM database is hosted on a Cloud SQL for PostgreSQL instance. The marketing team wants to use your customers’ information from the two databases and the customer behavioral data to create marketing campaigns for yearly active customers. You need to ensure that the marketing team can run the campaigns over 100 times a day on typical days and up to 300 during sales. At the same time, you want to keep the load on the Cloud SQL databases to a minimum. What should you do?",Create BigQuery connections to both Cloud SQL databases. Use BigQuery federated queries on the two databases and the Google Analytics data on BigQuery to run these queries.,Create a job on Apache Spark with Dataproc Serverless to query both Cloud SQL databases and the Google Analytics data on BigQuery for these queries.,Create streams in Datastream to replicate the required tables from both Cloud SQL databases to BigQuery for these queries.,"Create a Dataproc cluster with Trino to establish connections to both Cloud SQL databases and BigQuery, to execute the queries.",,,C,292,"- Datastream: It's a fully managed, serverless service for real-time data replication. It allows to stream data from various sources, including Cloud SQL, into BigQuery. - Reduced Load on Cloud SQL: By replicating the required tables from both Cloud SQL databases into BigQuery, you minimize the load on the Cloud SQL instances. The marketing team's queries will be run against BigQuery, which is designed to handle high-volume analytics workloads. - Frequency of Queries: BigQuery can easily handle the high frequency of queries (100 times daily, up to 300 during sales events) due to its powerful data processing capabilities. - Combining Data Sources: Once the data is in BigQuery, you can efficiently combine it with the Google Analytics data for comprehensive analysis and campaign planning."
"You designed a data warehouse in BigQuery to analyze sales data. You want a self-serving, low-maintenance, and cost- effective solution to share the sales dataset to other business units in your organization. What should you do?","Create an Analytics Hub private exchange, and publish the sales dataset.",Enable the other business units’ projects to access the authorized views of the sales dataset.,Create and share views with the users in the other business units.,Use the BigQuery Data Transfer Service to create a schedule that copies the sales dataset to the other business units’ projects.,,,A,291,Analytics Hub offers a centralized platform for managing data sharing and access within the organization. This simplifies access control management.
"You are designing a messaging system by using Pub/Sub to process clickstream data with an event-driven consumer app that relies on a push subscription. You need to configure the messaging system that is reliable enough to handle temporary downtime of the consumer app. You also need the messaging system to store the input messages that cannot be consumed by the subscriber. The system needs to retry failed messages gradually, avoiding overloading the consumer app, and store the failed messages after a maximum of 10 retries in a topic. How should you configure the Pub/Sub subscription?",Increase the acknowledgement deadline to 10 minutes.,"Use immediate redelivery as the subscription retry policy, and configure dead lettering to a different topic with maximum delivery attempts set to 10.","Use exponential backoff as the subscription retry policy, and configure dead lettering to the same source topic with maximum delivery attempts set to 10.","Use exponential backoff as the subscription retry policy, and configure dead lettering to a different topic with maximum delivery attempts set to 10.",,,D,290,"- Exponential Backoff: This retry policy gradually increases the delay between retries, which helps to avoid overloading the consumer app. - Dead Lettering to a Different Topic: Configuring dead lettering sends messages that couldn't be processed after the specified number of delivery attempts (10 in this case) to a separate topic. This allows for handling of failed messages without interrupting the regular flow of new messages. - Maximum Delivery Attempts Set to 10: This setting ensures that the system retries each message up to 10 times before considering it a failure and moving it to the dead letter topic."
"You have data located in BigQuery that is used to generate reports for your company. You have noticed some weekly executive report fields do not correspond to format according to company standards. For example, report errors include different telephone formats and different country code identifiers. This is a frequent issue, so you need to create a recurring job to normalize the data. You want a quick solution that requires no coding. What should you do?","Use Cloud Data Fusion and Wrangler to normalize the data, and set up a recurring job.","Use Dataflow SQL to create a job that normalizes the data, and that after the first run of the job, schedule the pipeline to execute recurrently.",Create a Spark job and submit it to Dataproc Serverless.,"Use BigQuery and GoogleSQL to normalize the data, and schedule recurring queries in BigQuery.",,,A,289,"Definitely A, cloud data fusion and wrangler to setup the clean up pipeline with no coding required"
"You are part of a healthcare organization where data is organized and managed by respective data owners in various storage services. As a result of this decentralized ecosystem, discovering and managing data has become difficult. You need to quickly identify and implement a cost-optimized solution to assist your organization with the following:",Use BigLake to convert the current solution into a data lake architecture.,Build a new data discovery tool on Google Kubernetes Engine that helps with new source onboarding and data lineage tracking.,"Use BigQuery to track data lineage, and use Dataprep to manage data and perform data quality validation.","Use Dataplex to manage data, track data lineage, and perform data quality validation.",,,D,288,
You are administering shared BigQuery datasets that contain views used by multiple teams in your organization. The marketing team is concerned about the variability of their monthly BigQuery analytics spend using the on-demand billing model. You need to help the marketing team establish a consistent BigQuery analytics spend each month. What should you do?,"Create a BigQuery Enterprise reservation with a baseline of 250 slots and autoscaling set to 500 for the marketing team, and bill them back accordingly.","Establish a BigQuery quota for the marketing team, and limit the maximum number of bytes scanned each day.","Create a BigQuery reservation with a baseline of 500 slots with no autoscaling for the marketing team, and bill them back accordingly.","Create a BigQuery Standard pay-as-you go reservation with a baseline of 0 slots and autoscaling set to 500 for the marketing team, and bill them back accordingly.",,,C,287,"Reservations guarantee a fixed number of slots (computational resources) for BigQuery queries, ensuring a predictable monthly cost, addressing the marketing team's concern about variability."
You have thousands of Apache Spark jobs running in your on-premises Apache Hadoop cluster. You want to migrate the jobs to Google Cloud. You want to use managed services to run your jobs instead of maintaining a long-lived Hadoop cluster yourself. You have a tight timeline and want to keep code changes to a minimum. What should you do?,Move your data to BigQuery. Convert your Spark scripts to a SQL-based processing approach.,Rewrite your jobs in Apache Beam. Run your jobs in Dataflow.,Copy your data to Compute Engine disks. Manage and run your jobs directly on those instances.,Move your data to Cloud Storage. Run your jobs on Dataproc.,,,D,286,
"You have 100 GB of data stored in a BigQuery table. This data is outdated and will only be accessed one or two times a year for analytics with SQL. For backup purposes, you want to store this data to be immutable for 3 years. You want to minimize storage costs. What should you do?","1. Create a BigQuery table clone.
2. Query the clone when you need to perform analytics","1. Create a BigQuery table snapshot.
2. Restore the snapshot when you need to perform analytics.","1. Perform a BigQuery export to a Cloud Storage bucket with archive storage class.
2. Enable versioning on the bucket.
3. Create a BigQuery external table on the exported files.","1. Perform a BigQuery export to a Cloud Storage bucket with archive storage class.
2. Set a locked retention policy on the bucket.
3. Create a BigQuery external table on the exported files.",,,D,285,
"You have a network of 1000 sensors. The sensors generate time series data: one metric per sensor per second, along with a timestamp. You already have 1 TB of data, and expect the data to grow by 1 GB every day. You need to access this data in two ways. The first access pattern requires retrieving the metric from one specific sensor stored at a specific timestamp, with a median single-digit millisecond latency. The second access pattern requires running complex analytic queries on the data, including joins, once a day. How should you store this data?","Store your data in BigQuery. Concatenate the sensor ID and timestamp, and use it as the primary key.",Store your data in Bigtable. Concatenate the sensor ID and timestamp and use it as the row key. Perform an export to BigQuery every day.,"Store your data in Bigtable. Concatenate the sensor ID and metric, and use it as the row key. Perform an export to BigQuery every day.",Store your data in BigQuery. Use the metric as a primary key.,,,B,284,
"You have created an external table for Apache Hive partitioned data that resides in a Cloud Storage bucket, which contains a large number of files. You notice that queries against this table are slow. You want to improve the performance of these queries. What should you do?",Change the storage class of the Hive partitioned data objects from Coldline to Standard.,Create an individual external table for each Hive partition by using a common table name prefix. Use wildcard table queries to reference the partitioned data.,Upgrade the external table to a BigLake table. Enable metadata caching for the table.,Migrate the Hive partitioned data objects to a multi-region Cloud Storage bucket.,,,C,283,- BigLake Table: BigLake allows for more efficient querying of data lakes stored in Cloud Storage. It can handle large datasets more effectively than standard external tables. - Metadata Caching: Enabling metadata caching can significantly improve query performance by reducing the time taken to read and process metadata from a large number of files.
"You are using a Dataflow streaming job to read messages from a message bus that does not support exactly-once delivery. Your job then applies some transformations, and loads the result into BigQuery. You want to ensure that your data is being streamed into BigQuery with exactly-once delivery semantics. You expect your ingestion throughput into BigQuery to be about 1.5 GB per second. What should you do?",Use the BigQuery Storage Write API and ensure that your target BigQuery table is regional.,Use the BigQuery Storage Write API and ensure that your target BigQuery table is multiregional.,Use the BigQuery Streaming API and ensure that your target BigQuery table is regional.,Use the BigQuery Streaming API and ensure that your target BigQuery table is multiregional.,,,A,282,"- BigQuery Storage Write API: This API is designed for high-throughput, low-latency writing of data into BigQuery. It also provides tools to prevent data duplication, which is essential for exactly-once delivery semantics. - Regional Table: Choosing a regional location for the BigQuery table could potentially provide better performance and lower latency, as it would be closer to the Dataflow job if they are in the same region."
"You work for a large ecommerce company. You store your customer's order data in Bigtable. You have a garbage collection policy set to delete the data after 30 days and the number of versions is set to 1. When the data analysts run a query to report total customer spending, the analysts sometimes see customer data that is older than 30 days. You need to ensure that the analysts do not see customer data older than 30 days while minimizing cost and overhead. What should you do?",Set the expiring values of the column families to 29 days and keep the number of versions to 1.,Use a timestamp range filter in the query to fetch the customer's data for a specific range.,Schedule a job daily to scan the data in the table and delete data older than 30 days.,Set the expiring values of the column families to 30 days and set the number of versions to 2.,,,B,281,"Agree. https://cloud.google.com/bigtable/docs/garbage-collection#data-removed ""Because it can take up to a week for expired data to be deleted, you should never rely solely on garbage collection policies to ensure that read requests return the desired data. Always apply a filter to your read requests that excludes the same values as your garbage collection rules. You can filter by limiting the number of cells per column or by specifying a timestamp range."""
"You are running a streaming pipeline with Dataflow and are using hopping windows to group the data as the data arrives. You noticed that some data is arriving late but is not being marked as late data, which is resulting in inaccurate aggregations downstream. You need to find a solution that allows you to capture the late data in the appropriate window. What should you do?",Use watermarks to define the expected data arrival window. Allow late data as it arrives.,Change your windowing function to tumbling windows to avoid overlapping window periods.,Change your windowing function to session windows to define your windows based on certain activity.,Expand your hopping window so that the late data has more time to arrive within the grouping.,,,A,280,"- Watermarks: Watermarks in a streaming pipeline are used to specify the point in time when Dataflow expects all data up to that point to have arrived. - Allow Late Data: configure the pipeline to accept and correctly process data that arrives after the watermark, ensuring it's captured in the appropriate window."
"You want to store your team’s shared tables in a single dataset to make data easily accessible to various analysts. You want to make this data readable but unmodifiable by analysts. At the same time, you want to provide the analysts with individual workspaces in the same project, where they can create and store tables for their own use, without the tables being accessible by other analysts. What should you do?","Give analysts the BigQuery Data Viewer role at the project level. Create one other dataset, and give the analysts the BigQuery Data Editor role on that dataset.","Give analysts the BigQuery Data Viewer role on the shared dataset. Create a dataset for each analyst, and give each analyst the BigQuery Data Editor role at the dataset level for their assigned dataset.",Give analysts the BigQuery Data Viewer role on the shared dataset. Create one other dataset and give the analysts the BigQuery Data Editor role on that dataset.,,,,C,279,- Data Viewer on Shared Dataset: Grants read-only access to the shared dataset. - Data Editor on Individual Datasets: Giving each analyst Data Editor role on their respective dataset creates private workspaces where they can create and store personal tables without exposing them to other analysts.
"Your car factory is pushing machine measurements as messages into a Pub/Sub topic in your Google Cloud project. A Dataflow streaming job, that you wrote with the Apache Beam SDK, reads these messages, sends acknowledgment to Pub/Sub, applies some custom business logic in a DoFn instance, and writes the result to BigQuery. You want to ensure that if your business logic fails on a message, the message will be sent to a Pub/Sub topic that you want to monitor for alerting purposes. What should you do?",Enable retaining of acknowledged messages in your Pub/Sub pull subscription. Use Cloud Monitoring to monitor the subscription/num_retained_acked_messages metric on this subscription.,Use an exception handling block in your Dataflow’s DoFn code to push the messages that failed to be transformed through a side output and to a new Pub/Sub topic. Use Cloud Monitoring to monitor the topic/num_unacked_messages_by_region metric on this new topic.,"Enable dead lettering in your Pub/Sub pull subscription, and specify a new Pub/Sub topic as the dead letter topic. Use Cloud Monitoring to monitor the subscription/dead_letter_message_count metric on your pull subscription.",Create a snapshot of your Pub/Sub pull subscription. Use Cloud Monitoring to monitor the snapshot/num_messages metric on this snapshot.,,,B,278,- Exception Handling in DoFn: Implementing an exception handling block within DoFn in Dataflow to catch failures during processing is a direct way to manage errors. - Side Output to New Topic: Using a side output to redirect failed messages to a new Pub/Sub topic is an effective way to isolate and manage these messages. - Monitoring: Monitoring the num_unacked_messages_by_region on the new topic can alert you to the presence of failed messages.
"You are designing a real-time system for a ride hailing app that identifies areas with high demand for rides to effectively reroute available drivers to meet the demand. The system ingests data from multiple sources to Pub/Sub, processes the data, and stores the results for visualization and analysis in real-time dashboards. The data sources include driver location updates every 5 seconds and app-based booking events from riders. The data processing involves real-time aggregation of supply and demand data for the last 30 seconds, every 2 seconds, and storing the results in a low-latency system for visualization. What should you do?","Group the data by using a tumbling window in a Dataflow pipeline, and write the aggregated data to Memorystore.","Group the data by using a hopping window in a Dataflow pipeline, and write the aggregated data to Memorystore.","Group the data by using a session window in a Dataflow pipeline, and write the aggregated data to BigQuery.","Group the data by using a hopping window in a Dataflow pipeline, and write the aggregated data to BigQuery.",,,B,277,"- Hopping Window: Hopping windows are fixed-sized, overlapping intervals. - Aggregate data over the last 30 seconds, every 2 seconds, as hopping windows allow for overlapping data analysis. - Memorystore: Ideal for low-latency access required for real-time visualization and analysis."
You are designing a Dataflow pipeline for a batch processing job. You want to mitigate multiple zonal failures at job submission time. What should you do?,Submit duplicate pipelines in two different zones by using the --zone flag.,Set the pipeline staging location as a regional Cloud Storage bucket.,Specify a worker region by using the --region flag.,Create an Eventarc trigger to resubmit the job in case of zonal failure when submitting the job.,,,C,276,- Specifying a worker region (instead of a specific zone) allows Google Cloud's Dataflow service to manage the distribution of resources across multiple zones within that region
You created an analytics environment on Google Cloud so that your data scientist team can explore data without impacting the on-premises Apache Hadoop solution. The data in the on-premises Hadoop Distributed File System (HDFS) cluster is in Optimized Row Columnar (ORC) formatted files with multiple columns of Hive partitioning. The data scientist team needs to be able to explore the data in a similar way as they used the on-premises HDFS cluster with SQL on the Hive query engine. You need to choose the most cost-effective storage and processing solution. What should you do?,Import the ORC files to Bigtable tables for the data scientist team.,Import the ORC files to BigQuery tables for the data scientist team.,"Copy the ORC files on Cloud Storage, then deploy a Dataproc cluster for the data scientist team.","Copy the ORC files on Cloud Storage, then create external BigQuery tables for the data scientist team.",,,D,275,"- It leverages the strengths of BigQuery for SQL-based exploration while avoiding additional costs and complexity associated with data transformation or migration. - The data remains in ORC format in Cloud Storage, and BigQuery's external tables feature allows direct querying of this data."
You have a BigQuery table that ingests data directly from a Pub/Sub subscription. The ingested data is encrypted with a Google-managed encryption key. You need to meet a new organization policy that requires you to use keys from a centralized Cloud Key Management Service (Cloud KMS) project to encrypt data at rest. What should you do?,Use Cloud KMS encryption key with Dataflow to ingest the existing Pub/Sub subscription to the existing BigQuery table.,"Create a new BigQuery table by using customer-managed encryption keys (CMEK), and migrate the data from the old BigQuery table",Create a new Pub/Sub topic with CMEK and use the existing BigQuery table by using Google-managed encryption key.,"Create a new BigQuery table and Pub/Sub topic by using customer-managed encryption keys (CMEK), and migrate the data from the old BigQuery table.",,,B,274,- New BigQuery Table with CMEK: This option involves creating a new BigQuery table configured to use a CMEK from Cloud KMS. It directly addresses the need to use a CMEK for data at rest in BigQuery. - Migrate Data: Migrating data from the old table (encrypted with a Google-managed key) to the new table (encrypted with CMEK) ensures that all existing data complies with the new policy.
You are creating the CI/CD cycle for the code of the directed acyclic graphs (DAGs) running in Cloud Composer. Your team has two Cloud Composer instances: one instance for development and another instance for production. Your team is using a Git repository to maintain and develop the code of the DAGs. You want to deploy the DAGs automatically to Cloud Composer when a certain tag is pushed to the Git repository. What should you do?,"1. Use Cloud Build to copy the code of the DAG to the Cloud Storage bucket of the development instance for DAG testing.
2. If the tests pass, use Cloud Build to copy the code to the bucket of the production instance.","1. Use Cloud Build to build a container with the code of the DAG and the KubernetesPodOperator to deploy the code to the Google Kubernetes Engine (GKE) cluster of the development instance for testing.
2. If the tests pass, use the KubernetesPodOperator to deploy the container to the GKE cluster of the production instance.","1. Use Cloud Build to build a container and the KubernetesPodOperator to deploy the code of the DAG to the Google Kubernetes Engine (GKE) cluster of the development instance for testing.
2. If the tests pass, copy the code to the Cloud Storage bucket of the production instance.","1. Use Cloud Build to copy the code of the DAG to the Cloud Storage bucket of the development instance for DAG testing.
2. If the tests pass, use Cloud Build to build a container with the code of the DAG and the KubernetesPodOperator to deploy the container to the Google Kubernetes Engine (GKE) cluster of the production instance.",,,A,273,"The Answer is A. Given that there are two instances (development and production) already available, and the goal is to deploy DAGs to Cloud Composer not entire composer infra build. Explanation: - This approach leverages Cloud Build to manage the deployment process. - It first deploys the code to the Cloud Storage bucket of the development instance for testing purposes. - If the tests are successful in the development environment, the same Cloud Build process is used to copy the code to the Cloud Storage bucket of the production instance. B. GKE-based approach is not standard for Cloud Composer. C. GKE used for testing is unconventional for DAG deployments. D. Involves unnecessary GKE deployment for production. Testing DAGs should use Composer instances directly, not Kubernetes containers in GKE."
"You have a BigQuery dataset named “customers”. All tables will be tagged by using a Data Catalog tag template named “gdpr”. The template contains one mandatory field, “has_sensitive_data”, with a boolean value. All employees must be able to do a simple search and find tables in the dataset that have either true or false in the “has_sensitive_data’ field. However, only the Human Resources (HR) group should be able to see the data inside the tables for which “has_sensitive data” is true. You give the all employees group the bigquery.metadataViewer and bigquery.connectionUser roles on the dataset. You want to minimize configuration overhead. What should you do next?",Create the “gdpr” tag template with private visibility. Assign the bigquery.dataViewer role to the HR group on the tables that contain sensitive data.,"Create the “gdpr” tag template with private visibility. Assign the datacatalog.tagTemplateViewer role on this tag to the all employees group, and assign the bigquery.dataViewer role to the HR group on the tables that contain sensitive data.",Create the “gdpr” tag template with public visibility. Assign the bigquery.dataViewer role to the HR group on the tables that contain sensitive data.,"Create the “gdpr” tag template with public visibility. Assign the datacatalog.tagTemplateViewer role on this tag to the all employees group, and assign the bigquery.dataViewer role to the HR group on the tables that contain sensitive data.",,,C,272,"- The most straightforward solution with minimal configuration overhead. - By creating the ""gdpr"" tag template with public visibility, you ensure that all employees can search and find tables based on the ""has_sensitive_data"" field. - Assigning the bigquery.dataViewer role to the HR group on tables with sensitive data ensures that only they can view the actual data in these tables."
"You are monitoring your organization’s data lake hosted on BigQuery. The ingestion pipelines read data from Pub/Sub and write the data into tables on BigQuery. After a new version of the ingestion pipelines is deployed, the daily stored data increased by 50%. The volumes of data in Pub/Sub remained the same and only some tables had their daily partition data size doubled. You need to investigate and fix the cause of the data increase. What should you do?","1. Check for duplicate rows in the BigQuery tables that have the daily partition data size doubled.
2. Schedule daily SQL jobs to deduplicate the affected tables.
3. Share the deduplication script with the other operational teams to reuse if this occurs to other tables.","1. Check for code errors in the deployed pipelines.
2. Check for multiple writing to pipeline BigQuery sink.
3. Check for errors in Cloud Logging during the day of the release of the new pipelines.
4. If no errors, restore the BigQuery tables to their content before the last release by using time travel.","1. Check for duplicate rows in the BigQuery tables that have the daily partition data size doubled.
2. Check the BigQuery Audit logs to find job IDs.
3. Use Cloud Monitoring to determine when the identified Dataflow jobs started and the pipeline code version.
4. When more than one pipeline ingests data into a table, stop all versions except the latest one.","1. Roll back the last deployment.
2. Restore the BigQuery tables to their content before the last release by using time travel.
3. Restart the Dataflow jobs and replay the messages by seeking the subscription to the timestamp of the release.",,,C,271,"- Detailed Investigation of Logs and Jobs Checking for duplicate rows targets the potential immediate cause of the issue. - Checking the BigQuery Audit logs helps identify which jobs might be contributing to the increased data volume. - Using Cloud Monitoring to correlate job starts with pipeline versions helps identify if a specific version of the pipeline is responsible. - Managing multiple versions of pipelines ensures that only the intended version is active, addressing any versioning errors that might have occurred during deployment. ======= Why not B While it addresses the symptom (excess data), it doesn't necessarily stop the problem from recurring. (The questions asked to investigate and fix)"
You need to create a SQL pipeline. The pipeline runs an aggregate SQL transformation on a BigQuery table every two hours and appends the result to another existing BigQuery table. You need to configure the pipeline to retry if errors occur. You want the pipeline to send an email notification after three consecutive failures. What should you do?,"Use the BigQueryUpsertTableOperator in Cloud Composer, set the retry parameter to three, and set the email_on_failure parameter to true.","Use the BigQueryInsertJobOperator in Cloud Composer, set the retry parameter to three, and set the email_on_failure parameter to true.","Create a BigQuery scheduled query to run the SQL transformation with schedule options that repeats every two hours, and enable email notifications.","Create a BigQuery scheduled query to run the SQL transformation with schedule options that repeats every two hours, and enable notification to Pub/Sub topic. Use Pub/Sub and Cloud Functions to send an email after three failed executions.",,,B,270,"- It provides a direct and controlled way to manage the SQL pipeline using Cloud Composer (Apache Airflow). - The BigQueryInsertJobOperator is well-suited for running SQL jobs in BigQuery, including aggregate transformations and handling of results. - The retry and email_on_failure parameters align with the requirements for error handling and notifications. - Cloud Composer requires more setup than using BigQuery's scheduled queries directly, but it offers robust workflow management, retry logic, and notification capabilities, making it suitable for more complex and controlled data pipeline requirements."
"Your organization's data assets are stored in BigQuery, Pub/Sub, and a PostgreSQL instance running on Compute Engine. Because there are multiple domains and diverse teams using the data, teams in your organization are unable to discover existing data assets. You need to design a solution to improve data discoverability while keeping development and configuration efforts to a minimum. What should you do?",Use Data Catalog to automatically catalog BigQuery datasets. Use Data Catalog APIs to manually catalog Pub/Sub topics and PostgreSQL tables.,Use Data Catalog to automatically catalog BigQuery datasets and Pub/Sub topics. Use Data Catalog APIs to manually catalog PostgreSQL tables.,Use Data Catalog to automatically catalog BigQuery datasets and Pub/Sub topics. Use custom connectors to manually catalog PostgreSQL tables.,"Use customer connectors to manually catalog BigQuery datasets, Pub/Sub topics, and PostgreSQL tables.",,,C,269,"Data Catalog is the best choice. But for catalogging PostgreSQL it is better to use a connector when available, instead of using API. https://cloud.google.com/data-catalog/docs/integrate-data-sources#integrate_unsupported_data_sources"
"You created a new version of a Dataflow streaming data ingestion pipeline that reads from Pub/Sub and writes to BigQuery. The previous version of the pipeline that runs in production uses a 5-minute window for processing. You need to deploy the new version of the pipeline without losing any data, creating inconsistencies, or increasing the processing latency by more than 10 minutes. What should you do?",Update the old pipeline with the new pipeline code.,"Snapshot the old pipeline, stop the old pipeline, and then start the new pipeline from the snapshot.","Drain the old pipeline, then start the new pipeline.","Cancel the old pipeline, then start the new pipeline.",,,C,268,"- Graceful Data Transition: Draining the old pipeline ensures it processes all existing data in its buffers and watermarks before shutting down, preventing data loss or inconsistencies. - Minimal Latency Increase: The latency increase will be limited to the amount of time it takes to drain the old pipeline, typically within the acceptable 10-minute threshold."
"You are creating a data model in BigQuery that will hold retail transaction data. Your two largest tables, sales_transaction_header and sales_transaction_line, have a tightly coupled immutable relationship. These tables are rarely modified after load and are frequently joined when queried. You need to model the sales_transaction_header and sales_transaction_line tables to improve the performance of data analytics queries. What should you do?",Create a sales_transaction table that holds the sales_transaction_header information as rows and the sales_transaction_line rows as nested and repeated fields.,"Create a sales_transaction table that holds the sales_transaction_header and sales_transaction_line information as rows, duplicating the sales_transaction_header data for each line.",Create a sales_transaction table that stores the sales_transaction_header and sales_transaction_line data as a JSON data type.,"Create separate sales_transaction_header and sales_transaction_line tables and, when querying, specify the sales_transaction_line first in the WHERE clause.",,,A,267,Option A https://cloud.google.com/bigquery/docs/best-practices-performance-nested
"You are building a streaming Dataflow pipeline that ingests noise level data from hundreds of sensors placed near construction sites across a city. The sensors measure noise level every ten seconds, and send that data to the pipeline when levels reach above 70 dBA. You need to detect the average noise level from a sensor when data is received for a duration of more than 30 minutes, but the window ends when no data has been received for 15 minutes. What should you do?",Use session windows with a 15-minute gap duration.,Use session windows with a 30-minute gap duration.,"Use hopping windows with a 15-minute window, and a thirty-minute period.",Use tumbling windows with a 15-minute window and a fifteen-minute .withAllowedLateness operator.,,,A,266,"to detect average noise levels from sensors, the best approach is to use session windows with a 15-minute gap duration (Option A). Session windows are ideal for cases like this where the events (sensor data) are sporadic. They group events that occur within a certain time interval (15 minutes in your case) and a new window is started if no data is received for the duration of the gap. This matches your requirement to end the window when no data is received for 15 minutes, ensuring that the average noise level is calculated over periods of continuous data"
You are designing a fault-tolerant architecture to store data in a regional BigQuery dataset. You need to ensure that your application is able to recover from a corruption event in your tables that occurred within the past seven days. You want to adopt managed services with the lowest RPO and most cost-effective solution. What should you do?,Access historical data by using time travel in BigQuery.,Export the data from BigQuery into a new table that excludes the corrupted data,Create a BigQuery table snapshot on a daily basis.,Migrate your data to multi-region BigQuery buckets.,,,A,265,"- Lowest RPO: Time travel offers point-in-time recovery for the past seven days by default, providing the shortest possible recovery point objective (RPO) among the given options. You can recover data to any state within that window. - No Additional Costs: Time travel is a built-in feature of BigQuery, incurring no extra storage or operational costs. - Managed Service: BigQuery handles time travel automatically, eliminating manual backup and restore processes."
"You are running your BigQuery project in the on-demand billing model and are executing a change data capture (CDC) process that ingests data. The CDC process loads 1 GB of data every 10 minutes into a temporary table, and then performs a merge into a 10 TB target table. This process is very scan intensive and you want to explore options to enable a predictable cost model. You need to create a BigQuery reservation based on utilization information gathered from BigQuery Monitoring and apply the reservation to the CDC process. What should you do?",Create a BigQuery reservation for the dataset.,Create a BigQuery reservation for the job.,Create a BigQuery reservation for the service account running the job.,Create a BigQuery reservation for the project.,,,D,264,https://cloud.google.com/blog/products/data-analytics/manage-bigquery-costs-with-custom-quotas. Quotas can be applied on Project or User Level
"You maintain ETL pipelines. You notice that a streaming pipeline running on Dataflow is taking a long time to process incoming data, which causes output delays. You also noticed that the pipeline graph was automatically optimized by Dataflow and merged into one step. You want to identify where the potential bottleneck is occurring. What should you do?","Insert a Reshuffle operation after each processing step, and monitor the execution details in the Dataflow console.","Insert output sinks after each key processing step, and observe the writing throughput of each block.","Log debug information in each ParDo function, and analyze the logs at execution time.",Verify that the Dataflow service accounts have appropriate permissions to write the processed data to the output sinks.,,,A,263,"- The Reshuffle operation is used in Dataflow pipelines to break fusion and redistribute elements, which can sometimes help improve parallelization and identify bottlenecks. - By inserting Reshuffle after each processing step and observing the pipeline's performance in the Dataflow console, you can potentially identify stages that are disproportionately slow or stalled. - This can help in pinpointing the step where the bottleneck might be occurring."
You are on the data governance team and are implementing security requirements. You need to encrypt all your data in BigQuery by using an encryption key managed by your team. You must implement a mechanism to generate and store encryption material only on your on-premises hardware security module (HSM). You want to rely on Google managed solutions. What should you do?,"Create the encryption key in the on-premises HSM, and import it into a Cloud Key Management Service (Cloud KMS) key. Associate the created Cloud KMS key while creating the BigQuery resources.",Create the encryption key in the on-premises HSM and link it to a Cloud External Key Manager (Cloud EKM) key. Associate the created Cloud KMS key while creating the BigQuery resources.,"Create the encryption key in the on-premises HSM, and import it into Cloud Key Management Service (Cloud HSM) key. Associate the created Cloud HSM key while creating the BigQuery resources.",Create the encryption key in the on-premises HSM. Create BigQuery resources and encrypt data while ingesting them into BigQuery.,,,B,262,"- Cloud EKM allows you to use encryption keys managed in external key management systems, including on-premises HSMs, while using Google Cloud services. - This means that the key material remains in your control and environment, and Google Cloud services use it via the Cloud EKM integration. - This approach aligns with the need to generate and store encryption material only on your on-premises HSM and is the correct way to integrate such keys with BigQuery."
"You want to migrate your existing Teradata data warehouse to BigQuery. You want to move the historical data to BigQuery by using the most efficient method that requires the least amount of programming, but local storage space on your existing data warehouse is limited. What should you do?",Use BigQuery Data Transfer Service by using the Java Database Connectivity (JDBC) driver with FastExport connection.,"Create a Teradata Parallel Transporter (TPT) export script to export the historical data, and import to BigQuery by using the bq command-line tool.",Use BigQuery Data Transfer Service with the Teradata Parallel Transporter (TPT) tbuild utility.,"Create a script to export the historical data, and upload in batches to Cloud Storage. Set up a BigQuery Data Transfer Service instance from Cloud Storage to BigQuery.",,,A,261,"- Reduced Local Storage: By using FastExport, data is directly streamed from Teradata to BigQuery without the need for local storage, addressing your storage limitations. - Minimal Programming: BigQuery Data Transfer Service offers a user-friendly interface, eliminating the need for extensive scripting or coding."
You have two projects where you run BigQuery jobs:,Create a single Enterprise Edition reservation for both projects. Set a baseline of 300 slots. Enable autoscaling up to 700 slots.,"Create two reservations, one for each of the projects. For the SLA project, use an Enterprise Edition with a baseline of 300 slots and enable autoscaling up to 500 slots. For the ad-hoc project, configure on-demand billing.","Create two Enterprise Edition reservations, one for each of the projects. For the SLA project, set a baseline of 300 slots and enable autoscaling up to 500 slots. For the ad-hoc project, set a reservation baseline of 0 slots and set the ignore idle slots flag to False.","Create two Enterprise Edition reservations, one for each of the projects. For the SLA project, set a baseline of 800 slots. For the ad-hoc project, enable autoscaling up to 200 slots.",,,B,260,"- The SLA project gets a dedicated reservation with autoscaling to handle spikes, ensuring it meets its strict completion time SLAs. - The ad-hoc project uses on-demand billing, which means it will be billed based on the amount of data scanned rather than slot capacity, fitting the billing preference for ad-hoc queries."
"Your business users need a way to clean and prepare data before using the data for analysis. Your business users are less technically savvy and prefer to work with graphical user interfaces to define their transformations. After the data has been transformed, the business users want to perform their analysis directly in a spreadsheet. You need to recommend a solution that they can use. What should you do?","Use Dataprep to clean the data, and write the results to BigQuery. Analyze the data by using Connected Sheets.","Use Dataprep to clean the data, and write the results to BigQuery. Analyze the data by using Looker Studio.","Use Dataflow to clean the data, and write the results to BigQuery. Analyze the data by using Connected Sheets.","Use Dataflow to clean the data, and write the results to BigQuery. Analyze the data by using Looker Studio.",,,A,259,https://cloud.google.com/bigquery/docs/connected-sheets https://cloud.google.com/dataprep
"You have several different file type data sources, such as Apache Parquet and CSV. You want to store the data in Cloud Storage. You need to set up an object sink for your data that allows you to use your own encryption keys. You want to use a GUI-based solution. What should you do?",Use Storage Transfer Service to move files into Cloud Storage.,Use Cloud Data Fusion to move files into Cloud Storage.,Use Dataflow to move files into Cloud Storage.,Use BigQuery Data Transfer Service to move files into BigQuery.,,,B,258,"- Cloud Data Fusion is a fully managed, code-free, GUI-based data integration service that allows you to visually connect, transform, and move data between various sources and sinks. - It supports various file formats and can write to Cloud Storage. - You can configure it to use Customer-Managed Encryption Keys (CMEK) for the buckets where it writes data."
"You are planning to use Cloud Storage as part of your data lake solution. The Cloud Storage bucket will contain objects ingested from external systems. Each object will be ingested once, and the access patterns of individual objects will be random. You want to minimize the cost of storing and retrieving these objects. You want to ensure that any cost optimization efforts are transparent to the users and applications. What should you do?",Create a Cloud Storage bucket with Autoclass enabled.,Create a Cloud Storage bucket with an Object Lifecycle Management policy to transition objects from Standard to Coldline storage class if an object age reaches 30 days.,Create a Cloud Storage bucket with an Object Lifecycle Management policy to transition objects from Standard to Coldline storage class if an object is not live.,"Create two Cloud Storage buckets. Use the Standard storage class for the first bucket, and use the Coldline storage class for the second bucket. Migrate objects from the first bucket to the second bucket after 30 days.",,,A,257,
"You are deploying an Apache Airflow directed acyclic graph (DAG) in a Cloud Composer 2 instance. You have incoming files in a Cloud Storage bucket that the DAG processes, one file at a time. The Cloud Composer instance is deployed in a subnetwork with no Internet access. Instead of running the DAG based on a schedule, you want to run the DAG in a reactive way every time a new file is received. What should you do?","1. Enable Private Google Access in the subnetwork, and set up Cloud Storage notifications to a Pub/Sub topic.
2. Create a push subscription that points to the web server URL.","1. Enable the Cloud Composer API, and set up Cloud Storage notifications to trigger a Cloud Function.
2. Write a Cloud Function instance to call the DAG by using the Cloud Composer API and the web server URL.
3. Use VPC Serverless Access to reach the web server URL.","1. Enable the Airflow REST API, and set up Cloud Storage notifications to trigger a Cloud Function instance.
2. Create a Private Service Connect (PSC) endpoint.
3. Write a Cloud Function that connects to the Cloud Composer cluster through the PSC endpoint. ","1. Enable the Airflow REST API, and set up Cloud Storage notifications to trigger a Cloud Function instance.
2. Write a Cloud Function instance to call the DAG by using the Airflow REST API and the web server URL.
3. Use VPC Serverless Access to reach the web server URL.",,,C,256,"- Enable Airflow REST API: In Cloud Composer, enable the ""Airflow web server"" option. - Set Up Cloud Storage Notifications: Create a notification for new files, routing to a Cloud Function. - Create PSC Endpoint: Establish a PSC endpoint for Cloud Composer. - Write Cloud Function: Code the function to use the Airflow REST API (via PSC endpoint) to trigger the DAG."
You have an Oracle database deployed in a VM as part of a Virtual Private Cloud (VPC) network. You want to replicate and continuously synchronize 50 tables to BigQuery. You want to minimize the need to manage infrastructure. What should you do?,"Deploy Apache Kafka in the same VPC network, use Kafka Connect Oracle Change Data Capture (CDC), and Dataflow to stream the Kafka topic to BigQuery.","Create a Pub/Sub subscription to write to BigQuery directly. Deploy the Debezium Oracle connector to capture changes in the Oracle database, and sink to the Pub/Sub topic.","Deploy Apache Kafka in the same VPC network, use Kafka Connect Oracle change data capture (CDC), and the Kafka Connect Google BigQuery Sink Connector.","Create a Datastream service from Oracle to BigQuery, use a private connectivity configuration to the same VPC network, and a connection profile to BigQuery.",,,D,255,"- Datastream is a serverless and easy-to-use change data capture (CDC) and replication service. - You would create a Datastream service that sources from your Oracle database and targets BigQuery, with private connectivity configuration to the same VPC. - This option is designed to minimize the need to manage infrastructure and is a fully managed service."
"You are running a Dataflow streaming pipeline, with Streaming Engine and Horizontal Autoscaling enabled. You have set the maximum number of workers to 1000. The input of your pipeline is Pub/Sub messages with notifications from Cloud Storage. One of the pipeline transforms reads CSV files and emits an element for every CSV line. The job performance is low, the pipeline is using only 10 workers, and you notice that the autoscaler is not spinning up additional workers. What should you do to improve performance?",Enable Vertical Autoscaling to let the pipeline use larger workers.,"Change the pipeline code, and introduce a Reshuffle step to prevent fusion.",Update the job to increase the maximum number of workers.,"Use Dataflow Prime, and enable Right Fitting to increase the worker resources.",,,B,254,"- Fusion optimization in Dataflow can lead to steps being ""fused"" together, which can sometimes hinder parallelization. - Introducing a Reshuffle step can prevent fusion and force the distribution of work across more workers. - This can be an effective way to improve parallelism and potentially trigger the autoscaler to increase the number of workers."
"You are deploying a batch pipeline in Dataflow. This pipeline reads data from Cloud Storage, transforms the data, and then writes the data into BigQuery. The security team has enabled an organizational constraint in Google Cloud, requiring all Compute Engine instances to use only internal IP addresses and no external IP addresses. What should you do?",Ensure that your workers have network tags to access Cloud Storage and BigQuery. Use Dataflow with only internal IP addresses.,Ensure that the firewall rules allow access to Cloud Storage and BigQuery. Use Dataflow with only internal IPs.,"Create a VPC Service Controls perimeter that contains the VPC network and add Dataflow, Cloud Storage, and BigQuery as allowed services in the perimeter. Use Dataflow with only internal IP addresses.",Ensure that Private Google Access is enabled in the subnetwork. Use Dataflow with only internal IP addresses.,,,D,253,"- Private Google Access for services allows VM instances with only internal IP addresses in a VPC network or on-premises networks (via Cloud VPN or Cloud Interconnect) to reach Google APIs and services. - When you launch a Dataflow job, you can specify that it should use worker instances without external IP addresses if Private Google Access is enabled on the subnetwork where these instances are launched. - This way, your Dataflow workers will be able to access Cloud Storage and BigQuery without violating the organizational constraint of no external IPs."
"You are designing a data warehouse in BigQuery to analyze sales data for a telecommunication service provider. You need to create a data model for customers, products, and subscriptions. All customers, products, and subscriptions can be updated monthly, but you must maintain a historical record of all data. You plan to use the visualization layer for current and historical reporting. You need to ensure that the data model is simple, easy-to-use, and cost-effective. What should you do?",Create a normalized model with tables for each entity. Use snapshots before updates to track historical data.,Create a normalized model with tables for each entity. Keep all input files in a Cloud Storage bucket to track historical data.,Create a denormalized model with nested and repeated fields. Update the table and use snapshots to track historical data.,"Create a denormalized, append-only model with nested and repeated fields. Use the ingestion timestamp to track historical data.",,,D,252,"- A denormalized, append-only model simplifies query complexity by eliminating the need for joins. - Adding data with an ingestion timestamp allows for easy retrieval of both current and historical states. - Instead of updating records, new records are appended, which maintains historical information without the need to create separate snapshots."
You have important legal hold documents in a Cloud Storage bucket. You need to ensure that these documents are not deleted or modified. What should you do?,Set a retention policy. Lock the retention policy.,Set a retention policy. Set the default storage class to Archive for long-term digital preservation.,Enable the Object Versioning feature. Add a lifecycle rule.,Enable the Object Versioning feature. Create a copy in a bucket in a different region.,,,A,251,"- Setting a retention policy on a Cloud Storage bucket prevents objects from being deleted for the duration of the retention period. - Locking the policy makes it immutable, meaning that the retention period cannot be reduced or removed, thus ensuring that the documents cannot be deleted or overwritten until the retention period expires."
"Your company's data platform ingests CSV file dumps of booking and user profile data from upstream sources into Cloud Storage. The data analyst team wants to join these datasets on the email field available in both the datasets to perform analysis. However, personally identifiable information (PII) should not be accessible to the analysts. You need to de-identify the email field in both the datasets before loading them into BigQuery for analysts. What should you do?","1. Create a pipeline to de-identify the email field by using recordTransformations in Cloud Data Loss Prevention (Cloud DLP) with masking as the de-identification transformations type.
2. Load the booking and user profile data into a BigQuery table.","1. Create a pipeline to de-identify the email field by using recordTransformations in Cloud DLP with format-preserving encryption with FFX as the de-identification transformation type.
2. Load the booking and user profile data into a BigQuery table.","1. Load the CSV files from Cloud Storage into a BigQuery table, and enable dynamic data masking.
2. Create a policy tag with the email mask as the data masking rule.
3. Assign the policy to the email field in both tables. A
4. Assign the Identity and Access Management bigquerydatapolicy.maskedReader role for the BigQuery tables to the analysts.","1. Load the CSV files from Cloud Storage into a BigQuery table, and enable dynamic data masking.
2. Create a policy tag with the default masking value as the data masking rule.
3. Assign the policy to the email field in both tables.
4. Assign the Identity and Access Management bigquerydatapolicy.maskedReader role for the BigQuery tables to the analysts",,,B,250,"Format-preserving encryption (FPE) with FFX in Cloud DLP is a strong choice for de-identifying PII like email addresses. FPE maintains the format of the data and ensures that the same input results in the same encrypted output consistently. This means the email fields in both datasets can be encrypted to the same value, allowing for accurate joins in BigQuery while keeping the actual email addresses hidden."
"Your team is building a data lake platform on Google Cloud. As a part of the data foundation design, you are planning to store all the raw data in Cloud Storage. You are expecting to ingest approximately 25 GB of data a day and your billing department is worried about the increasing cost of storing old data. The current business requirements are:",Create the bucket with the Autoclass storage class feature.,"Create an Object Lifecycle Management policy to modify the storage class for data older than 30 days to nearline, 90 days to coldline, and 365 days to archive storage class. Delete old data as needed.","Create an Object Lifecycle Management policy to modify the storage class for data older than 30 days to coldline, 90 days to nearline, and 365 days to archive storage class. Delete old data as needed.","Create an Object Lifecycle Management policy to modify the storage class for data older than 30 days to nearline, 45 days to coldline, and 60 days to archive storage class. Delete old data as needed.",,,A,249,"- Autoclass automatically moves objects between storage classes without impacting performance or availability, nor incurring retrieval costs. - It continuously optimizes storage costs based on access patterns without the need to set specific lifecycle management policies."
You have an inventory of VM data stored in the BigQuery table. You want to prepare the data for regular reporting in the most cost-effective way. You need to exclude VM rows with fewer than 8 vCPU in your report. What should you do?,"Create a view with a filter to drop rows with fewer than 8 vCPU, and use the UNNEST operator.","Create a materialized view with a filter to drop rows with fewer than 8 vCPU, and use the WITH common table expression.","Create a view with a filter to drop rows with fewer than 8 vCPU, and use the WITH common table expression.",Use Dataflow to batch process and write the result to another BigQuery table.,,,A,248,- The table structure shows that the vCPU data is stored in a nested field within the components column. - Using the UNNEST operator to flatten the nested field and apply the filter
"Your company operates in three domains: airlines, hotels, and ride-hailing services. Each domain has two teams: analytics and data science, which create data assets in BigQuery with the help of a central data platform team. However, as each domain is evolving rapidly, the central data platform team is becoming a bottleneck. This is causing delays in deriving insights from data, and resulting in stale data when pipelines are not kept up to date. You need to design a data mesh architecture by using Dataplex to eliminate the bottleneck. What should you do?","A. 1. Create one lake for each team. Inside each lake, create one zone for each domain.
2. Attach each of the BigQuery datasets created by the individual teams as assets to the respective zone.
3. Have the central data platform team manage all zones’ data assets.","1. Create one lake for each team. Inside each lake, create one zone for each domain.
2. Attach each of the BigQuery datasets created by the individual teams as assets to the respective zone.
3. Direct each domain to manage their own zone’s data assets.","1. Create one lake for each domain. Inside each lake, create one zone for each team.
2. Attach each of the BigQuery datasets created by the individual teams as assets to the respective zone.
3. Direct each domain to manage their own lake’s data assets.","1. Create one lake for each domain. Inside each lake, create one zone for each team.
2. Attach each of the BigQuery datasets created by the individual teams as assets to the respective zone.
3. Have the central data platform team manage all lakes’ data assets.",,,C,247,- each domain should manage their own lake’s data assets
You have one BigQuery dataset which includes customers’ street addresses. You want to retrieve all occurrences of street addresses from the dataset. What should you do?,Write a SQL query in BigQuery by using REGEXP_CONTAINS on all tables in your dataset to find rows where the word “street” appears.,Create a deep inspection job on each table in your dataset with Cloud Data Loss Prevention and create an inspection template that includes the STREET_ADDRESS infoType.,Create a discovery scan configuration on your organization with Cloud Data Loss Prevention and create an inspection template that includes the STREET_ADDRESS infoType.,Create a de-identification job in Cloud Data Loss Prevention and use the masking transformation.,,,B,246,"- Cloud Data Loss Prevention (Cloud DLP) provides powerful inspection capabilities for sensitive data, including predefined detectors for infoTypes such as STREET_ADDRESS. - By creating a deep inspection job for each table with the STREET_ADDRESS infoType, you can accurately identify and retrieve rows that contain street addresses."
You are developing a model to identify the factors that lead to sales conversions for your customers. You have completed processing your data. You want to continue through the model development lifecycle. What should you do next?,Use your model to run predictions on fresh customer input data.,"Monitor your model performance, and make any adjustments needed.",Delineate what data will be used for testing and what will be used for training the model.,Test and evaluate your model on your curated data to determine how well the model performs.,,,C,245,"- Before you can train a model, you need to decide how to split your dataset."
"Different teams in your organization store customer and performance data in BigQuery. Each team needs to keep full control of their collected data, be able to query data within their projects, and be able to exchange their data with other teams. You need to implement an organization-wide solution, while minimizing operational tasks and costs. What should you do?",Ask each team to create authorized views of their data. Grant the biquery.jobUser role to each team.,Create a BigQuery scheduled query to replicate all customer data into team projects.,Ask each team to publish their data in Analytics Hub. Direct the other teams to subscribe to them.,Enable each team to create materialized views of the data they need to access in their projects.,,,C,244,"Centralized Data Exchange: Analytics Hub provides a unified platform for data sharing across teams and organizations. It simplifies the process of publishing, discovering, and subscribing to datasets, reducing operational overhead. Data Ownership and Control: Each team retains full control over their data, deciding which datasets to publish and who can access them. This ensures data governance and security. Cross-Project Querying: Once a team subscribes to a dataset in Analytics Hub, they can query it directly from their own BigQuery project, enabling seamless data access without data replication. Cost Efficiency: Analytics Hub eliminates the need for data duplication or complex ETL processes, reducing storage and processing costs."
You are preparing data that your machine learning team will use to train a model using BigQueryML. They want to predict the price per square foot of real estate. The training data has a column for the price and a column for the number of square feet. Another feature column called ‘feature1’ contains null values due to missing data. You want to replace the nulls with zeros to keep more data points. Which query should you use?,"SELECT *EXCEPT(featurel),
IFNULL(featurel,0)As featurel cleaned FRoM training data;","SELECT * EXCEPT(price,square_feet),price/square_feet As price_per_sqft FRoM training data WHERE featurel IS NOT NULL;","SELECT * EXCEPT(price,square_feet,featurel),price/square_feet as price_per_sqft,IFNULL(featurel,0)As featurel cleaned FRoM training data;","SELECT *
FRoM training data WHERE featurel IS NOT NULL;",,,A,243,"Option A is the correct choice because it retains all the original columns and specifically addresses the issue of null values in ‘feature1’ by replacing them with zeros, without altering any other columns or performing unnecessary calculations. This makes the data ready for use in BigQueryML without losing any important information. Option C is not the best choice because it includes the EXCEPT clause for the price and square_feet columns, which would exclude these columns from the results. This is not desirable since you need these columns for the machine learning model to predict the price per square foot"
"You have designed an Apache Beam processing pipeline that reads from a Pub/Sub topic. The topic has a message retention duration of one day, and writes to a Cloud Storage bucket. You need to select a bucket location and processing strategy to prevent data loss in case of a regional outage with an RPO of 15 minutes. What should you do?","1. Use a dual-region Cloud Storage bucket.
2. Monitor Dataflow metrics with Cloud Monitoring to determine when an outage occurs.
3. Seek the subscription back in time by 15 minutes to recover the acknowledged messages.
4. Start the Dataflow job in a secondary region.","1. Use a multi-regional Cloud Storage bucket.
2. Monitor Dataflow metrics with Cloud Monitoring to determine when an outage occurs.
3. Seek the subscription back in time by 60 minutes to recover the acknowledged messages.
4. Start the Dataflow job in a secondary region.","1. Use a regional Cloud Storage bucket.
2. Monitor Dataflow metrics with Cloud Monitoring to determine when an outage occurs.
3. Seek the subscription back in time by one day to recover the acknowledged messages.
4. Start the Dataflow job in a secondary region and write in a bucket in the same region.","1. Use a dual-region Cloud Storage bucket with turbo replication enabled.
2. Monitor Dataflow metrics with Cloud Monitoring to determine when an outage occurs.
3. Seek the subscription back in time by 60 minutes to recover the acknowledged messages.
4. Start the Dataflow job in a secondary region.",,,D,242,RPO of 15 minutes is guaranteed when turbo replication is used https://cloud.google.com/storage/docs/availability-durability
"You are designing the architecture of your application to store data in Cloud Storage. Your application consists of pipelines that read data from a Cloud Storage bucket that contains raw data, and write the data to a second bucket after processing. You want to design an architecture with Cloud Storage resources that are capable of being resilient if a Google Cloud regional failure occurs. You want to minimize the recovery point objective (RPO) if a failure occurs, with no impact on applications that use the stored data. What should you do?",Adopt multi-regional Cloud Storage buckets in your architecture.,"Adopt two regional Cloud Storage buckets, and update your application to write the output on both buckets.","Adopt a dual-region Cloud Storage bucket, and enable turbo replication in your architecture.","Adopt two regional Cloud Storage buckets, and create a daily task to copy from one bucket to the other.",,,C,241,"- Dual-region buckets are a specific type of storage that automatically replicates data between two geographically distinct regions. - Turbo replication is an enhanced feature that provides faster replication between the two regions, thus minimizing RPO. - This option ensures that your data is resilient to regional failures and is replicated quickly, meeting the needs for low RPO and no impact on application performance."
"You are designing a data mesh on Google Cloud by using Dataplex to manage data in BigQuery and Cloud Storage. You want to simplify data asset permissions. You are creating a customer virtual lake with two user groups:

• Data engineers, which require full data lake access
• Analytic users, which require access to curated data

You need to assign access rights to these two groups. What should you do?","1. Grant the dataplex.dataOwner role to the data engineer group on the customer data lake.
2. Grant the dataplex.dataReader role to the analytic user group on the customer curated zone.","1. Grant the dataplex.dataReader role to the data engineer group on the customer data lake.
2. Grant the dataplex.dataOwner to the analytic user group on the customer curated zone.","1. Grant the bigquery.dataOwner role on BigQuery datasets and the storage.objectCreator role on Cloud Storage buckets to data engineers.
2. Grant the bigquery.dataViewer role on BigQuery datasets and the storage.objectViewer role on Cloud Storage buckets to analytic users.","1. Grant the bigquery.dataViewer role on BigQuery datasets and the storage.objectViewer role on Cloud Storage buckets to data engineers.
2. Grant the bigquery.dataOwner role on BigQuery datasets and the storage.objectEditor role on Cloud Storage buckets to analytic users.",,,A,240,"- dataplex.dataOwner: Grants full control over data assets, including reading, writing, managing, and granting access to others.
- dataplex.dataReader: Allows users to read data but not modify it."
You are responsible for a web application on Compute Engine. You want your support team to be notified automatically if users experience high latency for at least 5 minutes. You need a Google-recommended solution with no development cost. What should you do?,Export Cloud Monitoring metrics to BigQuery and use a Looker Studio dashboard to monitor your web application’s latency.,Create an alert policy to send a notification when the HTTP response latency exceeds the specified threshold.,Implement an App Engine service which invokes the Cloud Monitoring API and sends a notification in case of anomalies.,Use the Cloud Monitoring dashboard to observe latency and take the necessary actions when the response latency exceeds the specified threshold.,,,B,239,"""You need a Google-recommended solution with no development cost"""
You want to encrypt the customer data stored in BigQuery. You need to implement per-user crypto-deletion on data stored in your tables. You want to adopt native features in Google Cloud to avoid custom solutions. What should you do?,Implement Authenticated Encryption with Associated Data (AEAD) BigQuery functions while storing your data in BigQuery.,Create a customer-managed encryption key (CMEK) in Cloud KMS. Associate the key to the table while creating the table.,Create a customer-managed encryption key (CMEK) in Cloud KMS. Use the key to encrypt data before storing in BigQuery.,Encrypt your data during ingestion by using a cryptographic library supported by your ETL pipeline.,,,A,238,"- AEAD cryptographic functions in BigQuery allow for encryption and decryption of data at the column level.
- You can encrypt specific data fields using a unique key per user and manage these keys outside of BigQuery (for example, in your application or using a key management system).
- By ""deleting"" or revoking access to the key for a specific user, you effectively make their data unreadable, achieving crypto-deletion.
- This method provides fine-grained encryption control but requires careful key management and integration with your applications."
"You are planning to load some of your existing on-premises data into BigQuery on Google Cloud. You want to either stream or batch-load data, depending on your use case. Additionally, you want to mask some sensitive data before loading into BigQuery. You need to do this in a programmatic way while keeping costs to a minimum. What should you do?","Use Cloud Data Fusion to design your pipeline, use the Cloud DLP plug-in to de-identify data within your pipeline, and then move the data into BigQuery.","Use the BigQuery Data Transfer Service to schedule your migration. After the data is populated in BigQuery, use the connection to the Cloud Data Loss Prevention (Cloud DLP) API to de-identify the necessary data.","Create your pipeline with Dataflow through the Apache Beam SDK for Python, customizing separate options within your code for streaming, batch processing, and Cloud DLP. Select BigQuery as your data sink.",Set up Datastream to replicate your on-premise data on BigQuery.,,,C,237,"- Programmatic Flexibility: Apache Beam provides extensive control over pipeline design, allowing for customization of data transformations, including integration with Cloud DLP for sensitive data masking.
- Streaming and Batch Support: Beam seamlessly supports both streaming and batch data processing modes, enabling flexibility in data loading patterns.
- Cost-Effective Processing: Dataflow offers a serverless model, scaling resources as needed, and only charging for resources used, helping optimize costs.
- Integration with Cloud DLP: Beam integrates well with Cloud DLP for sensitive data masking, ensuring data privacy before loading into BigQuery."
"You are deploying a MySQL database workload onto Cloud SQL. The database must be able to scale up to support several readers from various geographic regions. The database must be highly available and meet low RTO and RPO requirements, even in the event of a regional outage. You need to ensure that interruptions to the readers are minimal during a database failover. What should you do?",Create a highly available Cloud SQL instance in region Create a highly available read replica in region B. Scale up read workloads by creating cascading read replicas in multiple regions. Backup the Cloud SQL instances to a multi-regional Cloud Storage bucket. Restore the Cloud SQL backup to a new instance in another region when Region A is down.,Create a highly available Cloud SQL instance in region A. Scale up read workloads by creating read replicas in multiple regions. Promote one of the read replicas when region A is down.,Create a highly available Cloud SQL instance in region A. Create a highly available read replica in region B. Scale up read workloads by creating cascading read replicas in multiple regions. Promote the read replica in region B when region A is down.,Create a highly available Cloud SQL instance in region A. Scale up read workloads by creating read replicas in the same region. Failover to the standby Cloud SQL instance when the primary instance fails.,,,C,236,Option C: Because HA read replica in multiple regions. NotA: Coz restore from back up is time taking NotB: No HA in Multiple regions read replica Not D: Only one region mentioned.
"You want to schedule a number of sequential load and transformation jobs. Data files will be added to a Cloud Storage bucket by an upstream process. There is no fixed schedule for when the new data arrives. Next, a Dataproc job is triggered to perform some transformations and write the data to BigQuery. You then need to run additional transformation jobs in BigQuery. The transformation jobs are different for every table. These jobs might take hours to complete. You need to determine the most efficient and maintainable workflow to process hundreds of tables and provide the freshest data to your end users. What should you do?","1. Create an Apache Airflow directed acyclic graph (DAG) in Cloud Composer with sequential tasks by using the Cloud Storage, Dataproc, and BigQuery operators.
2. Use a single shared DAG for all tables that need to go through the pipeline.
3. Schedule the DAG to run hourly.","1. Create an Apache Airflow directed acyclic graph (DAG) in Cloud Composer with sequential tasks by using the Cloud Storage, Dataproc, and BigQuery operators.
2. Create a separate DAG for each table that needs to go through the pipeline.
3. Schedule the DAGs to run hourly.","1. Create an Apache Airflow directed acyclic graph (DAG) in Cloud Composer with sequential tasks by using the Dataproc and BigQuery operators.
2. Use a single shared DAG for all tables that need to go through the pipeline.
3. Use a Cloud Storage object trigger to launch a Cloud Function that triggers the DAG.","1. Create an Apache Airflow directed acyclic graph (DAG) in Cloud Composer with sequential tasks by using the Dataproc and BigQuery operators.
2. Create a separate DAG for each table that needs to go through the pipeline.
3. Use a Cloud Storage object trigger to launch a Cloud Function that triggers the DAG.",,,D,235,"* Transformations are in Dataproc and BigQuery. So you don't need operators for GCS (A and B can be discard) * ""There is no fixed schedule for when the new data arrives."" so you trigger the DAG when a file arrives * ""The transformation jobs are different for every table. "" so you need a DAG for each table. Then, D is the most suitable answer"
"You migrated a data backend for an application that serves 10 PB of historical product data for analytics. Only the last known state for a product, which is about 10 GB of data, needs to be served through an API to the other applications. You need to choose a cost-effective persistent storage solution that can accommodate the analytics requirements and the API performance of up to 1000 queries per second (QPS) with less than 1 second latency. What should you do?","1. Store the historical data in BigQuery for analytics.
2. Use a materialized view to precompute the last state of a product.
3. Serve the last state data directly from BigQuery to the API.","1. Store the products as a collection in Firestore with each product having a set of historical changes.
2. Use simple and compound queries for analytics.
3. Serve the last state data directly from Firestore to the API.","1. Store the historical data in Cloud SQL for analytics.
2. In a separate table, store the last state of the product after every product change.
3. Serve the last state data directly from Cloud SQL to the API.","1. Store the historical data in BigQuery for analytics.
2. In a Cloud SQL table, store the last state of the product after every product change.
3. Serve the last state data directly from Cloud SQL to the API.",,,D,234,"This approach leverages BigQuery's scalability and efficiency for handling large datasets for analytics. BigQuery is well-suited for managing the 10 PB of historical product data. Meanwhile, Cloud SQL provides the necessary performance to handle the API queries with the required low latency. By storing the latest state of each product in Cloud SQL, you can efficiently handle the high QPS with sub-second latency, which is crucial for the API's performance. This combination of BigQuery and Cloud SQL offers a balanced solution for both the large-scale analytics and the high-performance API needs."
"You are a BigQuery admin supporting a team of data consumers who run ad hoc queries and downstream reporting in tools such as Looker. All data and users are combined under a single organizational project. You recently noticed some slowness in query results and want to troubleshoot where the slowdowns are occurring. You think that there might be some job queuing or slot contention occurring as users run jobs, which slows down access to results. You need to investigate the query job information and determine where performance is being affected. What should you do?",Use slot reservations for your project to ensure that you have enough query processing capacity and are able to allocate available slots to the slower queries.,Use Cloud Monitoring to view BigQuery metrics and set up alerts that let you know when a certain percentage of slots were used.,Use available administrative resource charts to determine how slots are being used and how jobs are performing over time. Run a query on the INFORMATION_SCHEMA to review query performance.,Use Cloud Logging to determine if any users or downstream consumers are changing or deleting access grants on tagged resources.,,,C,233,"- BigQuery provides administrative resource charts that show slot utilization and job performance, which can help identify patterns of heavy usage or contention. - Additionally, querying the INFORMATION_SCHEMA with the JOBS or JOBS_BY_PROJECT view can provide detailed information about specific queries, including execution time, slot usage, and whether they were queued."
"You are on the data governance team and are implementing security requirements to deploy resources. You need to ensure that resources are limited to only the europe-west3 region. You want to follow Google-recommended practices.

What should you do?",Set the constraints/gcp.resourceLocations organization policy constraint to in:europe-west3-locations.,Deploy resources with Terraform and implement a variable validation rule to ensure that the region is set to the europe-west3 region for all resources.,Set the constraints/gcp.resourceLocations organization policy constraint to in:eu-locations.,Create a Cloud Function to monitor all resources created and automatically destroy the ones created outside the europe-west3 region.,,,A,232,- The constraints/gcp.resourceLocations organization policy constraint is used to define where resources in the organization can be created. - Setting it to in:europe-west3-locations would specify that resources can only be created in the europe-west3 region.
"You recently deployed several data processing jobs into your Cloud Composer 2 environment. You notice that some tasks are failing in Apache Airflow. On the monitoring dashboard, you see an increase in the total workers memory usage, and there were worker pod evictions. You need to resolve these errors. What should you do? (Choose two.)",Increase the directed acyclic graph (DAG) file parsing interval.,Increase the Cloud Composer 2 environment size from medium to large.,Increase the maximum number of workers and reduce worker concurrency.,Increase the memory available to the Airflow workers.,Increase the memory available to the Airflow triggerer.,,"C,D",231,"If an Airflow worker pod is evicted, all task instances running on that pod are interrupted, and later marked as failed by Airflow. The majority of issues with worker pod evictions happen because of out-of-memory situations in workers. You might want to: - (D) Increase the memory available to workers. - (C) Reduce worker concurrency. In this way, a single worker handles fewer tasks at once. This provides more memory or storage to each individual task. If you change worker concurrency, you might also want to increase the maximum number of workers. In this way, the number of tasks that your environment can handle at once stays the same. For example, if you reduce worker Concurrency from 12 to 6, you might want to double the maximum number of workers."
"You need to modernize your existing on-premises data strategy. Your organization currently uses:
• Apache Hadoop clusters for processing multiple large data sets, including on-premises Hadoop Distributed File System (HDFS) for data replication.
• Apache Airflow to orchestrate hundreds of ETL pipelines with thousands of job steps.

You need to set up a new architecture in Google Cloud that can handle your Hadoop workloads and requires minimal changes to your existing orchestration processes. What should you do?","Use Bigtable for your large workloads, with connections to Cloud Storage to handle any HDFS use cases. Orchestrate your pipelines with Cloud Composer.","Use Dataproc to migrate Hadoop clusters to Google Cloud, and Cloud Storage to handle any HDFS use cases. Orchestrate your pipelines with Cloud Composer.","Use Dataproc to migrate Hadoop clusters to Google Cloud, and Cloud Storage to handle any HDFS use cases. Convert your ETL pipelines to Dataflow.","Use Dataproc to migrate your Hadoop clusters to Google Cloud, and Cloud Storage to handle any HDFS use cases. Use Cloud Data Fusion to visually design and deploy your ETL pipelines.",,,B,230,"You can use Dataproc for doing Apache Hadoop process, then Cloud Storage to replace the HDFS, and using Cloud Composer (built in Apache Airflow) for orchestrator."
"You have an upstream process that writes data to Cloud Storage. This data is then read by an Apache Spark job that runs on Dataproc. These jobs are run in the us-central1 region, but the data could be stored anywhere in the United States. You need to have a recovery process in place in case of a catastrophic single region failure. You need an approach with a maximum of 15 minutes of data loss (RPO=15 mins). You want to ensure that there is minimal latency when reading the data. What should you do?","1. Create two regional Cloud Storage buckets, one in the us-central1 region and one in the us-south1 region.
2. Have the upstream process write data to the us-central1 bucket. Use the Storage Transfer Service to copy data hourly from the us-central1 bucket to the us-south1 bucket.
3. Run the Dataproc cluster in a zone in the us-central1 region, reading from the bucket in that region.
4. In case of regional failure, redeploy your Dataproc clusters to the us-south1 region and read from the bucket in that region instead.","1. Create a Cloud Storage bucket in the US multi-region.
2. Run the Dataproc cluster in a zone in the us-central1 region, reading data from the US multi-region bucket.
3. In case of a regional failure, redeploy the Dataproc cluster to the us-central2 region and continue reading from the same bucket.","1. Create a dual-region Cloud Storage bucket in the us-central1 and us-south1 regions.
2. Enable turbo replication.
3. Run the Dataproc cluster in a zone in the us-central1 region, reading from the bucket in the us-south1 region.
4. In case of a regional failure, redeploy your Dataproc cluster to the us-south1 region and continue reading from the same bucket.","1. Create a dual-region Cloud Storage bucket in the us-central1 and us-south1 regions.
2. Enable turbo replication.
3. Run the Dataproc cluster in a zone in the us-central1 region, reading from the bucket in the same region.
4. In case of a regional failure, redeploy the Dataproc clusters to the us-south1 region and read from the same bucket.",,,D,229,"- Rapid Replication: Turbo replication ensures near-real-time data synchronization between regions, achieving an RPO of 15 minutes or less. - Minimal Latency: Dataproc clusters can read from the bucket in the same region, minimizing data transfer latency and optimizing performance. - Disaster Recovery: In case of regional failure, Dataproc clusters can seamlessly redeploy to the other region and continue reading from the same bucket, ensuring business continuity."
You have a streaming pipeline that ingests data from Pub/Sub in production. You need to update this streaming pipeline with improved business logic. You need to ensure that the updated pipeline reprocesses the previous two days of delivered Pub/Sub messages. What should you do? (Choose two.),Use the Pub/Sub subscription clear-retry-policy flag,Use Pub/Sub Snapshot capture two days before the deployment.,Create a new Pub/Sub subscription two days before the deployment.,Use the Pub/Sub subscription retain-acked-messages flag.,Use Pub/Sub Seek with a timestamp.,,"D,E",228,"Another way to replay messages that have been acknowledged is to seek to a timestamp. To seek to a timestamp, you must first configure the subscription to retain acknowledged messages using retain-acked-messages. If retain-acked-messages is set, Pub/Sub retains acknowledged messages for 7 days.

You only need to do this step if you intend to seek to a timestamp, not to a snapshot.

https://cloud.google.com/pubsub/docs/replay-message"
"You stream order data by using a Dataflow pipeline, and write the aggregated result to Memorystore. You provisioned a Memorystore for Redis instance with Basic Tier, 4 GB capacity, which is used by 40 clients for read-only access. You are expecting the number of read-only clients to increase significantly to a few hundred and you need to be able to support the demand. You want to ensure that read and write access availability is not impacted, and any changes you make can be deployed quickly. What should you do?",Create a new Memorystore for Redis instance with Standard Tier. Set capacity to 4 GB and read replica to No read replicas (high availability only). Delete the old instance.,Create a new Memorystore for Redis instance with Standard Tier. Set capacity to 5 GB and create multiple read replicas. Delete the old instance.,"Create a new Memorystore for Memcached instance. Set a minimum of three nodes, and memory per node to 4 GB. Modify the Dataflow pipeline and all clients to use the Memcached instance. Delete the old instance.",Create multiple new Memorystore for Redis instances with Basic Tier (4 GB capacity). Modify the Dataflow pipeline and new clients to use all instances.,,,B,227,"- Upgrading to the Standard Tier and adding read replicas is an effective way to scale and manage increased read load. - The additional capacity (5 GB) provides more space for data, and read replicas help distribute the read load across multiple instances."
"Your organization has two Google Cloud projects, project A and project B. In project A, you have a Pub/Sub topic that receives data from confidential sources. Only the resources in project A should be able to access the data in that topic. You want to ensure that project B and any future project cannot access data in the project A topic. What should you do?",Add firewall rules in project A so only traffic from the VPC in project A is permitted.,Configure VPC Service Controls in the organization with a perimeter around project A.,Use Identity and Access Management conditions to ensure that only users and service accounts in project A. can access resources in project A.,Configure VPC Service Controls in the organization with a perimeter around the VPC of project A.,,,C,226,"And I would agree with GPT. The question is about that who can do what within GCP environment. It's all about permissions and access management, not about networking."
"Your organization stores customer data in an on-premises Apache Hadoop cluster in Apache Parquet format. Data is processed on a daily basis by Apache Spark jobs that run on the cluster. You are migrating the Spark jobs and Parquet data to Google Cloud. BigQuery will be used on future transformation pipelines so you need to ensure that your data is available in BigQuery. You want to use managed services, while minimizing ETL data processing changes and overhead costs. What should you do?","Migrate your data to Cloud Storage and migrate the metadata to Dataproc Metastore (DPMS). Refactor Spark pipelines to write and read data on Cloud Storage, and run them on Dataproc Serverless.","Migrate your data to Cloud Storage and register the bucket as a Dataplex asset. Refactor Spark pipelines to write and read data on Cloud Storage, and run them on Dataproc Serverless.","Migrate your data to BigQuery. Refactor Spark pipelines to write and read data on BigQuery, and run them on Dataproc Serverless.","Migrate your data to BigLake. Refactor Spark pipelines to write and read data on Cloud Storage, and run them on Dataproc on Compute Engine.",,,A,225,"The question says ""You want to use managed services, while minimizing ETL data processing changes and overhead costs"". Dataproc is a managed service that doesn't need to refactor the data transformation Spark code you already have (you will have to refactor only the wrtie and read code), an it has a Big Query connector for future use. https://cloud.google.com/dataproc/docs/concepts/connectors/bigquery"
"A web server sends click events to a Pub/Sub topic as messages. The web server includes an eventTimestamp attribute in the messages, which is the time when the click occurred. You have a Dataflow streaming job that reads from this Pub/Sub topic through a subscription, applies some transformations, and writes the result to another Pub/Sub topic for use by the advertising department. The advertising department needs to receive each message within 30 seconds of the corresponding click occurrence, but they report receiving the messages late. Your Dataflow job's system lag is about 5 seconds, and the data freshness is about 40 seconds. Inspecting a few messages show no more than 1 second lag between their eventTimestamp and publishTime. What is the problem and what should you do?",The advertising department is causing delays when consuming the messages. Work with the advertising department to fix this.,Messages in your Dataflow job are taking more than 30 seconds to process. Optimize your job or increase the number of workers to fix this.,"Messages in your Dataflow job are processed in less than 30 seconds, but your job cannot keep up with the backlog in the Pub/Sub subscription. Optimize your job or increase the number of workers to fix this.",The web server is not pushing messages fast enough to Pub/Sub. Work with the web server team to fix this.,,,C,224,"System Lag vs. Data Freshness: System lag is low (5 seconds), indicating that individual messages are processed quickly. However, data freshness is high (40 seconds), suggesting a backlog in the pipeline. Not Advertising's Fault: The issue is upstream of their consumption, as they're already receiving delayed messages. Not Web Server's Fault: The lag between eventTimestamp and publishTime is minimal (1 second), meaning the server is publishing messages promptly"
You are building an ELT solution in BigQuery by using Dataform. You need to perform uniqueness and null value checks on your final tables. What should you do to efficiently integrate these checks into your pipeline?,Build BigQuery user-defined functions (UDFs).,Create Dataplex data quality tasks.,Build Dataform assertions into your code.,Write a Spark-based stored procedure.,,,C,223,"- Dataform provides a feature called ""assertions,"" which are essentially SQL-based tests that you can define to verify the quality of your data. - Assertions in Dataform are a built-in way to perform data quality checks, including checking for uniqueness and null values in your tables."
"You have a variety of files in Cloud Storage that your data science team wants to use in their models. Currently, users do not have a method to explore, cleanse, and validate the data in Cloud Storage. You are looking for a low code solution that can be used by your data science team to quickly cleanse and explore data within Cloud Storage. What should you do?",Provide the data science team access to Dataflow to create a pipeline to prepare and validate the raw data and load data into BigQuery for data exploration.,Create an external table in BigQuery and use SQL to transform the data as necessary. Provide the data science team access to the external tables to explore the raw data.,Load the data into BigQuery and use SQL to transform the data as necessary. Provide the data science team access to staging tables to explore the raw data.,"Provide the data science team access to Dataprep to prepare, validate, and explore the data within Cloud Storage.",,,D,222,"- Dataprep is a serverless, no-code data preparation tool that allows users to visually explore, cleanse, and prepare data for analysis. - It's designed for business analysts, data scientists, and others who want to work with data without writing code. - Dataprep can directly access and transform data in Cloud Storage, making it a suitable choice for a team that prefers a low-code, user-friendly solution."
"You store and analyze your relational data in BigQuery on Google Cloud with all data that resides in US regions. You also have a variety of object stores across Microsoft Azure and Amazon Web Services (AWS), also in US regions. You want to query all your data in BigQuery daily with as little movement of data as possible. What should you do?",Use BigQuery Data Transfer Service to load files from Azure and AWS into BigQuery.,Create a Dataflow pipeline to ingest files from Azure and AWS to BigQuery.,Load files from AWS and Azure to Cloud Storage with Cloud Shell gsutil rsync arguments.,Use the BigQuery Omni functionality and BigLake tables to query files in Azure and AWS.,,,D,221,"BigQuery Omni allows you to query data in Azure and AWS object stores directly without physically moving it to BigQuery, reducing data transfer costs and delays. BigLake Tables: Provide a unified view of both BigQuery tables and external object storage files, enabling seamless querying across multi-cloud data."
You are migrating your on-premises data warehouse to BigQuery. One of the upstream data sources resides on a MySQL. database that runs in your on-premises data center with no public IP addresses. You want to ensure that the data ingestion into BigQuery is done securely and does not go through the public internet. What should you do?,Update your existing on-premises ETL tool to write to BigQuery by using the BigQuery Open Database Connectivity (ODBC) driver. Set up the proxy parameter in the simba.googlebigqueryodbc.ini file to point to your data center’s NAT gateway.,Use Datastream to replicate data from your on-premises MySQL database to BigQuery. Set up Cloud Interconnect between your on-premises data center and Google Cloud. Use Private connectivity as the connectivity method and allocate an IP address range within your VPC network to the Datastream connectivity configuration. Use Server-only as the encryption type when setting up the connection profile in Datastream.,Use Datastream to replicate data from your on-premises MySQL database to BigQuery. Use Forward-SSH tunnel as the connectivity method to establish a secure tunnel between Datastream and your on-premises MySQL database through a tunnel server in your on-premises data center. Use None as the encryption type when setting up the connection profile in Datastream.,Use Datastream to replicate data from your on-premises MySQL database to BigQuery. Gather Datastream public IP addresses of the Google Cloud region that will be used to set up the stream. Add those IP addresses to the firewall allowlist of your on-premises data center. Use IP Allowlisting as the connectivity method and Server-only as the encryption type when setting up the connection profile in Datastream.,,,B,220,"- Datastream is a serverless change data capture and replication service, which can be used to replicate data changes from MySQL to BigQuery. - Using Cloud Interconnect provides a private, secure connection between your on-premises environment and Google Cloud ==> This method ensures that data doesn't go through the public internet and is a recommended approach for secure, large-scale data migrations. - Setting up private connectivity with Datastream allows for secure and direct data transfer."
You orchestrate ETL pipelines by using Cloud Composer. One of the tasks in the Apache Airflow directed acyclic graph (DAG) relies on a third-party service. You want to be notified when the task does not succeed. What should you do?,Assign a function with notification logic to the on_retry_callback parameter for the operator responsible for the task at risk.,Configure a Cloud Monitoring alert on the sla_missed metric associated with the task at risk to trigger a notification.,Assign a function with notification logic to the on_failure_callback parameter tor the operator responsible for the task at risk,Assign a function with notification logic to the sla_miss_callback parameter for the operator responsible for the task at risk.,,,C,219,"- The on_failure_callback is a function that gets called when a task fails. - Assigning a function with notification logic to this parameter is a direct way to handle task failures. - When the task fails, this function can trigger a notification, making it an appropriate solution for the need to be alerted on task failures."
You have a Cloud SQL for PostgreSQL instance in Region’ with one read replica in Region2 and another read replica in Region3. An unexpected event in Region’ requires that you perform disaster recovery by promoting a read replica in Region2. You need to ensure that your application has the same database capacity available before you switch over the connections. What should you do?,Enable zonal high availability on the primary instance. Create a new read replica in a new region.,Create a cascading read replica from the existing read replica in Region3.,"Create two new read replicas from the new primary instance, one in Region3 and one in a new region.","Create a new read replica in Region1, promote the new read replica to be the primary instance, and enable zonal high availability.",,,C,218,"The best option here is C. Create two new read replicas from the new primary instance, one in Region3 and one in a new region. Here's the breakdown: Capacity Restoration: Promoting the Region2 replica makes it the new primary. You need to replicate from this new primary to maintain redundancy and capacity. Creating two replicas (Region3, new region) accomplishes this. Geographic Distribution: Distributing replicas across regions ensures availability if another regional event occurs. Speed: Creating new replicas from the promoted primary is likely faster than promoting another existing replica."
"You have a BigQuery table that contains customer data, including sensitive information such as names and addresses. You need to share the customer data with your data analytics and consumer support teams securely. The data analytics team needs to access the data of all the customers, but must not be able to access the sensitive data. The consumer support team needs access to all data columns, but must not be able to access customers that no longer have active contracts. You enforced these requirements by using an authorized dataset and policy tags. After implementing these steps, the data analytics team reports that they still have access to the sensitive columns. You need to ensure that the data analytics team does not have access to restricted data. What should you do? (Choose two.)",Create two separate authorized datasets; one for the data analytics team and another for the consumer support team.,Ensure that the data analytics team members do not have the Data Catalog Fine-Grained Reader role for the policy tags.,Replace the authorized dataset with an authorized view. Use row-level security and apply filter_expression to limit data access.,Remove the bigquery.dataViewer role from the data analytics team on the authorized datasets.,Enforce access control in the policy tag taxonomy.,,"B,E",217,B - It will ensure they don't have access to secure columns E- It will allow to enforce column level security Ref - https://cloud.google.com/bigquery/docs/column-level-security-intro
"You are developing an Apache Beam pipeline to extract data from a Cloud SQL instance by using JdbcIO. You have two projects running in Google Cloud. The pipeline will be deployed and executed on Dataflow in Project A. The Cloud SQL. instance is running in Project B and does not have a public IP address. After deploying the pipeline, you noticed that the pipeline failed to extract data from the Cloud SQL instance due to connection failure. You verified that VPC Service Controls and shared VPC are not in use in these projects. You want to resolve this error while ensuring that the data does not go through the public internet. What should you do?",Set up VPC Network Peering between Project A and Project B. Add a firewall rule to allow the peered subnet range to access all instances on the network.,Turn off the external IP addresses on the Dataflow worker. Enable Cloud NAT in Project A.,Add the external IP addresses of the Dataflow worker as authorized networks in the Cloud SQL instance.,Set up VPC Network Peering between Project A and Project B. Create a Compute Engine instance without external IP address in Project B on the peered subnet to serve as a proxy server to the Cloud SQL database.,,,A,216,A - The requirement for a proxy is un-necessary: https://cloud.google.com/sql/docs/mysql/private-ip#multiple_vpc_connectivity
You are administering a BigQuery dataset that uses a customer-managed encryption key (CMEK). You need to share the dataset with a partner organization that does not have access to your CMEK. What should you do?,Provide the partner organization a copy of your CMEKs to decrypt the data.,Export the tables to parquet files to a Cloud Storage bucket and grant the storageinsights.viewer role on the bucket to the partner organization,Copy the tables you need to share to a dataset without CMEKs. Create an Analytics Hub listing for this dataset.,Create an authorized view that contains the CMEK to decrypt the data when accessed.,,,C,215,"- Create a copy of the necessary tables into a new dataset that doesn't use CMEK, ensuring the data is accessible without requiring the partner to have access to the encryption key. - Analytics Hub can then be used to share this data securely and efficiently with the partner organization, maintaining control and governance over the shared data."
"You have a Standard Tier Memorystore for Redis instance deployed in a production environment. You need to simulate a Redis instance failover in the most accurate disaster recovery situation, and ensure that the failover has no impact on production data. What should you do?",Create a Standard Tier Memorystore for Redis instance in the development environment. Initiate a manual failover by using the limited-data-loss data protection mode.,Create a Standard Tier Memorystore for Redis instance in a development environment. Initiate a manual failover by using the force-data-loss data protection mode.,Increase one replica to Redis instance in production environment. Initiate a manual failover by using the force-data-loss data protection mode.,Initiate a manual failover by using the limited-data-loss data protection mode to the Memorystore for Redis instance in the production environment.,,,B,214,"The best option is B - Create a Standard Tier Memorystore for Redis instance in a development environment. Initiate a manual failover by using the force-data-loss data protection mode. The key points are: • The failover should be tested in a separate development environment, not production, to avoid impacting real data. • The force-data-loss mode will simulate a full failover and restart, which is the most accurate test of disaster recovery. • Limited-data-loss mode only fails over reads which does not fully test write capabilities. • Increasing replicas in production and failing over (C) risks losing real production data. • Failing over production (D) also risks impacting real data and traffic. So option B isolates the test from production and uses the most rigorous failover mode to fully validate disaster recovery capabilities."
You are building a data lake on Google Cloud for your Internet of Things (IoT) application. The IoT application has millions of sensors that are constantly streaming structured and unstructured data to your backend in the cloud. You want to build a highly available and resilient architecture based on Google-recommended practices. What should you do?,"Stream data to Pub/Sub, and use Dataflow to send data to Cloud Storage.","Stream data to Pub/Sub, and use Storage Transfer Service to send data to BigQuery.","Stream data to Dataflow, and use Dataprep by Trifacta to send data to Bigtable.","Stream data to Dataflow, and use Storage Transfer Service to send data to BigQuery.",,,A,213,"A. Streaming data to Pub/Sub allows you to decouple the ingestion of data from the processing and storage, providing a scalable and reliable message queue that can handle the high volume of data coming from millions of sensors. Using Dataflow to consume data from Pub/Sub and send it to Cloud Storage allows for real-time data processing and storage. Dataflow is a fully managed service for processing data in real-time or batch mode, making it an ideal choice for handling the constant stream of data from IoT sensors. Storing data in Cloud Storage offers high durability and availability, providing a robust foundation for building a data lake. Cloud Storage is a scalable object storage service that can handle large volumes of structured and unstructured data, making it well-suited for the IoT application's data requirements."
You are troubleshooting your Dataflow pipeline that processes data from Cloud Storage to BigQuery. You have discovered that the Dataflow worker nodes cannot communicate with one another. Your networking team relies on Google Cloud network tags to define firewall rules. You need to identify the issue while following Google-recommended networking security practices. What should you do?,Determine whether your Dataflow pipeline has a custom network tag set.,Determine whether there is a firewall rule set to allow traffic on TCP ports 12345 and 12346 for the Dataflow network tag.,Determine whether there is a firewall rule set to allow traffic on TCP ports 12345 and 12346 on the subnet used by Dataflow workers.,Determine whether your Dataflow pipeline is deployed with the external IP address option enabled.,,,B,212,"The best approach would be to check if there is a firewall rule allowing traffic on TCP ports 12345 and 12346 for the Dataflow network tag. Dataflow uses TCP ports 12345 and 12346 for communication between worker nodes. Using network tags and associated firewall rules is a Google-recommended security practice for controlling access between Compute Engine instances like Dataflow workers. So the key things to check would be: 1. Ensure your Dataflow pipeline is using the Dataflow network tag on the worker nodes. This tag is applied by default unless overridden. 2. Check if there is a firewall rule allowing TCP 12345 and 12346 ingress and egress traffic for instances with the Dataflow network tag. If not, add the rule. Options A, C and D relate to other networking aspects but do not directly address the Google recommended practice of using network tags and firewall rules."
"You are using BigQuery with a multi-region dataset that includes a table with the daily sales volumes. This table is updated multiple times per day. You need to protect your sales table in case of regional failures with a recovery point objective (RPO) of less than 24 hours, while keeping costs to a minimum. What should you do?",Schedule a daily export of the table to a Cloud Storage dual or multi-region bucket.,Schedule a daily copy of the dataset to a backup region.,Schedule a daily BigQuery snapshot of the table.,Modify ETL job to load the data into both the current and another backup region.,,,A,211,"Based on the information provided and the need to avoid data loss in the case of a hard regional failure in BigQuery, which could result in the destruction of all data in that region, the focus should be on creating backups in a geographically distinct region. Considering this scenario, the most suitable option would be Option A"
"You are designing a data mesh on Google Cloud with multiple distinct data engineering teams building data products. The typical data curation design pattern consists of landing files in Cloud Storage, transforming raw data in Cloud Storage and BigQuery datasets, and storing the final curated data product in BigQuery datasets. You need to configure Dataplex to ensure that each team can access only the assets needed to build their data products. You also need to ensure that teams can easily share the curated data product. What should you do?","1. Create a single Dataplex virtual lake and create a single zone to contain landing, raw, and curated data.
2. Provide each data engineering team access to the virtual lake.","1. Create a single Dataplex virtual lake and create a single zone to contain landing, raw, and curated data.
2. Build separate assets for each data product within the zone.
3. Assign permissions to the data engineering teams at the zone level.","1. Create a Dataplex virtual lake for each data product, and create a single zone to contain landing, raw, and curated data.
2. Provide the data engineering teams with full access to the virtual lake assigned to their data product.","1. Create a Dataplex virtual lake for each data product, and create multiple zones for landing, raw, and curated data.
2. Provide the data engineering teams with full access to the virtual lake assigned to their data product.",,,D,210,"The best approach is to create a Dataplex virtual lake for each data product, with multiple zones for landing, raw, and curated data. Then provide the data engineering teams with access only to the zones they need within the virtual lake assigned to their product. To enable teams to easily share curated data products, you should use cross-lake sharing in Dataplex. This allows curated zones to be shared across virtual lakes while maintaining data isolation for other zones."
"A shipping company has live package-tracking data that is sent to an Apache Kafka stream in real time. This is then loaded into BigQuery. Analysts in your company want to query the tracking data in BigQuery to analyze geospatial trends in the lifecycle of a package. The table was originally created with ingest-date partitioning. Over time, the query processing time has increased. You need to copy all the data to a new clustered table. What should you do?",Re-create the table using data partitioning on the package delivery date.,Implement clustering in BigQuery on the package-tracking ID column.,Implement clustering in BigQuery on the ingest date column.,Tier older data onto Cloud Storage files and create a BigQuery table using Cloud Storage as an external data source.,,,B,209,Implement clustering in BigQuery on the package-tracking ID column.
"A live TV show asks viewers to cast votes using their mobile phones. The event generates a large volume of data during a 3-minute period. You are in charge of the ""Voting infrastructure"" and must ensure that the platform can handle the load and that all votes are processed. You must display partial results while voting is open. After voting closes, you need to count the votes exactly once while optimizing cost. What should you do?",Create a Memorystore instance with a high availability (HA) configuration.,Create a Cloud SQL for PostgreSQL database with high availability (HA) configuration and multiple read replicas.,Write votes to a Pub/Sub topic and have Cloud Functions subscribe to it and write votes to BigQuery.,Write votes to a Pub/Sub topic and load into both Bigtable and BigQuery via a Dataflow pipeline. Query Bigtable for real-time results and BigQuery for later analysis. Shut down the Bigtable instance when voting concludes.,,,D,208,Write votes to a Pub/Sub topic and load into both Bigtable and BigQuery via a Dataflow pipeline. Query Bigtable for real-time results and BigQuery for later analysis. Shut down the Bigtable instance when voting concludes.
"Your DevOps team uses Packer to build Compute Engine images by using this process:

1. Create an ephemeral Compute Engine VM.
2. Copy a binary from a Cloud Storage bucket to the VM's file system.
3. Update the VM's package manager.
4. Install external packages from the internet onto the VM.

Your security team just enabled the organizational policy, constraints/ compute.vmExternalIpAccess, to restrict the usage of public IP Addresses on VMs. In response, your DevOps team updated their scripts to remove public IP addresses on the Compute Engine VMs; however, the build pipeline is failing due to connectivity issues.

What should you do? (Choose two.)",Provision an HTTP load balancer with the VM in an unmanaged instance group to allow inbound connections from the internet to your VM.,Provision a Cloud NAT instance in the same VPC and region as the Compute Engine VM.,Enable Private Google Access on the subnet that the Compute Engine VM is deployed within.,Update the VPC routes to allow traffic to and from the internet.,Provision a Cloud VPN tunnel in the same VPC and region as the Compute Engine VM.,,"B,C",207,"Provision a Cloud NAT instance (Option B): Cloud NAT allows your Compute Engine instances without public IP addresses to access the internet while preserving the security restrictions imposed by your organizational policy. By provisioning a Cloud NAT instance in the same VPC and region as your Compute Engine VMs, you enable outbound connectivity for these VMs. Enable Private Google Access (Option C): Enabling Private Google Access on the subnet where your Compute Engine VMs are deployed allows these instances to access Google Cloud services over the private IP address range. This can help with accessing external resources needed during the Packer image build process without exposing the VMs to the public internet."
You need ads data to serve AI models and historical data for analytics. Longtail and outlier data points need to be identified. You want to cleanse the data in near-real time before running it through AI models. What should you do?,"Use Cloud Storage as a data warehouse, shell scripts for processing, and BigQuery to create views for desired datasets.","Use Dataflow to identify longtail and outlier data points programmatically, with BigQuery as a sink.","Use BigQuery to ingest, prepare, and then analyze the data, and then run queries to create views.","Use Cloud Composer to identify longtail and outlier data points, and then output a usable dataset to BigQuery.",,,B,206,"Dataflow for Real-Time Processing: Dataflow allows you to process data in near-real time, making it well-suited for identifying longtail and outlier data points as they occur. You can use Dataflow to implement custom data cleansing and outlier detection algorithms that operate on streaming data. BigQuery as a Sink: Using BigQuery as a sink allows you to store the cleaned and processed data efficiently for further analysis or use in AI models. Dataflow can write the cleaned data to BigQuery tables, enabling seamless integration with downstream processes."
"You have a data processing application that runs on Google Kubernetes Engine (GKE). Containers need to be launched with their latest available configurations from a container registry. Your GKE nodes need to have GPUs, local SSDs, and 8 Gbps bandwidth. You want to efficiently provision the data processing infrastructure and manage the deployment process. What should you do?","Use Compute Engine startup scripts to pull container images, and use gcloud commands to provision the infrastructure.",Use Cloud Build to schedule a job using Terraform build to provision the infrastructure and launch with the most current container images.,"Use GKE to autoscale containers, and use gcloud commands to provision the infrastructure.","Use Dataflow to provision the data pipeline, and use Cloud Scheduler to run the job.",,,B,205,- Dataflow is a fully managed service for stream and batch data processing and is well-suited for real-time data processing tasks like identifying longtail and outlier data points. - Using BigQuery as a sink allows to efficiently store the cleansed and processed data for further analysis and serving it to AI models.
You want to create a machine learning model using BigQuery ML and create an endpoint for hosting the model using Vertex AI. This will enable the processing of continuous streaming data in near-real time from multiple vendors. The data may contain invalid values. What should you do?,"Create a new BigQuery dataset and use streaming inserts to land the data from multiple vendors. Configure your BigQuery ML model to use the ""ingestion"" dataset as the framing data.",Use BigQuery streaming inserts to land the data from multiple vendors where your BigQuery dataset ML model is deployed.,Create a Pub/Sub topic and send all vendor data to it. Connect a Cloud Function to the topic to process the data and store it in BigQuery.,Create a Pub/Sub topic and send all vendor data to it. Use Dataflow to process and sanitize the Pub/Sub data and stream it to BigQuery.,,,D,204,Option D -Dataflow provides a scalable and flexible way to process and clean the incoming data in real-time before loading it into BigQuery.
Your company completed the acquisition of a startup and is now merging the IT systems of both companies. The startup had a production Google Cloud project in their organization. You need to move this project into your organization and ensure that the project is billed to your organization. You want to accomplish this task with minimal effort. What should you do?,Use the projects.move method to move the project to your organization. Update the billing account of the project to that of your organization.,"Ensure that you have an Organization Administrator Identity and Access Management (IAM) role assigned to you in both organizations. Navigate to the Resource Manager in the startup’s Google Cloud organization, and drag the project to your company's organization.","Create a Private Catalog for the Google Cloud Marketplace, and upload the resources of the startup's production project to the Catalog. Share the Catalog with your organization, and deploy the resources in your company’s project.","Create an infrastructure-as-code template for all resources in the project by using Terraform, and deploy that template to a new project in your organization. Delete the project from the startup’s Google Cloud organization.",,,A,203,"Option A is correct as it suggests using the ""projects.move"" method provided by Google Cloud to move the project from the startup's organization to your organization. This method allows you to transfer the ownership and control of a project to another organization. By moving the project, you can ensure that it is under your organization's management. While the other options contain elements that may be relevant in certain scenarios, they do not directly address the requirement of moving the project and ensuring billing to your organization."
"Your platform on your on-premises environment generates 100 GB of data daily, composed of millions of structured JSON text files. Your on-premises environment cannot be accessed from the public internet. You want to use Google Cloud products to query and explore the platform data. What should you do?",Use Cloud Scheduler to copy data daily from your on-premises environment to Cloud Storage. Use the BigQuery Data Transfer Service to import data into BigQuery.,Use a Transfer Appliance to copy data from your on-premises environment to Cloud Storage. Use the BigQuery Data Transfer Service to import data into BigQuery.,Use Transfer Service for on-premises data to copy data from your on-premises environment to Cloud Storage. Use the BigQuery Data Transfer Service to import data into BigQuery.,Use the BigQuery Data Transfer Service dataset copy to transfer all data into BigQuery.,,,C,202,"Therefore, the correct option is C. Use Transfer Service for on-premises data to copy data from your on-premises environment to Cloud Storage. Use the BigQuery Data Transfer Service to import data into BigQuery. Option A is incorrect because Cloud Scheduler is not designed for data transfer, but rather for scheduling the execution of Cloud Functions, Cloud Run, or App Engine applications. Option B is incorrect because Transfer Appliance is designed for large-scale data transfers from on-premises environments to Google Cloud and is not suitable for transferring data on a daily basis. Option D is also incorrect because the BigQuery Data Transfer Service dataset copy feature is designed for copying datasets between BigQuery projects and not suitable for copying data from on-premises environments to BigQuery."
"You need to migrate a Redis database from an on-premises data center to a Memorystore for Redis instance. You want to follow Google-recommended practices and perform the migration for minimal cost, time and effort. What should you do?","Make an RDB backup of the Redis database, use the gsutil utility to copy the RDB file into a Cloud Storage bucket, and then import the RDB file into the Memorystore for Redis instance.",Make a secondary instance of the Redis database on a Compute Engine instance and then perform a live cutover.,Create a Dataflow job to read the Redis database from the on-premises data center and write the data to a Memorystore for Redis instance.,Write a shell script to migrate the Redis data and create a new Memorystore for Redis instance.,,,A,201,"Make an RDB backup of the Redis database, use the gsutil utility to copy the RDB file into a Cloud Storage bucket, and then import the RDB file into the Memorystore for Redis instance. The import and export feature uses the native RDB snapshot feature of Redis to import data into or export data out of a Memorystore for Redis instance. The use of the native RDB format prevents lock-in and makes it very easy to move data within Google Cloud or outside of Google Cloud. Import and export uses Cloud Storage buckets to store RDB files. Reference: https://cloud.google.com/memorystore/docs/redis/import-export-overview"
"Government regulations in the banking industry mandate the protection of clients' personally identifiable information (PII). Your company requires PII to be access controlled, encrypted, and compliant with major data protection standards. In addition to using Cloud Data Loss Prevention (Cloud DLP), you want to follow
Google-recommended practices and use service accounts to control access to PII. What should you do?","Assign the required Identity and Access Management (IAM) roles to every employee, and create a single service account to access project resources.","Use one service account to access a Cloud SQL database, and use separate service accounts for each human user.",Use Cloud Storage to comply with major data protection standards. Use one service account shared by all users.,Use Cloud Storage to comply with major data protection standards. Use multiple service accounts attached to IAM groups to grant the appropriate access to each group.,,,D,200,"for A: please refer to this link below which suggests ""Sharing a single service account across multiple applications can complicate the management of the service account"" - meaning it's not a best practice. https://cloud.google.com/iam/docs/best-practices-service-accounts#single-purpose Also, what if we have hundreds of users, does it really make sense to manage each user's IAM individually? for D: it's indeed not one of the best practices but I believe it's much more managable and better than A"
You are using BigQuery and Data Studio to design a customer-facing dashboard that displays large quantities of aggregated data. You expect a high volume of concurrent users. You need to optimize the dashboard to provide quick visualizations with minimal latency. What should you do?,Use BigQuery BI Engine with materialized views.,Use BigQuery BI Engine with logical views.,Use BigQuery BI Engine with streaming data.,Use BigQuery BI Engine with authorized views.,,,A,199,"https://cloud.google.com/bigquery/docs/materialized-views-intro In BigQuery, materialized views are precomputed views that periodically cache the results of a query for increased performance and efficiency. BigQuery leverages precomputed results from materialized views and whenever possible reads only delta changes from the base tables to compute up-to-date results. Materialized views can be queried directly or can be used by the BigQuery optimizer to process queries to the base tables. Queries that use materialized views are generally faster and consume fewer resources than queries that retrieve the same data only from the base tables. Materialized views can significantly improve the performance of workloads that have the characteristic of common and repeated queries."
You are implementing workflow pipeline scheduling using open source-based tools and Google Kubernetes Engine (GKE). You want to use a Google managed service to simplify and automate the task. You also want to accommodate Shared VPC networking considerations. What should you do?,Use Dataflow for your workflow pipelines. Use Cloud Run triggers for scheduling.,Use Dataflow for your workflow pipelines. Use shell scripts to schedule workflows.,Use Cloud Composer in a Shared VPC configuration. Place the Cloud Composer resources in the host project.,Use Cloud Composer in a Shared VPC configuration. Place the Cloud Composer resources in the service project.,,,D,198,"Shared VPC requires that you designate a host project to which networks and subnetworks belong and a service project, which is attached to the host project. When Cloud Composer participates in a Shared VPC, the Cloud Composer environment is in the service project. Reference: https://cloud.google.com/composer/docs/how-to/managing/configuring-shared-vpc"
"You are designing a system that requires an ACID-compliant database. You must ensure that the system requires minimal human intervention in case of a failure.
What should you do?",Configure a Cloud SQL for MySQL instance with point-in-time recovery enabled.,Configure a Cloud SQL for PostgreSQL instance with high availability enabled.,Configure a Bigtable instance with more than one cluster.,Configure a BigQuery table with a multi-region configuration.,,,B,197,"We exclude [C[ as non ACID and [D] for being invalid (location is configured on Dataset level, not Table). Then, let's focus on ""minimal human intervention in case of a failure"" requirement in order to eliminate one answer among [A] and [B]. Basically, we have to compare point-in-time recovery with high availability. It doesn't matter whether it's about MySQL or PostgreSQL since both databases support those features. - Point-in-time recovery logs are created automatically, but restoring an instance in case of failure requires manual steps (described here: https://cloud.google.com/sql/docs/mysql/backup-recovery/pitr#perform-pitr) - High availability, in case of failure requires no human intervention: ""If an HA-configured instance becomes unresponsive, Cloud SQL automatically switches to serving data from the standby instance."" (from https://cloud.google.com/sql/docs/postgres/high-availability#failover-overview)"
You have 15 TB of data in your on-premises data center that you want to transfer to Google Cloud. Your data changes weekly and is stored in a POSIX-compliant source. The network operations team has granted you 500 Mbps bandwidth to the public internet. You want to follow Google-recommended practices to reliably transfer your data to Google Cloud on a weekly basis. What should you do?,Use Cloud Scheduler to trigger the gsutil command. Use the -m parameter for optimal parallelism.,"Use Transfer Appliance to migrate your data into a Google Kubernetes Engine cluster, and then configure a weekly transfer job.","Install Storage Transfer Service for on-premises data in your data center, and then configure a weekly transfer job.","Install Storage Transfer Service for on-premises data on a Google Cloud virtual machine, and then configure a weekly transfer job.",,,C,196,"https://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#storage-transfer-service-for-large-transfers-of-on-premises-data Like gsutil, Storage Transfer Service for on-premises data enables transfers from network file system (NFS) storage to Cloud Storage. Although gsutil can support small transfer sizes (up to 1 TB), Storage Transfer Service for on-premises data is designed for large-scale transfers (up to petabytes of data, billions of files)."
"Your company wants to be able to retrieve large result sets of medical information from your current system, which has over 10 TBs in the database, and store the data in new tables for further query. The database must have a low-maintenance architecture and be accessible via SQL. You need to implement a cost-effective solution that can support data analytics for large result sets. What should you do?","Use Cloud SQL, but first organize the data into tables. Use JOIN in queries to retrieve data.",Use BigQuery as a data warehouse. Set output destinations for caching large queries.,Use a MySQL cluster installed on a Compute Engine managed instance group for scalability.,Use Cloud Spanner to replicate the data across regions. Normalize the data in a series of tables.,,,B,195,Use BigQuery as a data warehouse. Set output destinations for caching large queries.
An online brokerage company requires a high volume trade processing architecture. You need to create a secure queuing system that triggers jobs. The jobs will run in Google Cloud and call the company's Python API to execute trades. You need to efficiently implement a solution. What should you do?,Use a Pub/Sub push subscription to trigger a Cloud Function to pass the data to the Python API.,Write an application hosted on a Compute Engine instance that makes a push subscription to the Pub/Sub topic.,Write an application that makes a queue in a NoSQL database.,Use Cloud Composer to subscribe to a Pub/Sub topic and call the Python API.,,,A,194,An online brokerage company requires a high volume trade processing architecture. You need to create a secure queuing system that triggers jobs. The jobs will run in Google Cloud and call the company's Python API to execute trades. You need to efficiently implement a solution. What should you do?
"An aerospace company uses a proprietary data format to store its flight data. You need to connect this new data source to BigQuery and stream the data into
BigQuery. You want to efficiently import the data into BigQuery while consuming as few resources as possible. What should you do?",Write a shell script that triggers a Cloud Function that performs periodic ETL batch jobs on the new data source.,"Use a standard Dataflow pipeline to store the raw data in BigQuery, and then transform the format later when the data is used.",Use Apache Hive to write a Dataproc job that streams the data into BigQuery in CSV format.,Use an Apache Beam custom connector to write a Dataflow pipeline that streams the data into BigQuery in Avro format.,,,,193,"The key reasons: • Dataflow provides managed resource scaling for efficient stream processing • Avro format has schema evolution capabilities and efficient serialization for flight telemetry data • Apache Beam connectors avoid having to write much code to integrate proprietary data sources • Streaming inserts data efficiently compared to periodic batch jobs In contrast, option A uses Cloud Functions which lack native streaming capabilities. Option B stores data in less efficient JSON format. Option C uses Dataproc which requires manual cluster management. So leveraging Dataflow + Avro + Beam provides the most efficient way to stream proprietary flight data into BigQuery while using minimal resources."
"You are implementing a chatbot to help an online retailer streamline their customer service. The chatbot must be able to respond to both text and voice inquiries.
You are looking for a low-code or no-cade option, and you want to be able to easily train the chatbot to provide answers to keywords. What should you do?",Use the Cloud Speech-to-Text API to build a Python application in App Engine.,Use the Cloud Speech-to-Text API to build a Python application in a Compute Engine instance.,Use Dialogflow for simple queries and the Cloud Speech-to-Text API for complex queries.,"Use Dialogflow to implement the chatbot, defining the intents based on the most common queries collected.",,,D,192,"https://cloud.google.com/dialogflow/docs Dialogflow is a natural language understanding platform that makes it easy to design and integrate a conversational user interface into your mobile app, web application, device, bot, interactive voice response system, and so on. Using Dialogflow, you can provide new and engaging ways for users to interact with your product. Dialogflow can analyze multiple types of input from your customers, including text or audio inputs (like from a phone or voice recording). It can also respond to your customers in a couple of ways, either through text or with synthetic speech."
"You are developing a new deep learning model that predicts a customer's likelihood to buy on your ecommerce site. After running an evaluation of the model against both the original training data and new test data, you find that your model is overfitting the data. You want to improve the accuracy of the model when predicting new data. What should you do?","Increase the size of the training dataset, and increase the number of input features.","Increase the size of the training dataset, and decrease the number of input features.","Reduce the size of the training dataset, and increase the number of input features.","Reduce the size of the training dataset, and decrease the number of input features.",,,B,191,There 2 parts and they are relevant to each other 1. Overfit is fixed by decreasing the number of input features (select only essential features) 2. Accuracy is improved by increasing the amount of training data examples.
"You are loading CSV files from Cloud Storage to BigQuery. The files have known data quality issues, including mismatched data types, such as STRINGs and
INT64s in the same column, and inconsistent formatting of values such as phone numbers or addresses. You need to create the data pipeline to maintain data quality and perform the required cleansing and transformation. What should you do?",Use Data Fusion to transform the data before loading it into BigQuery.,"Use Data Fusion to convert the CSV files to a self-describing data format, such as AVRO, before loading the data to BigQuery.","Load the CSV files into a staging table with the desired schema, perform the transformations with SQL, and then write the results to the final destination table.","Create a table with the desired schema, load the CSV files into the table, and perform the transformations in place using SQL.",,,A,190,"Data Fusion's advantages: Visual interface: Offers a user-friendly interface for designing data pipelines without extensive coding, making it accessible to a wider range of users. Built-in transformations: Includes a wide range of pre-built transformations to handle common data quality issues, such as: Data type conversions Data cleansing (e.g., removing invalid characters, correcting formatting) Data validation (e.g., checking for missing values, enforcing constraints) Data enrichment (e.g., adding derived fields, joining with other datasets) Custom transformations: Allows for custom transformations using SQL or Java code for more complex cleaning tasks. Scalability: Can handle large datasets efficiently, making it suitable for processing CSV files with potential data quality issues. Integration with BigQuery: Integrates seamlessly with BigQuery, allowing for direct loading of transformed data."
You need to migrate 1 PB of data from an on-premises data center to Google Cloud. Data transfer time during the migration should take only a few hours. You want to follow Google-recommended practices to facilitate the large data transfer over a secure connection. What should you do?,"Establish a Cloud Interconnect connection between the on-premises data center and Google Cloud, and then use the Storage Transfer Service.","Use a Transfer Appliance and have engineers manually encrypt, decrypt, and verify the data.","Establish a Cloud VPN connection, start gcloud compute scp jobs in parallel, and run checksums to verify the data.","Reduce the data into 3 TB batches, transfer the data using gsutil, and run checksums to verify the data.",,,A,189,"Cloud Interconnect provides a dedicated private connection between on-prem and Google Cloud for high bandwidth (up to 100 Gbps) and low latency. This facilitates large, fast data transfers. Storage Transfer Service supports parallel data transfers over Cloud Interconnect. It can transfer petabyte-scale datasets faster by transferring objects in parallel. Storage Transfer Service uses HTTPS encryption in transit and at rest by default for secure data transfers. It follows Google-recommended practices for large data migrations vs ad hoc methods like gsutil or scp. The other options would take too long for a 1 PB transfer (VPN capped at 3 Gbps, manual transfers) or introduce extra steps like batching and checksums. Cloud Interconnect + Storage Transfer is the recommended Google solution."
"Your startup has a web application that currently serves customers out of a single region in Asia. You are targeting funding that will allow your startup to serve customers globally. Your current goal is to optimize for cost, and your post-funding goal is to optimize for global presence and performance. You must use a native
JDBC driver. What should you do?","Use Cloud Spanner to configure a single region instance initially, and then configure multi-region Cloud Spanner instances after securing funding.","Use a Cloud SQL for PostgreSQL highly available instance first, and Bigtable with US, Europe, and Asia replication after securing funding.","Use a Cloud SQL for PostgreSQL zonal instance first, and Bigtable with US, Europe, and Asia after securing funding.","Use a Cloud SQL for PostgreSQL zonal instance first, and Cloud SQL for PostgreSQL with highly available configuration after securing funding.",,,A,188,"A. Use Cloud Spanner to configure a single region instance initially, and then configure multi-region Cloud Spanner instances after securing funding. When you create a Cloud Spanner instance, you must configure it as either regional (that is, all the resources are contained within a single Google Cloud region) or multi-region (that is, the resources span more than one region). You can change the instance configuration to multi-regional (or global) at anytime."
You need to manage a Cloud Spanner instance for best query performance. Your instance in production runs in a single Google Cloud region. You need to improve performance in the shortest amount of time. You want to follow Google best practices for service configuration. What should you do?,"Create an alert in Cloud Monitoring to alert when the percentage of high priority CPU utilization reaches 45%. If you exceed this threshold, add nodes to your instance.","Create an alert in Cloud Monitoring to alert when the percentage of high priority CPU utilization reaches 45%. Use database query statistics to identify queries that result in high CPU usage, and then rewrite those queries to optimize their resource usage.","Create an alert in Cloud Monitoring to alert when the percentage of high priority CPU utilization reaches 65%. If you exceed this threshold, add nodes to your instance.","Create an alert in Cloud Monitoring to alert when the percentage of high priority CPU utilization reaches 65%. Use database query statistics to identify queries that result in high CPU usage, and then rewrite those queries to optimize their resource usage.",,,C,187,"looks correct, increase instances on single region if CPU above 65% https://cloud.google.com/spanner/docs/cpu-utilization#recommended-max"
Your new customer has requested daily reports that show their net consumption of Google Cloud compute resources and who used the resources. You need to quickly and efficiently generate these daily reports. What should you do?,"Do daily exports of Cloud Logging data to BigQuery. Create views filtering by project, log type, resource, and user.","Filter data in Cloud Logging by project, resource, and user; then export the data in CSV format.","Filter data in Cloud Logging by project, log type, resource, and user, then import the data into BigQuery.","Export Cloud Logging data to Cloud Storage in CSV format. Cleanse the data using Dataprep, filtering by project, resource, and user.",,,A,186,"Do daily exports of Cloud Logging data to BigQuery. Create views filtering by project, log type, resource, and user. You cannot import custom or filtered billing criteria into BigQuery. There are three types of Cloud Billing data tables with a fixed schema that must further drilled-down via BigQuery views. Reference: https://cloud.google.com/billing/docs/how-to/export-data-bigquery#setup"
"You issue a new batch job to Dataflow. The job starts successfully, processes a few elements, and then suddenly fails and shuts down. You navigate to the
Dataflow monitoring interface where you find errors related to a particular DoFn in your pipeline. What is the most likely cause of the errors?",Job validation,Exceptions in worker code,Graph or pipeline construction,Insufficient permissions,,,B,185,"While your job is running, you might encounter errors or exceptions in your worker code. These errors generally mean that the DoFns in your pipeline code have generated unhandled exceptions, which result in failed tasks in your Dataflow job. Exceptions in user code (for example, your DoFn instances) are reported in the Dataflow monitoring interface. Reference (Lists all answer choices and when to pick each one): https://cloud.google.com/dataflow/docs/guides/troubleshooting-your-pipeline#Causes"
"You are building a report-only data warehouse where the data is streamed into BigQuery via the streaming API. Following Google's best practices, you have both a staging and a production table for the data. How should you design your data loading to ensure that there is only one master dataset without affecting performance on either the ingestion or reporting pieces?","Have a staging table that is an append-only model, and then update the production table every three hours with the changes written to staging.","Have a staging table that is an append-only model, and then update the production table every ninety minutes with the changes written to staging.",Have a staging table that moves the staged data over to the production table and deletes the contents of the staging table every three hours.,Have a staging table that moves the staged data over to the production table and deletes the contents of the staging table every thirty minutes.,,,C,184,"I found the correct answer based on a real case, where Google's Solutions Architect team decided to move an internal process to use BigQuery. The related doc is here: https://cloud.google.com/blog/products/data-analytics/moving-a-publishing-workflow-to-bigquery-for-new-data-insights"
"You are using Bigtable to persist and serve stock market data for each of the major indices. To serve the trading application, you need to access only the most recent stock prices that are streaming in. How should you design your row key and tables to ensure that you can access the data with the simplest query?","Create one unique table for all of the indices, and then use the index and timestamp as the row key design.","Create one unique table for all of the indices, and then use a reverse timestamp as the row key design.","For each index, have a separate table and use a timestamp as the row key design.","For each index, have a separate table and use a reverse timestamp as the row key design.",,,B,183,"https://cloud.google.com/bigtable/docs/schema-design#time-based If you usually retrieve the most recent records first, you can use a reversed timestamp in the row key by subtracting the timestamp from your programming language's maximum value for long integers (in Java, java.lang.Long.MAX_VALUE). With a reversed timestamp, the records will be ordered from most recent to least recent."
"You are migrating your data warehouse to Google Cloud and decommissioning your on-premises data center. Because this is a priority for your company, you know that bandwidth will be made available for the initial data load to the cloud. The files being transferred are not large in number, but each file is 90 GB.
Additionally, you want your transactional systems to continually update the warehouse on Google Cloud in real time. What tools should you use to migrate the data and ensure that it continues to write to your warehouse?",Storage Transfer Service for the migration; Pub/Sub and Cloud Data Fusion for the real-time updates,BigQuery Data Transfer Service for the migration; Pub/Sub and Dataproc for the real-time updates,gsutil for the migration; Pub/Sub and Dataflow for the real-time updates,gsutil for both the migration and the real-time updates,,,C,182,"https://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#gsutil_for_smaller_transfers_of_on-premises_data The gsutil tool is the standard tool for small- to medium-sized transfers (less than 1 TB) over a typical enterprise-scale network, from a private data center to Google Cloud."
You need to give new website users a globally unique identifier (GUID) using a service that takes in data points and returns a GUID. This data is sourced from both internal and external systems via HTTP calls that you will make via microservices within your pipeline. There will be tens of thousands of messages per second and that can be multi-threaded. and you worry about the backpressure on the system. How should you design your pipeline to minimize that backpressure?,Call out to the service via HTTP.,Create the pipeline statically in the class definition.,Create a new object in the startBundle method of DoFn.,Batch the job into ten-second increments.,,,D,181,"D: I have insisted on this choice all aling. please read find the keyword massive backpressure https://cloud.google.com/blog/products/data-analytics/guide-to-common-cloud-dataflow-use-case-patterns-part-1 if the call takes on average 1 sec, that would cause massive backpressure on the pipeline. In these circumstances you should consider batching these requests, instead."
You have just created a new project which will be used to deploy a globally distributed application. You will use Cloud Spanner for data storage. You want to create a Cloud Spanner instance. You want to perform the first step in preparation of creating the instance. What should you do?,Enable the Cloud Spanner API.,Configure your Cloud Spanner instance to be multi-regional.,Create a new VPC network with subnetworks in all desired regions.,Grant yourself the IAM role of Cloud Spanner Admin.,,,A,180,"If you click on Create instance, the message is show in bottom: Cloud Spanner API for your project has been enabled."
"You are building a real-time prediction engine that streams files, which may contain PII (personal identifiable information) data, into Cloud Storage and eventually into BigQuery. You want to ensure that the sensitive data is masked but still maintains referential integrity, because names and emails are often used as join keys.
How should you use the Cloud Data Loss Prevention API (DLP API) to ensure that the PII data is not accessible by unauthorized individuals?","Create a pseudonym by replacing the PII data with cryptogenic tokens, and store the non-tokenized data in a locked-down button.","Redact all PII data, and store a version of the unredacted data in a locked-down bucket.","Scan every table in BigQuery, and mask the data it finds that has PII.",Create a pseudonym by replacing PII data with a cryptographic format-preserving token.,,,D,179,"https://cloud.google.com/dlp/docs/pseudonymization#supported-methods Format preserving encryption: An input value is replaced with a value that has been encrypted using the FPE-FFX encryption algorithm with a cryptographic key, and then prepended with a surrogate annotation, if specified. By design, both the character set and the length of the input value are preserved in the output value. Encrypted values can be re-identified using the original cryptographic key and the entire output value, including surrogate annotation."
"You are testing a Dataflow pipeline to ingest and transform text files. The files are compressed gzip, errors are written to a dead-letter queue, and you are using
SideInputs to join data. You noticed that the pipeline is taking longer to complete than expected; what should you do to expedite the Dataflow job?",Switch to compressed Avro files.,Reduce the batch size.,Retry records that throw an error.,Use CoGroupByKey instead of the SideInput.,,,D,178,There are a lot of reference doc to tell about comparison between them https://cloud.google.com/architecture/building-production-ready-data-pipelines-using-dataflow-developing-and-testing#choose_correctly_between_side_inputs_or_cogroupbykey_for_joins https://cloud.google.com/blog/products/data-analytics/guide-to-common-cloud-dataflow-use-case-patterns-part-2 https://stackoverflow.com/questions/58080383/sideinput-i-o-kills-performance
"You want to rebuild your batch pipeline for structured data on Google Cloud. You are using PySpark to conduct data transformations at scale, but your pipelines are taking over twelve hours to run. To expedite development and pipeline run time, you want to use a serverless tool and SOL syntax. You have already moved your raw data into Cloud Storage. How should you build the pipeline on Google Cloud while meeting speed and processing requirements?","Convert your PySpark commands into SparkSQL queries to transform the data, and then run your pipeline on Dataproc to write the data into BigQuery.","Ingest your data into Cloud SQL, convert your PySpark commands into SparkSQL queries to transform the data, and then use federated quenes from BigQuery for machine learning.","Ingest your data into BigQuery from Cloud Storage, convert your PySpark commands into BigQuery SQL queries to transform the data, and then write the transformations to a new table.","Use Apache Beam Python SDK to build the transformation pipelines, and write the data into BigQuery.",,,C,177,"The question is C but not because the SQL Syntax, as you can perfectly use SparkSQL on Dataproc reading files from GCS. It's because the ""serverless"" requirement."
"You have uploaded 5 years of log data to Cloud Storage. A user reported that some data points in the log data are outside of their expected ranges, which indicates errors. You need to address this issue and be able to run the process again in the future while keeping the original data for compliance reasons. What should you do?","Import the data from Cloud Storage into BigQuery. Create a new BigQuery table, and skip the rows with errors.",Create a Compute Engine instance and create a new copy of the data in Cloud Storage. Skip the rows with errors.,"Create a Dataflow workflow that reads the data from Cloud Storage, checks for values outside the expected range, sets the value to an appropriate default, and writes the updated records to a new dataset in Cloud Storage.","Create a Dataflow workflow that reads the data from Cloud Storage, checks for values outside the expected range, sets the value to an appropriate default, and writes the updated records to the same dataset in Cloud Storage.",,,C,176,"You can't filter out data using BQ load commands. You must imbed the logic to filter out data (i.e. time ranges) in another decoupled way (i.e. Dataflow, Cloud Functions, etc.). Therefore, A and B add additional complexity and deviates from the Data Lake design paradigm. D is wrong as the question strictly implies that the existing data set needs to be retained for compliance."
"Your company is implementing a data warehouse using BigQuery, and you have been tasked with designing the data model. You move your on-premises sales data warehouse with a star data schema to BigQuery but notice performance issues when querying the data of the past 30 days. Based on Google's recommended practices, what should you do to speed up the query without increasing storage costs?",Denormalize the data.,Shard the data by customer ID.,Materialize the dimensional data in views.,Partition the data by transaction date.,,,D,175,D is the right answer because it does not increase storage costs. A is not correct because denormalization typically increases the amount of storage needed.
"You work for a large financial institution that is planning to use Dialogflow to create a chatbot for the company's mobile app. You have reviewed old chat logs and tagged each conversation for intent based on each customer's stated intention for contacting customer service. About 70% of customer requests are simple requests that are solved within 10 intents. The remaining 30% of inquiries require much longer, more complicated requests. Which intents should you automate first?",Automate the 10 intents that cover 70% of the requests so that live agents can handle more complicated requests.,Automate the more complicated requests first because those require more of the agents' time.,Automate a blend of the shortest and longest intents to be representative of all intents.,Automate intents in places where common words such as 'payment' appear only once so the software isn't confused.,,,A,174,"This is the best approach because it follows the Pareto principle (80/20 rule). By automating the most common 10 intents that address 70% of customer requests, you free up the live agents to focus their time and effort on the more complex 30% of requests that likely require human insight/judgement. Automating the simpler high-volume requests first allows the chatbot to handle those easily, efficiently routing only the trickier cases to agents. This makes the best use of automation for high-volume simple cases and human expertise for lower-volume complex issues."
"You are designing a pipeline that publishes application events to a Pub/Sub topic. Although message ordering is not important, you need to be able to aggregate events across disjoint hourly intervals before loading the results to BigQuery for analysis. What technology should you use to process and load this data to
BigQuery while ensuring that it will scale with large volumes of events?",Create a Cloud Function to perform the necessary data processing that executes using the Pub/Sub trigger every time a new message is published to the topic.,"Schedule a Cloud Function to run hourly, pulling all available messages from the Pub/Sub topic and performing the necessary aggregations.","Schedule a batch Dataflow job to run hourly, pulling all available messages from the Pub/Sub topic and performing the necessary aggregations.",Create a streaming Dataflow job that reads continually from the Pub/Sub topic and performs the necessary aggregations using tumbling windows.,,,D,173,D TUMBLE=> fixed windows. HOP=> sliding windows. SESSION=> session windows.
"You are analyzing the price of a company's stock. Every 5 seconds, you need to compute a moving average of the past 30 seconds' worth of data. You are reading data from Pub/Sub and using DataFlow to conduct the analysis. How should you set up your windowed pipeline?",Use a fixed window with a duration of 5 seconds. Emit results by setting the following trigger: AfterProcessingTime.pastFirstElementInPane().plusDelayOf (Duration.standardSeconds(30)),Use a fixed window with a duration of 30 seconds. Emit results by setting the following trigger: AfterWatermark.pastEndOfWindow().plusDelayOf (Duration.standardSeconds(5)),Use a sliding window with a duration of 5 seconds. Emit results by setting the following trigger: AfterProcessingTime.pastFirstElementInPane().plusDelayOf (Duration.standardSeconds(30)),Use a sliding window with a duration of 30 seconds and a period of 5 seconds. Emit results by setting the following trigger: AfterWatermark.pastEndOfWindow (),,,D,172,Use a sliding window with a duration of 30 seconds and a period of 5 seconds. Emit results by setting the following trigger: AfterWatermark.pastEndOfWindow () Reveal Solution
"You work for a large real estate firm and are preparing 6 TB of home sales data to be used for machine learning. You will use SQL to transform the data and use
BigQuery ML to create a machine learning model. You plan to use the model for predictions against a raw dataset that has not been transformed. How should you set up your workflow in order to prevent skew at prediction time?","When creating your model, use BigQuery's TRANSFORM clause to define preprocessing steps. At prediction time, use BigQuery's ML.EVALUATE clause without specifying any transformations on the raw input data.","When creating your model, use BigQuery's TRANSFORM clause to define preprocessing steps. Before requesting predictions, use a saved query to transform your raw input data, and then use ML.EVALUATE.","Use a BigQuery view to define your preprocessing logic. When creating your model, use the view as your model training data. At prediction time, use BigQuery's ML.EVALUATE clause without specifying any transformations on the raw input data.","Preprocess all data using Dataflow. At prediction time, use BigQuery's ML.EVALUATE clause without specifying any further transformations on the input data.",,,A,171,"Using the TRANSFORM clause, you can specify all preprocessing during model creation. The preprocessing is automatically applied during the prediction and evaluation phases of machine learning. Reference: https://cloud.google.com/bigquery-ml/docs/bigqueryml-transform"
"You are updating the code for a subscriber to a Pub/Sub feed. You are concerned that upon deployment the subscriber may erroneously acknowledge messages, leading to message loss. Your subscriber is not set up to retain acknowledged messages. What should you do to ensure that you can recover from errors after deployment?",Set up the Pub/Sub emulator on your local machine. Validate the behavior of your new subscriber logic before deploying it to production.,Create a Pub/Sub snapshot before deploying new subscriber code. Use a Seek operation to re-deliver messages that became available after the snapshot was created.,"Use Cloud Build for your deployment. If an error occurs after deployment, use a Seek operation to locate a timestamp logged by Cloud Build at the start of the deployment.","Enable dead-lettering on the Pub/Sub topic to capture messages that aren't successfully acknowledged. If an error occurs after deployment, re-deliver any messages captured by the dead-letter queue.",,,B,170,"According to the second reference in the list below, a concern with deploying new subscriber code is that the new executable may erroneously acknowledge messages, leading to message loss. Incorporating snapshots into your deployment process gives you a way to recover from bugs in new subscriber code. Answer cannot be C because To seek to a timestamp, you must first configure the subscription to retain acknowledged messages using retain-acked-messages. If retain-acked-messages is set, Pub/Sub retains acknowledged messages for 7 days. References: https://cloud.google.com/pubsub/docs/replay-message https://cloud.google.com/pubsub/docs/replay-overview#seek_use_cases"
"You are migrating a table to BigQuery and are deciding on the data model. Your table stores information related to purchases made across several store locations and includes information like the time of the transaction, items purchased, the store ID, and the city and state in which the store is located. You frequently query this table to see how many of each item were sold over the past 30 days and to look at purchasing trends by state, city, and individual store. How would you model this table for the best query performance?","Partition by transaction time; cluster by state first, then city, then store ID.","Partition by transaction time; cluster by store ID first, then city, then state.","Top-level cluster by state first, then city, then store ID.","Top-level cluster by store ID first, then city, then state.",,,A,169,"Partition by transaction time; cluster by state first, then city, then store ID."
"You work for a financial institution that lets customers register online. As new customers register, their user data is sent to Pub/Sub before being ingested into
BigQuery. For security reasons, you decide to redact your customers' Government issued Identification Number while allowing customer service representatives to view the original values when necessary. What should you do?",Use BigQuery's built-in AEAD encryption to encrypt the SSN column. Save the keys to a new table that is only viewable by permissioned users.,Use BigQuery column-level security. Set the table permissions so that only members of the Customer Service user group can see the SSN column.,"Before loading the data into BigQuery, use Cloud Data Loss Prevention (DLP) to replace input values with a cryptographic hash.","Before loading the data into BigQuery, use Cloud Data Loss Prevention (DLP) to replace input values with a cryptographic format-preserving encryption token.",,,B,168,"B. BigQuery column-level security: Pros: Granular control over column access, ensures only authorized users see the SSN column. Cons: Doesn't truly redact the data. The SSN values are still stored in BigQuery, even if hidden from unauthorized users. A potential security breach could expose them."
"Your company currently runs a large on-premises cluster using Spark, Hive, and HDFS in a colocation facility. The cluster is designed to accommodate peak usage on the system; however, many jobs are batch in nature, and usage of the cluster fluctuates quite dramatically. Your company is eager to move to the cloud to reduce the overhead associated with on-premises infrastructure and maintenance and to benefit from the cost savings. They are also hoping to modernize their existing infrastructure to use more serverless offerings in order to take advantage of the cloud. Because of the timing of their contract renewal with the colocation facility, they have only 2 months for their initial migration. How would you recommend they approach their upcoming migration strategy so they can maximize their cost savings in the cloud while still executing the migration in time?",Migrate the workloads to Dataproc plus HDFS; modernize later.,Migrate the workloads to Dataproc plus HDFS; modernize later.,"Migrate the Spark workload to Dataproc plus HDFS, and modernize the Hive workload for BigQuery.",Modernize the Spark workload for Dataflow and the Hive workload for BigQuery.,,,B,167,"https://cloud.google.com/architecture/hadoop/migrating-apache-spark-jobs-to-cloud-dataproc#overview When you want to move your Apache Spark workloads from an on-premises environment to Google Cloud, we recommend using Dataproc to run Apache Spark/Apache Hadoop clusters. Dataproc is a fully managed, fully supported service offered by Google Cloud. It allows you to separate storage and compute, which helps you to manage your costs and be more flexible in scaling your workloads. https://cloud.google.com/bigquery/docs/migration/hive#data_migration Migrating Hive data from your on-premises or other cloud-based source cluster to BigQuery has two steps: 1. Copying data from a source cluster to Cloud Storage 2. Loading data from Cloud Storage into BigQuery"
"A shipping company has live package-tracking data that is sent to an Apache Kafka stream in real time. This is then loaded into BigQuery. Analysts in your company want to query the tracking data in BigQuery to analyze geospatial trends in the lifecycle of a package. The table was originally created with ingest-date partitioning. Over time, the query processing time has increased. You need to implement a change that would improve query performance in BigQuery. What should you do?",Implement clustering in BigQuery on the ingest date column.,Implement clustering in BigQuery on the package-tracking ID column.,Tier older data onto Cloud Storage files and create a BigQuery table using Cloud Storage as an external data source.,Re-create the table using data partitioning on the package delivery date.,,,B,166,"https://cloud.google.com/bigquery/docs/clustered-tables Clustered tables in BigQuery are tables that have a user-defined column sort order using clustered columns. Clustered tables can improve query performance and reduce query costs. In BigQuery, a clustered column is a user-defined table property that sorts storage blocks based on the values in the clustered columns. The storage blocks are adaptively sized based on the size of the table. A clustered table maintains the sort properties in the context of each operation that modifies it. Queries that filter or aggregate by the clustered columns only scan the relevant blocks based on the clustered columns instead of the entire table or table partition."
You work for a large bank that operates in locations throughout North America. You are setting up a data storage system that will handle bank account transactions. You require ACID compliance and the ability to access data with SQL. Which solution is appropriate?,Store transaction data in Cloud Spanner. Enable stale reads to reduce latency.,Store transaction in Cloud Spanner. Use locking read-write transactions.,Store transaction data in BigQuery. Disabled the query cache to ensure consistency.,Store transaction data in Cloud SQL. Use a federated query BigQuery for analysis.,,,B,165,"I'd say B as the documentation primarily says ACID compliance for Spanner, not Cloud SQL. https://cloud.google.com/blog/topics/developers-practitioners/your-google-cloud-database-options-explained Also, spanner supports read-write transactions for use cases, as handling bank transactions: https://cloud.google.com/spanner/docs/transactions#read-write_transactions"
"You are working on a linear regression model on BigQuery ML to predict a customer's likelihood of purchasing your company's products. Your model uses a city name variable as a key predictive component. In order to train and serve the model, your data must be organized in columns. You want to prepare your data using the least amount of coding while maintaining the predictable variables. What should you do?",Create a new view with BigQuery that does not include a column with city information.,"Use SQL in BigQuery to transform the state column using a one-hot encoding method, and make each city a column with binary values.",Use TensorFlow to create a categorical variable with a vocabulary list. Create the vocabulary file and upload that as part of your model to BigQuery ML.,"Use Cloud Data Fusion to assign each city to a region that is labeled as 1, 2, 3, 4, or 5, and then use that number to represent the city in the model.",,,B,164,"One-hot encoding is a common technique used to handle categorical data in machine learning. This approach will transform the city name variable into a series of binary columns, one for each city. Each row will have a ""1"" in the column corresponding to the city it represents and ""0"" in all other city columns. This method is effective for linear regression models as it enables the model to use city data as a series of numeric, binary variables. BigQuery supports SQL operations that can easily implement one-hot encoding, thus minimizing the amount of coding required and efficiently preparing the data for the model."
"You have data pipelines running on BigQuery, Dataflow, and Dataproc. You need to perform health checks and monitor their behavior, and then notify the team managing the pipelines if they fail. You also need to be able to work across multiple projects. Your preference is to use managed products or features of the platform. What should you do?","Export the information to Cloud Monitoring, and set up an Alerting policy","Run a Virtual Machine in Compute Engine with Airflow, and export the information to Cloud Monitoring","Export the logs to BigQuery, and set up App Engine to read that information and send emails if you find a failure in the logs","Develop an App Engine application to consume logs using GCP API calls, and send emails if you find a failure in the logs",,,A,163,"Cloud Monitoring (formerly known as Stackdriver) is a fully managed monitoring service provided by GCP, which can collect metrics, logs, and other telemetry data from various GCP services, including BigQuery, Dataflow, and Dataproc. Alerting Policies: Cloud Monitoring allows you to define alerting policies based on specific conditions or thresholds, such as pipeline failures, latency spikes, or other custom metrics. When these conditions are met, Cloud Monitoring can trigger notifications (e.g., emails) to alert the team managing the pipelines. Cross-Project Monitoring: Cloud Monitoring supports monitoring resources across multiple GCP projects, making it suitable for your requirement to monitor pipelines in multiple projects. Managed Solution: Cloud Monitoring is a managed service, reducing the operational overhead compared to running your own virtual machine instances or building custom solutions."
"You want to archive data in Cloud Storage. Because some data is very sensitive, you want to use the `Trust No One` (TNO) approach to encrypt your data to prevent the cloud provider staff from decrypting your data. What should you do?","Use gcloud kms keys create to create a symmetric key. Then use gcloud kms encrypt to encrypt each archival file with the key and unique additional authenticated data (AAD). Use gsutil cp to upload each encrypted file to the Cloud Storage bucket, and keep the AAD outside of Google Cloud.","Use gcloud kms keys create to create a symmetric key. Then use gcloud kms encrypt to encrypt each archival file with the key. Use gsutil cp to upload each encrypted file to the Cloud Storage bucket. Manually destroy the key previously used for encryption, and rotate the key once.",Specify customer-supplied encryption key (CSEK) in the .boto configuration file. Use gsutil cp to upload each archival file to the Cloud Storage bucket. Save the CSEK in Cloud Memorystore as permanent storage of the secret.,Specify customer-supplied encryption key (CSEK) in the .boto configuration file. Use gsutil cp to upload each archival file to the Cloud Storage bucket. Save the CSEK in a different project that only the security team can access.,,,D,162,"A and B can be eliminated immediately since kms generated keys are considered potentially accessible by CSP. C is incorrect because memory store is essentially a cache service. Additional authenticated data (AAD) acts as a ""salt"", it is not a cipher."
"You need to choose a database to store time series CPU and memory usage for millions of computers. You need to store this data in one-second interval samples. Analysts will be performing real-time, ad hoc analytics against the database. You want to avoid being charged for every query executed and ensure that the schema design will allow for future growth of the dataset. Which database and data model should you choose?","Create a table in BigQuery, and append the new samples for CPU and memory to the table","Create a wide table in BigQuery, create a column for the sample value at each second, and update the row with the interval for each second",Create a narrow table in Bigtable with a row key that combines the Computer Engine computer identifier with the sample time at each second,"Create a wide table in Bigtable with a row key that combines the computer identifier with the sample time at each minute, and combine the values for each second as column data.",,,C,161,"A tall and narrow table has a small number of events per row, which could be just one event, whereas a short and wide table has a large number of events per row. As explained in a moment, tall and narrow tables are best suited for time-series data. For time series, you should generally use tall and narrow tables. This is for two reasons: Storing one event per row makes it easier to run queries against your data. Storing many events per row makes it more likely that the total row size will exceed the recommended maximum (see Rows can be big but are not infinite). https://cloud.google.com/bigtable/docs/schema-design-time-series#patterns_for_row_key_design"
"You work for a mid-sized enterprise that needs to move its operational system transaction data from an on-premises database to GCP. The database is about 20
TB in size. Which database should you choose?",Cloud SQL,Cloud Bigtable,Cloud Spanner,Cloud Datastore,,,A,160,"Cloud SQL is generally better for OLTP, and Cloud SQL is up to 64 TB now. https://cloud.google.com/sql/docs/quotas#storage_limits"
"You need to choose a database for a new project that has the following requirements:
✑ Fully managed
✑ Able to automatically scale up
✑ Transactionally consistent
✑ Able to scale up to 6 TB
✑ Able to be queried using SQL
Which database do you choose?",Cloud SQL,Cloud Bigtable,Cloud Spanner,Cloud Datastore,,,A,159,"It asks for scaling up which can be done in cloud sql, horizontal scaling is not possible in cloud sql Automatic storage increase If you enable this setting, Cloud SQL checks your available storage every 30 seconds. If the available storage falls below a threshold size, Cloud SQL automatically adds additional storage capacity. If the available storage repeatedly falls below the threshold size, Cloud SQL continues to add storage until it reaches the maximum of 30 TB."
You need to deploy additional dependencies to all nodes of a Cloud Dataproc cluster at startup using an existing initialization action. Company security policies require that Cloud Dataproc nodes do not have access to the Internet so public initialization actions cannot fetch resources. What should you do?,Deploy the Cloud SQL Proxy on the Cloud Dataproc master,Use an SSH tunnel to give the Cloud Dataproc cluster access to the Internet,Copy all dependencies to a Cloud Storage bucket within your VPC security perimeter,Use Resource Manager to add the service account used by the Cloud Dataproc cluster to the Network User role,,,C,158,"If you create a Dataproc cluster with internal IP addresses only, attempts to access the Internet in an initialization action will fail unless you have configured routes to direct the traffic through a NAT or a VPN gateway. Without access to the Internet, you can enable Private Google Access, and place job dependencies in Cloud Storage; cluster nodes can download the dependencies from Cloud Storage from internal IPs."
"Your team is working on a binary classification problem. You have trained a support vector machine (SVM) classifier with default parameters, and received an area under the Curve (AUC) of 0.87 on the validation set. You want to increase the AUC of the model. What should you do?",Perform hyperparameter tuning,"Train a classifier with deep neural networks, because neural networks would always beat SVMs",Deploy the model and measure the real-world AUC; it's always higher because of generalization,Scale predictions you get out of the model (tune a scaling factor as a hyperparameter) in order to get the highest AUC,,,A,157,"A. Preprocessing/scaling should be done with input features, instead of predictions (output)"
You are planning to migrate your current on-premises Apache Hadoop deployment to the cloud. You need to ensure that the deployment is as fault-tolerant and cost-effective as possible for long-running batch jobs. You want to use a managed service. What should you do?,"Deploy a Dataproc cluster. Use a standard persistent disk and 50% preemptible workers. Store data in Cloud Storage, and change references in scripts from hdfs:// to gs://","Deploy a Dataproc cluster. Use an SSD persistent disk and 50% preemptible workers. Store data in Cloud Storage, and change references in scripts from hdfs:// to gs://","Install Hadoop and Spark on a 10-node Compute Engine instance group with standard instances. Install the Cloud Storage connector, and store the data in Cloud Storage. Change references in scripts from hdfs:// to gs://",Install Hadoop and Spark on a 10-node Compute Engine instance group with preemptible instances. Store data in HDFS. Change references in scripts from hdfs:// to gs://,,,A,156,Ask for cost effective so persistent disk are HDD which are cheaper in comparison to SSD.
"Your company is selecting a system to centralize data ingestion and delivery. You are considering messaging and data integration systems to address the requirements. The key requirements are:
✑ The ability to seek to a particular offset in a topic, possibly back to the start of all data ever captured
✑ Support for publish/subscribe semantics on hundreds of topics

Retain per-key ordering -

Which system should you choose?",Apache Kafka,Cloud Storage,Dataflow,Firebase Cloud Messaging,,,A,155,"A: topics, offsets --> apache kafka"
You plan to deploy Cloud SQL using MySQL. You need to ensure high availability in the event of a zone failure. What should you do?,"Create a Cloud SQL instance in one zone, and create a failover replica in another zone within the same region.","Create a Cloud SQL instance in one zone, and create a read replica in another zone within the same region.","Create a Cloud SQL instance in one zone, and configure an external read replica in a zone in a different region.","Create a Cloud SQL instance in a region, and configure automatic backup to a Cloud Storage bucket in the same region.",,,A,154,"""Multiple zones (Highly available) Automatic failover to another zone within your selected region. Recommended for production instances. Increases cost."""
You operate an IoT pipeline built around Apache Kafka that normally receives around 5000 messages per second. You want to use Google Cloud Platform to create an alert as soon as the moving average over 1 hour drops below 4000 messages per second. What should you do?,"Consume the stream of data in Dataflow using Kafka IO. Set a sliding time window of 1 hour every 5 minutes. Compute the average when the window closes, and send an alert if the average is less than 4000 messages.","Consume the stream of data in Dataflow using Kafka IO. Set a fixed time window of 1 hour. Compute the average when the window closes, and send an alert if the average is less than 4000 messages.","Use Kafka Connect to link your Kafka message queue to Pub/Sub. Use a Dataflow template to write your messages from Pub/Sub to Bigtable. Use Cloud Scheduler to run a script every hour that counts the number of rows created in Bigtable in the last hour. If that number falls below 4000, send an alert.","Use Kafka Connect to link your Kafka message queue to Pub/Sub. Use a Dataflow template to write your messages from Pub/Sub to BigQuery. Use Cloud Scheduler to run a script every five minutes that counts the number of rows created in BigQuery in the last hour. If that number falls below 4000, send an alert.",,,A,153,"Dataflow with Sliding Time Windows: Dataflow allows you to work with event-time windows, making it suitable for time-series data like incoming IoT messages. Using sliding windows every 5 minutes allows you to compute moving averages efficiently. Sliding Time Window: The sliding time window of 1 hour every 5 minutes enables you to calculate the moving average over the specified time frame. Computing Averages: You can efficiently compute the average when each sliding window closes. This approach ensures that you have real-time visibility into the message rate and can detect deviations from the expected rate. Alerting: When the calculated average drops below 4000 messages per second, you can trigger an alert from within the Dataflow pipeline, sending it to your desired alerting mechanism, such as Cloud Monitoring, Pub/Sub, or another notification service. Scalability: Dataflow can scale automatically based on the incoming data volume, ensuring that you can handle the expected rate of 5000 messages per second."
"You work for a global shipping company. You want to train a model on 40 TB of data to predict which ships in each geographic region are likely to cause delivery delays on any given day. The model will be based on multiple attributes collected from multiple sources. Telemetry data, including location in GeoJSON format, will be pulled from each ship and loaded every hour. You want to have a dashboard that shows how many and which ships are likely to cause delays within a region. You want to use a storage solution that has native functionality for prediction and geospatial processing. Which storage solution should you use?",BigQuery,Cloud Bigtable,Cloud Datastore,Cloud SQL for PostgreSQL,,,A,152,Description: Geospatial and ML functionality is with bigquery
"You work for an advertising company, and you've developed a Spark ML model to predict click-through rates at advertisement blocks. You've been developing everything at your on-premises data center, and now your company is migrating to Google Cloud. Your data center will be closing soon, so a rapid lift-and-shift migration is necessary. However, the data you've been using will be migrated to migrated to BigQuery. You periodically retrain your Spark ML models, so you need to migrate existing training pipelines to Google Cloud. What should you do?",Use Vertex AI for training existing Spark ML models,"Rewrite your models on TensorFlow, and start using Vertex AI","Use Dataproc for training existing Spark ML models, but start reading data directly from BigQuery","Spin up a Spark cluster on Compute Engine, and train Spark ML models on the data exported from BigQuery",,,C,151,"Option C : It is the most rapid way to migrate your existing training pipelines to Google Cloud. It allows you to continue using your existing Spark ML models. It allows you to take advantage of the scalability and performance of Dataproc. It allows you to read data directly from BigQuery, which is a more efficient way to process large datasets"
"You want to build a managed Hadoop system as your data lake. The data transformation process is composed of a series of Hadoop jobs executed in sequence.
To accomplish the design of separating storage from compute, you decided to use the Cloud Storage connector to store all input data, output data, and intermediary data. However, you noticed that one Hadoop job runs very slowly with Cloud Dataproc, when compared with the on-premises bare-metal Hadoop environment (8-core nodes with 100-GB RAM). Analysis shows that this particular Hadoop job is disk I/O intensive. You want to resolve the issue. What should you do?","Allocate sufficient memory to the Hadoop cluster, so that the intermediary data of that particular Hadoop job can be held in memory","Allocate sufficient persistent disk space to the Hadoop cluster, and store the intermediate data of that particular Hadoop job on native HDFS",Allocate more CPU cores of the virtual machine instances of the Hadoop cluster so that the networking bandwidth for each instance can scale up,"Allocate additional network interface card (NIC), and configure link aggregation in the operating system to use the combined throughput when working with Cloud Storage",,,B,150,"Local HDFS storage is a good option if: Your jobs require a lot of metadata operations—for example, you have thousands of partitions and directories, and each file size is relatively small. You modify the HDFS data frequently or you rename directories. (Cloud Storage objects are immutable, so renaming a directory is an expensive operation because it consists of copying all objects to a new key and deleting them afterwards.) You heavily use the append operation on HDFS files. You have workloads that involve heavy I/O. For example, you have a lot of partitioned writes, such as the following: spark.read().write.partitionBy(...).parquet(""gs://"") You have I/O workloads that are especially sensitive to latency. For example, you require single-digit millisecond latency per storage operation."
You are migrating your data warehouse to BigQuery. You have migrated all of your data into tables in a dataset. Multiple users from your organization will be using the data. They should only see certain tables based on their team membership. How should you set user permissions?,Assign the users/groups data viewer access at the table level for each table,"Create SQL views for each team in the same dataset in which the data resides, and assign the users/groups data viewer access to the SQL views","Create authorized views for each team in the same dataset in which the data resides, and assign the users/groups data viewer access to the authorized views",Create authorized views for each team in datasets created for each team. Assign the authorized views data viewer access to the dataset in which the data resides. Assign the users/groups data viewer access to the datasets in which the authorized views reside,,,A,149,Now it is feasible to provide table level access to user by allowing user to query single table and no other table will be visible to user in same dataset.
You work for a shipping company that has distribution centers where packages move on delivery lines to route them properly. The company wants to add cameras to the delivery lines to detect and track any visual damage to the packages in transit. You need to create a way to automate the detection of damaged packages and flag them for human review in real time while the packages are in transit. Which solution should you choose?,"Use BigQuery machine learning to be able to train the model at scale, so you can analyze the packages in batches.","Train an AutoML model on your corpus of images, and build an API around that model to integrate with the package tracking applications.","Use the Cloud Vision API to detect for damage, and raise an alert through Cloud Functions. Integrate the package tracking applications with this function.",Use TensorFlow to create a model that is trained on your corpus of images. Create a Python notebook in Cloud Datalab that uses this model so you can analyze for damaged packages.,,,B,148,AutoML is used to train model and do damage detection Auto Vision is used is a pre trained model used to detect objects in images
"You are implementing several batch jobs that must be executed on a schedule. These jobs have many interdependent steps that must be executed in a specific order. Portions of the jobs involve executing shell scripts, running Hadoop jobs, and running queries in BigQuery. The jobs are expected to run for many minutes up to several hours. If the steps fail, they must be retried a fixed number of times. Which service should you use to manage the execution of these jobs?",Cloud Scheduler,Cloud Dataflow,Cloud Functions,Cloud Composer,,,D,147,"https://cloud.google.com/composer/docs/concepts/overview Cloud Composer is a fully managed workflow orchestration service, enabling you to create, schedule, monitor, and manage workflows that span across clouds and on-premises data centers."
"You want to migrate an on-premises Hadoop system to Cloud Dataproc. Hive is the primary tool in use, and the data format is Optimized Row Columnar (ORC).
All ORC files have been successfully copied to a Cloud Storage bucket. You need to replicate some data to the cluster's local Hadoop Distributed File System
(HDFS) to maximize performance. What are two ways to start using Hive in Cloud Dataproc? (Choose two.)",Run the gsutil utility to transfer all ORC files from the Cloud Storage bucket to HDFS. Mount the Hive tables locally.,Run the gsutil utility to transfer all ORC files from the Cloud Storage bucket to any node of the Dataproc cluster. Mount the Hive tables locally.,Run the gsutil utility to transfer all ORC files from the Cloud Storage bucket to the master node of the Dataproc cluster. Then run the Hadoop utility to copy them do HDFS. Mount the Hive tables from HDFS.,Leverage Cloud Storage connector for Hadoop to mount the ORC files as external Hive tables. Replicate external Hive tables to the native ones.,Load the ORC files into BigQuery. Leverage BigQuery connector for Hadoop to mount the BigQuery tables as external Hive tables. Replicate external Hive tables to the native ones.,,"C,D",146,I know it says to transfer all the files but with the options provided c is the best choice. Explaination A and B cannot be true as gsutil can copy data to master node and the to hdfs from master node. C -> works D->works Recommended by google E-> Will work but as the question says maximize performance this is not a case. As bigquery hadoop connecter stores all the BQ data to GCS as temp and then processes it to HDFS. As data is already in GCS we donot need to load it to bq and use a connector then unloads it back to GCS and then processes it.
"You receive data files in CSV format monthly from a third party. You need to cleanse this data, but every third month the schema of the files changes. Your requirements for implementing these transformations include:
✑ Executing the transformations on a schedule
✑ Enabling non-developer analysts to modify transformations
✑ Providing a graphical tool for designing transformations
What should you do?","Use Dataprep by Trifacta to build and maintain the transformation recipes, and execute them on a scheduled basis","Load each month's CSV data into BigQuery, and write a SQL query to transform the data to a standard schema. Merge the transformed tables together with a SQL query",Help the analysts write a Dataflow pipeline in Python to perform the transformation. The Python code should be stored in a revision control system and modified as the incoming data's schema changes,Use Apache Spark on Dataproc to infer the schema of the CSV file before creating a Dataframe. Then implement the transformations in Spark SQL before writing the data out to Cloud Storage and loading into BigQuery,,,A,145,Addresses Requirements: Scheduled Execution: Dataprep supports running transformations on a schedule. Analyst-Friendly: Dataprep's visual interface is designed for non-developer analysts to build and modify transformations easily. Graphical Tool: It provides a drag-and-drop environment for designing data transformations. Schema Flexibility: Dataprep can handle schema changes. Analysts can adapt recipes using the visual interface
"You need to move 2 PB of historical data from an on-premises storage appliance to Cloud Storage within six months, and your outbound network capacity is constrained to 20 Mb/sec. How should you migrate this data to Cloud Storage?",Use Transfer Appliance to copy the data to Cloud Storage,"Use gsutil cp ג€""J to compress the content being uploaded to Cloud Storage","Create a private URL for the historical data, and then use Storage Transfer Service to copy the data to Cloud Storage",Use trickle or ionice along with gsutil cp to limit the amount of bandwidth gsutil utilizes to less than 20 Mb/sec so it does not interfere with the production traffic,,,A,144,"Description: Huge amount of data with log network bandwidth, Transfer applicate is best for moving data over 100TB"
You are operating a streaming Cloud Dataflow pipeline. Your engineers have a new version of the pipeline with a different windowing algorithm and triggering strategy. You want to update the running pipeline with the new version. You want to ensure that no data is lost during the update. What should you do?,Update the Cloud Dataflow pipeline inflight by passing the --update option with the --jobName set to the existing job name,Update the Cloud Dataflow pipeline inflight by passing the --update option with the --jobName set to a new unique job name,Stop the Cloud Dataflow pipeline with the Cancel option. Create a new Cloud Dataflow job with the updated code,Stop the Cloud Dataflow pipeline with the Drain option. Create a new Cloud Dataflow job with the updated code,,,D,143,"New version is mayor changes. Stop and drain and then launch the new code is a lot is the safer way. We recommend that you attempt only smaller changes to your pipeline's windowing, such as changing the duration of fixed- or sliding-time windows. Making major changes to windowing or triggers, like changing the windowing algorithm, might have unpredictable results on your pipeline output. https://cloud.google.com/dataflow/docs/guides/updating-a-pipeline#changing_windowing"
"Each analytics team in your organization is running BigQuery jobs in their own projects. You want to enable each team to monitor slot usage within their projects.
What should you do?",Create a Cloud Monitoring dashboard based on the BigQuery metric query/scanned_bytes,Create a Cloud Monitoring dashboard based on the BigQuery metric slots/allocated_for_project,"Create a log export for each project, capture the BigQuery job execution logs, create a custom metric based on the totalSlotMs, and create a Cloud Monitoring dashboard based on the custom metric","Create an aggregated log export at the organization level, capture the BigQuery job execution logs, create a custom metric based on the totalSlotMs, and create a Cloud Monitoring dashboard based on the custom metric",,,B,142,"Viewing project and reservation slot usage in Stackdriver Monitoring Information is available from the ""Slots Allocated"" metric in Stackdriver Monitoring. This metric information includes a per-reservation and per-job breakdown of slot usage. The information can also be visualized by using the custom charts metric explorer. https://cloud.google.com/bigquery/docs/reservations-monitoring https://cloud.google.com/monitoring/api/metrics_gcp"
Data Analysts in your company have the Cloud IAM Owner role assigned to them in their projects to allow them to work with multiple GCP products in their projects. Your organization requires that all BigQuery data access logs be retained for 6 months. You need to ensure that only audit personnel in your company can access the data access logs for all projects. What should you do?,Enable data access logs in each Data Analyst's project. Restrict access to Stackdriver Logging via Cloud IAM roles.,Export the data access logs via a project-level export sink to a Cloud Storage bucket in the Data Analysts' projects. Restrict access to the Cloud Storage bucket.,Export the data access logs via a project-level export sink to a Cloud Storage bucket in a newly created projects for audit logs. Restrict access to the project with the exported logs.,Export the data access logs via an aggregated export sink to a Cloud Storage bucket in a newly created project for audit logs. Restrict access to the project that contains the exported logs,,,D,141,"Aggregated log sink will create a single sink for all projects, the destination can be a google cloud storage, pub/sub topic, bigquery table or a cloud logging bucket. without aggregated sink this will be required to be done for each project individually which will be cumbersome. https://cloud.google.com/logging/docs/export/aggregated_sinks"
"You need to create a new transaction table in Cloud Spanner that stores product sales data. You are deciding what to use as a primary key. From a performance perspective, which strategy should you choose?",The current epoch time,A concatenation of the product name and the current epoch time,A random universally unique identifier number (version 4 UUID),"The original order identification number from the sales system, which is a monotonically increasing integer",,,C,140,"Use a Universally Unique Identifier (UUID) You can use a Universally Unique Identifier (UUID) as defined by RFC 4122 as the primary key. Version 4 UUID is recommended, because it uses random values in the bit sequence. Version 1 UUID stores the timestamp in the high order bits and is not recommended. https://cloud.google.com/spanner/docs/schema-design"
You are building a new data pipeline to share data between two different types of applications: jobs generators and job runners. Your solution must scale to accommodate increases in usage and must accommodate the addition of new applications without negatively affecting the performance of existing ones. What should you do?,Create an API using App Engine to receive and send messages to the applications,"Use a Cloud Pub/Sub topic to publish jobs, and use subscriptions to execute them","Create a table on Cloud SQL, and insert and delete rows with the job information","Create a table on Cloud Spanner, and insert and delete rows with the job information",,,B,139,"Job generators (they would be the publishers). Job runners = subscribers Question mentions that it must scale (of which push subscription has automatic scaling) and can accommodate additional new applications (this can be solved by having multiple subscriptions, with each relating to a unique application) to a central topic"
"You have several Spark jobs that run on a Cloud Dataproc cluster on a schedule. Some of the jobs run in sequence, and some of the jobs run concurrently. You need to automate this process. What should you do?",Create a Cloud Dataproc Workflow Template,Create an initialization action to execute the jobs,Create a Directed Acyclic Graph in Cloud Composer,"Create a Bash script that uses the Cloud SDK to create a cluster, execute jobs, and then tear down the cluster",,,C,138,"DAG workflows: Cloud Composer excels at orchestrating complex workflows with dependencies, making it ideal for managing sequential and concurrent execution of your Spark jobs. You can define dependencies between tasks to ensure certain jobs only run after others finish. Automation: Cloud Composer lets you schedule workflows to run automatically based on triggers like time intervals or data availability, eliminating the need for manual intervention. Integration: Cloud Composer integrates seamlessly with Cloud Dataproc, allowing you to easily launch and manage your Spark clusters within the workflow. Scalability: Cloud Composer scales well to handle a large number of jobs and workflows, making it suitable for managing complex data pipelines."
You have a data pipeline with a Dataflow job that aggregates and writes time series metrics to Bigtable. You notice that data is slow to update in Bigtable. This data feeds a dashboard used by thousands of users across the organization. You need to support additional concurrent users and reduce the amount of time required to write the data. Which two actions should you take? (Choose two.),Configure your Dataflow pipeline to use local execution,Increase the maximum number of Dataflow workers by setting maxNumWorkers in PipelineOptions,Increase the number of nodes in the Bigtable cluster,Modify your Dataflow pipeline to use the Flatten transform before writing to Bigtable,Modify your Dataflow pipeline to use the CoGroupByKey transform before writing to Bigtable,,"B,C",137,"B. Increase the maximum number of Dataflow workers by setting maxNumWorkers in PipelineOptions: Increasing the number of Dataflow workers can help parallelize the processing of your data, which can result in faster data updates to Bigtable and improved concurrency. You can set maxNumWorkers to a higher value to achieve this. C. Increase the number of nodes in the Bigtable cluster: Increasing the number of nodes in your Bigtable cluster can improve the overall throughput and reduce latency when writing data. It allows Bigtable to handle a higher rate of data ingestion and queries, which is essential for supporting additional concurrent users."
"You are running a pipeline in Dataflow that receives messages from a Pub/Sub topic and writes the results to a BigQuery dataset in the EU. Currently, your pipeline is located in europe-west4 and has a maximum of 3 workers, instance type n1-standard-1. You notice that during peak periods, your pipeline is struggling to process records in a timely fashion, when all 3 workers are at maximum CPU utilization. Which two actions can you take to increase performance of your pipeline? (Choose two.)",Increase the number of max workers,Use a larger instance type for your Dataflow workers,Change the zone of your Dataflow pipeline to run in us-central1,"Create a temporary table in Bigtable that will act as a buffer for new data. Create a new step in your pipeline to write to this table first, and then create a new pipeline to write from Bigtable to BigQuery","Create a temporary table in Cloud Spanner that will act as a buffer for new data. Create a new step in your pipeline to write to this table first, and then create a new pipeline to write from Cloud Spanner to BigQuery",,"A,B",136,"With autoscaling enabled, the Dataflow service does not allow user control of the exact number of worker instances allocated to your job. You might still cap the number of workers by specifying the --max_num_workers option when you run your pipeline. Here as per question CAP is 3, So we can change that CAP. For batch jobs, the default machine type is n1-standard-1. For streaming jobs, the default machine type for Streaming Engine-enabled jobs is n1-standard-2 and the default machine type for non-Streaming Engine jobs is n1-standard-4. When using the default machine types, the Dataflow service can therefore allocate up to 4000 cores per job. If you need more cores for your job, you can select a larger machine type."
"You are building a new application that you need to collect data from in a scalable way. Data arrives continuously from the application throughout the day, and you expect to generate approximately 150 GB of JSON data per day by the end of the year. Your requirements are:
✑ Decoupling producer from consumer
✑ Space and cost-efficient storage of the raw ingested data, which is to be stored indefinitely
✑ Near real-time SQL query
✑ Maintain at least 2 years of historical data, which will be queried with SQL
Which pipeline should you use to meet these requirements?",Create an application that provides an API. Write a tool to poll the API and write data to Cloud Storage as gzipped JSON files.,Create an application that writes to a Cloud SQL database to store the data. Set up periodic exports of the database to write to Cloud Storage and load into BigQuery.,"Create an application that publishes events to Cloud Pub/Sub, and create Spark jobs on Cloud Dataproc to convert the JSON data to Avro format, stored on HDFS on Persistent Disk.","Create an application that publishes events to Cloud Pub/Sub, and create a Cloud Dataflow pipeline that transforms the JSON event payloads to Avro, writing the data to Cloud Storage and BigQuery.",,,D,135,"Decoupling Producer from Consumer: Cloud Pub/Sub provides a decoupled messaging system where the producer publishes events, and consumers (like Dataflow) can subscribe to these events. This decoupling ensures flexibility and scalability. Space and Cost-Efficient Storage: Storing data in Avro format is more space-efficient than JSON, and Cloud Storage is a cost-effective storage solution. Additionally, Cloud Pub/Sub and Dataflow allow you to process and transform data efficiently, reducing storage costs. Near Real-time SQL Query: By using Dataflow to transform and load data into BigQuery, you can achieve near real-time data availability for SQL queries. BigQuery is well-suited for ad-hoc SQL queries and provides excellent query performance."
"You are building an application to share financial market data with consumers, who will receive data feeds. Data is collected from the markets in real time.
Consumers will receive the data in the following ways:
✑ Real-time event stream
✑ ANSI SQL access to real-time stream and historical data
✑ Batch historical exports
Which solution should you use?","Cloud Dataflow, Cloud SQL, Cloud Spanner","Cloud Pub/Sub, Cloud Storage, BigQuery","Cloud Dataproc, Cloud Dataflow, BigQuery","Cloud Pub/Sub, Cloud Dataproc, Cloud SQL",,,B,134,"Here's how this solution aligns with your requirements: Real-time Event Stream: Cloud Pub/Sub is a managed messaging service that can handle real-time event streams efficiently. You can use Pub/Sub to ingest and publish real-time market data to consumers. ANSI SQL Access: BigQuery supports ANSI SQL queries, making it suitable for both real-time and historical data analysis. You can stream data into BigQuery tables from Pub/Sub and provide ANSI SQL access to consumers. Batch Historical Exports: Cloud Storage can be used for batch historical exports. You can export data from BigQuery to Cloud Storage in batch, making it available for consumers to download."
"A data scientist has created a BigQuery ML model and asks you to create an ML pipeline to serve predictions. You have a REST API application with the requirement to serve predictions for an individual user ID with latency under 100 milliseconds. You use the following query to generate predictions: SELECT predicted_label, user_id FROM ML.PREDICT (MODEL 'dataset.model', table user_features). How should you create the ML pipeline?","Add a WHERE clause to the query, and grant the BigQuery Data Viewer role to the application service account.",Create an Authorized View with the provided query. Share the dataset that contains the view with the application service account.,Create a Dataflow pipeline using BigQueryIO to read results from the query. Grant the Dataflow Worker role to the application service account.,Create a Dataflow pipeline using BigQueryIO to read predictions for all users from the query. Write the results to Bigtable using BigtableIO. Grant the Bigtable Reader role to the application service account so that the application can read predictions for individual users from Bigtable.,,,D,133,"The key requirements are serving predictions for individual user IDs with low (sub-100ms) latency. Option D meets this by batch predicting for all users in BigQuery ML, writing predictions to Bigtable for fast reads, and allowing the application access to query Bigtable directly for low latency reads. Since the application needs to serve low-latency predictions for individual user IDs, using Dataflow to batch predict for all users and write to Bigtable allows low-latency reads. Granting the Bigtable Reader role allows the application to retrieve predictions for a specific user ID from Bigtable."
"Your United States-based company has created an application for assessing and responding to user actions. The primary table's data volume grows by 250,000 records per second. Many third parties use your application's APIs to build the functionality into their own frontend applications. Your application's APIs should comply with the following requirements:
✑ Single global endpoint
✑ ANSI SQL support
✑ Consistent access to the most up-to-date data
What should you do?",Implement BigQuery with no region selected for storage or processing.,Implement Cloud Spanner with the leader in North America and read-only replicas in Asia and Europe.,Implement Cloud SQL for PostgreSQL with the master in North America and read replicas in Asia and Europe.,Implement Bigtable with the primary cluster in North America and secondary clusters in Asia and Europe.,,,B,132,A - BigQuery with NO Region ? (Looks wrong) B - Spanner (SQL support and Scalable and have replicas ) - Looks correct C - SQL (can't store so many records) (wrong) D - Bigtable - NO SQL (wrong)
"As your organization expands its usage of GCP, many teams have started to create their own projects. Projects are further multiplied to accommodate different stages of deployments and target audiences. Each project requires unique access control configurations. The central IT team needs to have access to all projects.
Furthermore, data from Cloud Storage buckets and BigQuery datasets must be shared for use in other projects in an ad hoc way. You want to simplify access control management by minimizing the number of policies. Which two steps should you take? (Choose two.)",Use Cloud Deployment Manager to automate access provision.,Introduce resource hierarchy to leverage access control policy inheritance.,"Create distinct groups for various teams, and specify groups in Cloud IAM policies.",Only use service accounts when sharing data for Cloud Storage buckets and BigQuery datasets.,"For each Cloud Storage bucket or BigQuery dataset, decide which projects need access. Find all the active members who have access to these projects, and create a Cloud IAM policy to grant access to all these users.",,"B,C",131,Google suggests that we should provide access by following google hierarchy and groups for users with similar roles
"The marketing team at your organization provides regular updates of a segment of your customer dataset. The marketing team has given you a CSV with 1 million records that must be updated in BigQuery. When you use the UPDATE statement in BigQuery, you receive a quotaExceeded error. What should you do?",Reduce the number of records updated each day to stay within the BigQuery UPDATE DML statement limit.,Increase the BigQuery UPDATE DML statement limit in the Quota management section of the Google Cloud Platform Console.,Split the source CSV file into smaller CSV files in Cloud Storage to reduce the number of BigQuery UPDATE DML statements per BigQuery job.,Import the new records from the CSV file into a new BigQuery table. Create a BigQuery job that merges the new records with the existing records and writes the results to a new BigQuery table.,,,D,130,https://cloud.google.com/blog/products/gcp/performing-large-scale-mutations-in-bigquery
"You use BigQuery as your centralized analytics platform. New data is loaded every day, and an ETL pipeline modifies the original data and prepares it for the final users. This ETL pipeline is regularly modified and can generate errors, but sometimes the errors are detected only after 2 weeks. You need to provide a method to recover from these errors, and your backups should be optimized for storage costs. How should you organize your data in BigQuery and store your backups?","Organize your data in a single table, export, and compress and store the BigQuery data in Cloud Storage.","Organize your data in separate tables for each month, and export, compress, and store the data in Cloud Storage.","Organize your data in separate tables for each month, and duplicate your data on a separate dataset in BigQuery.","Organize your data in separate tables for each month, and use snapshot decorators to restore the table to a time prior to the corruption.",,,B,129,"From : https://cloud.google.com/architecture/dr-scenarios-for-data#BigQuery It can't be D If the corruption is caught within 7 days, query the table to a point in time in the past to recover the table prior to the corruption using snapshot decorators. Store the original data on Cloud Storage. This allows you to create a new table and reload the uncorrupted data. From there, you can adjust your applications to point to the new table. => D"
"You deployed an application into a large Standard Google Kubernetes Engine (GKE) cluster. The application is stateless and multiple pods run at the same time. Your application receives inconsistent traffic. You need to ensure that the user experience remains consistent regardless of changes in traffic and that the resource usage of the cluster is optimized.

What should you do?",Configure a cron job to scale the deployment on a schedule,Configure a Horizontal Pod Autoscaler.,Configure a Vertical Pod Autoscaler,Configure cluster autoscaling on the node pool.,,,B,128,"The recommended approach to ensure a consistent user experience and optimize resource usage for a stateless application with inconsistent traffic in a large Standard Google Kubernetes Engine (GKE) cluster is (option B) Configure a Horizontal Pod Autoscaler (HPA). HPA automatically adjusts the number of replica pods based on observed CPU utilization or other custom metrics. In the context of varying traffic patterns, HPA dynamically scales the number of pods to meet demand, ensuring that there are enough instances to handle increased traffic and scaling down during periods of lower demand. This helps maintain consistent performance while optimizing resource utilization in response to changing workloads."
"The recommended approach to ensure a consistent user experience and optimize resource usage for a stateless application with inconsistent traffic in a large Standard Google Kubernetes Engine (GKE) cluster is (option B) Configure a Horizontal Pod Autoscaler (HPA). HPA automatically adjusts the number of replica pods based on observed CPU utilization or other custom metrics. In the context of varying traffic patterns, HPA dynamically scales the number of pods to meet demand, ensuring that there are enough instances to handle increased traffic and scaling down during periods of lower demand. This helps maintain consistent performance while optimizing resource utilization in response to changing workloads.",Use Cloud TPUs without any additional adjustment to your code.,Use Cloud TPUs after implementing GPU kernel support for your customs ops.,Use Cloud GPUs after implementing GPU kernel support for your customs ops.,"Stay on CPUs, and increase the size of the cluster you're training your model on.",,,C,127,The correct answer is C TPU does not support custom C++ tensorflow ops https://cloud.google.com/tpu/docs/tpus#when_to_use_tpus
You are migrating your 2 TB on-premises PostgreSQL cluster to Compute Engine. You want to set up your new environment in an Ubuntu virtual machine instance in Google Cloud and seed the data to a new instance. You need to plan your database migration to ensure minimum downtime. What should you do?,"1. Take a full export while the database is offline.
2. Create a bucket in Cloud Storage.
3. Transfer the dump file to the bucket you just created.
4. Import the dump file into the Google Cloud primary server.","1. Take a full export while the database is offline.
2. Create a bucket in Cloud Storage.
3. Transfer the dump file to the bucket you just created.
4. Restore the backup into the Google Cloud primary server.","1. Take a full backup while the database is online.
2. Create a bucket in Cloud Storage.
3. Transfer the backup to the bucket you just created.
4. Restore the backup into the Google Cloud primary server.
5. Create a recovery.conf file in the $PG_DATA directory.
6. Stop the source database.
7. Transfer the write ahead logs to the bucket you created before.
8. Start the PostgreSQL service.
9. Wait until Google Cloud primary server syncs with the running primary server.","1. Take a full export while the database is online.
2. Create a bucket in Cloud Storage.
3. Transfer the dump file and write-ahead logs to the bucket you just created.
4. Restore the dump file into the Google Cloud primary server.
5. Create a recovery.conf file in the $PG_DATA directory.
6. Stop the source database.
7. Transfer the write-ahead logs to the bucket you created before.
8. Start the PostgreSQL service.
9. Wait until the Google Cloud primary server syncs with the running primary server.",,,C,126,"1. Database remains online (see link below, and step 1, which says ""running master database"" 2. Backups are faster then exports (which generates new files) https://cloud.google.com/architecture/migrating-postgresql-to-gcp"
You have a petabyte of analytics data and need to design a storage and processing platform for it. You must be able to perform data warehouse-style analytics on the data in Google Cloud and expose the dataset as files for batch analysis tools in other cloud providers. What should you do?,Store and process the entire dataset in BigQuery.,Store and process the entire dataset in Bigtable.,"Store the full dataset in BigQuery, and store a compressed copy of the data in a Cloud Storage bucket.","Store the warm data as files in Cloud Storage, and store the active data in BigQuery. Keep this ratio as 80% warm and 20% active.",,,C,125,A and B can be eliminated right away as they do not talk about providing for other cloud providers. between C and D. The question says nothing about warm or cold data-rather that data should be made available for other providers--C--can fulfill this condition. Answer C.
"You are designing a cloud-native historical data processing system to meet the following conditions:
✑ The data being analyzed is in CSV, Avro, and PDF formats and will be accessed by multiple analysis tools including Dataproc, BigQuery, and Compute
Engine.
✑ A batch pipeline moves daily data.
✑ Performance is not a factor in the solution.
✑ The solution design should maximize availability.
How should you design data storage for this solution?","Create a Dataproc cluster with high availability. Store the data in HDFS, and perform analysis as needed.",Store the data in BigQuery. Access the data using the BigQuery Connector on Dataproc and Compute Engine.,"Store the data in a regional Cloud Storage bucket. Access the bucket directly using Dataproc, BigQuery, and Compute Engine.","Store the data in a multi-regional Cloud Storage bucket. Access the data directly using Dataproc, BigQuery, and Compute Engine.",,,D,124,"Considerations: High availability, performance not an issue

A → avoid HDFS
C → multi-regional > regional in terms of availability

B could be the answer but we’re dealing with PDF documents, we need blob storage (cloud storage). If we only have csv or Avro, this may be the answer"
"You need to create a data pipeline that copies time-series transaction data so that it can be queried from within BigQuery by your data science team for analysis.
Every hour, thousands of transactions are updated with a new status. The size of the initial dataset is 1.5 PB, and it will grow by 3 TB per day. The data is heavily structured, and your data science team will build machine learning models based on this data. You want to maximize performance and usability for your data science team. Which two strategies should you adopt? (Choose two.)",Denormalize the data as must as possible.,Preserve the structure of the data as much as possible.,Use BigQuery UPDATE to further reduce the size of the dataset.,Develop a data pipeline where status updates are appended to BigQuery instead of updated.,Copy a daily snapshot of transaction data to Cloud Storage and store it as an Avro file. Use BigQuery's support for external data sources to query.,,"A,D",123,"Denormalization will help in performance by reducing query time, update are not good with bigquery"
"You decided to use Cloud Datastore to ingest vehicle telemetry data in real time. You want to build a storage system that will account for the long-term data growth, while keeping the costs low. You also want to create snapshots of the data periodically, so that you can make a point-in-time (PIT) recovery, or clone a copy of the data for Cloud Datastore in a different environment. You want to archive these snapshots for a long time. Which two methods can accomplish this?
(Choose two.)","Use managed export, and store the data in a Cloud Storage bucket using Nearline or Coldline class.","Use managed export, and then import to Cloud Datastore in a separate project under a unique namespace reserved for that export.","Use managed export, and then import the data into a BigQuery table created just for that export, and delete temporary export files.","Write an application that uses Cloud Datastore client libraries to read all the entities. Treat each entity as a BigQuery table row via BigQuery streaming insert. Assign an export timestamp for each export, and attach it as an extra column for each row. Make sure that the BigQuery table is partitioned using the export timestamp column.",Write an application that uses Cloud Datastore client libraries to read all the entities. Format the exported data into a JSON file. Apply compression before storing the data in Cloud Source Repositories.,,"A,B",122,https://cloud.google.com/datastore/docs/export-import-entities#import-into-bigquery Data exported without specifying an entity filter cannot be loaded into BigQuery. This is not mentioned explicitly. Safe to assume there is no filter on the exports. So options are AB
"You currently have a single on-premises Kafka cluster in a data center in the us-east region that is responsible for ingesting messages from IoT devices globally.
Because large parts of globe have poor internet connectivity, messages sometimes batch at the edge, come in all at once, and cause a spike in load on your
Kafka cluster. This is becoming difficult to manage and prohibitively expensive. What is the Google-recommended cloud native architecture for this scenario?",Edge TPUs as sensor devices for storing and transmitting the messages.,Cloud Dataflow connected to the Kafka cluster to scale the processing of incoming messages.,"An IoT gateway connected to Cloud Pub/Sub, with Cloud Dataflow to read and process the messages from Cloud Pub/Sub.",A Kafka cluster virtualized on Compute Engine in us-east with Cloud Load Balancing to connect to the devices around the world.,,,C,121,"What is wrong with D, nothing, Cloud load balancing can shift traffic for high volume and low internet in one region. It cost avg. 0.01-0.25 $ per GB, or if volume is too high. 0.05 $ per Hour http request. This might be the answer if your exam for network engineer."
"You are operating a Cloud Dataflow streaming pipeline. The pipeline aggregates events from a Cloud Pub/Sub subscription source, within a window, and sinks the resulting aggregation to a Cloud Storage bucket. The source has consistent throughput. You want to monitor an alert on behavior of the pipeline with Cloud
Stackdriver to ensure that it is processing data. Which Stackdriver alerts should you create?",An alert based on a decrease of subscription/num_undelivered_messages for the source and a rate of change increase of instance/storage/ used_bytes for the destination,An alert based on an increase of subscription/num_undelivered_messages for the source and a rate of change decrease of instance/storage/ used_bytes for the destination,An alert based on a decrease of instance/storage/used_bytes for the source and a rate of change increase of subscription/ num_undelivered_messages for the destination,An alert based on an increase of instance/storage/used_bytes for the source and a rate of change decrease of subscription/ num_undelivered_messages for the destination,,,B,120,1) subscription/ num_undelivered_messages would pile up at a constant rate as the source has consistent throughput 2) instance/storage/ used_bytes will get closer to zero. Hence need to monitor it's rate of change
"You operate a database that stores stock trades and an application that retrieves average stock price for a given company over an adjustable window of time. The data is stored in Cloud Bigtable where the datetime of the stock trade is the beginning of the row key. Your application has thousands of concurrent users, and you notice that performance is starting to degrade as more stocks are added. What should you do to improve the performance of your application?",Change the row key syntax in your Cloud Bigtable table to begin with the stock symbol.,Change the row key syntax in your Cloud Bigtable table to begin with a random number per second.,"Change the data pipeline to use BigQuery for storing stock trades, and update your application.",Use Cloud Dataflow to write a summary of each day's stock trades to an Avro file on Cloud Storage. Update your application to read from Cloud Storage and Cloud Bigtable to compute the responses.,,,A,119,Timestamp at starting of rowkey causes bottleneck issues
"You are managing two different applications: Order Management and Sales Reporting. Both applications interact with the same Cloud SQL for MySQL database. The Order Management application reads and writes to the database 24/7, but the Sales Reporting application is read-only. Both applications need the latest data. You need to ensure that the Performance of the Order Management application is not affected by the Sales Reporting application. What should you do?",Create a read replica for the Sales Reporting application.,"Create two separate databases in the instance, and perform dual writes from the Order Management application.",Use a Cloud SQL federated query for the Sales Reporting application.,"Queue up all the requested reports in PubSub, and execute the reports at night.",,,A,118,Read replica is more than enough
You are designing a data processing pipeline. The pipeline must be able to scale automatically as load increases. Messages must be processed at least once and must be ordered within windows of 1 hour. How should you design the solution?,Use Apache Kafka for message ingestion and use Cloud Dataproc for streaming analysis.,Use Apache Kafka for message ingestion and use Cloud Dataflow for streaming analysis.,Use Cloud Pub/Sub for message ingestion and Cloud Dataproc for streaming analysis.,Use Cloud Pub/Sub for message ingestion and Cloud Dataflow for streaming analysis.,,,D,117,"Data proc is serverbased Dataflow is serverless which is used to run pipelines which uses apache framework in the background. Just need to mention the number of workers needed. so question saying we need scale automatically . so dataproc eliminate ho gaya now Dataflow is correct , pub/sub is recommended for this scenario."
Your organization is currently updating an existing corporate application that is running in another public cloud to access managed database services in Google Cloud. The application will remain in the other public cloud while the database is migrated to Google Cloud. You want to follow Google-recommended practices for authentication. You need to minimize user disruption during the migration. What should you do?,Use workload identity federation to impersonate a service account.,Ask existing users to set their Google password to match their corporate password.,"Migrate the application to Google Cloud, and use Identity and Access Management (IAM).",Use Google Workspace Password Sync to replicate passwords into Google Cloud.,,,A,116,"With identity federation, you can use Identity and Access Management (IAM) to grant external identities IAM roles, including the ability to impersonate service accounts. This lets you access resources directly, using a short-lived access token, and eliminates the maintenance and security burden associated with service account keys."
You use a dataset in BigQuery for analysis. You want to provide third-party companies with access to the same dataset. You need to keep the costs of data sharing low and ensure that the data is current. Which solution should you choose?,"Use Analytics Hub to control data access, and provide third party companies with access to the dataset.","Use Cloud Scheduler to export the data on a regular basis to Cloud Storage, and provide third-party companies with access to the bucket.","Create a separate dataset in BigQuery that contains the relevant data to share, and provide third-party companies with access to the new dataset.","Create a Dataflow job that reads the data in frequent time intervals, and writes it to the relevant BigQuery dataset or Cloud Storage bucket for third-party companies to use.",,,A,115,"https://cloud.google.com/bigquery/docs/analytics-hub-introduction nalytics Hub is a data exchange platform that enables you to share data and insights at scale across organizational boundaries with a robust security and privacy framework. As an Analytics Hub publisher, you can monetize data by sharing it with your partner network or within your own organization in real time. Listings let you share data without replicating the shared data. You can build a catalog of analytics-ready data sources with granular permissions that let you deliver data to the right audiences. You can also manage subscriptions to your listings."
Your company has a hybrid cloud initiative. You have a complex data pipeline that moves data between cloud provider services and leverages services from each of the cloud providers. Which cloud-native service should you use to orchestrate the entire pipeline?,Cloud Dataflow,Cloud Composer,Cloud Dataprep,Cloud Dataproc,,,B,114,No other option is aimed for this purpose
"You are a retailer that wants to integrate your online sales capabilities with different in-home assistants, such as Google Home. You need to interpret customer voice commands and issue an order to the backend systems. Which solutions should you choose?",Speech-to-Text API,Cloud Natural Language API,Dialogflow Enterprise Edition,AutoML Natural Language,,,C,113,"should be C, since we need to recognize both voice and intent"
"You operate a logistics company, and you want to improve event delivery reliability for vehicle-based sensors. You operate small data centers around the world to capture these events, but leased lines that provide connectivity from your event collection infrastructure to your event processing infrastructure are unreliable, with unpredictable latency. You want to address this issue in the most cost-effective way. What should you do?",Deploy small Kafka clusters in your data centers to buffer events.,Have the data acquisition devices publish data to Cloud Pub/Sub.,Establish a Cloud Interconnect between all remote data centers and Google.,Write a Cloud Dataflow pipeline that aggregates all data in session windows.,,,B,112,"Managed Service: Cloud Pub/Sub is a fully managed service, reducing the operational overhead compared to managing Kafka clusters. Reliability and Scalability: Cloud Pub/Sub can handle high volumes of data with low latency and provides built-in mechanisms for reliable message delivery, even in the face of intermittent connectivity. Cost-Effective: Cloud Pub/Sub offers a pay-as-you-go pricing model, which can be more cost-effective than setting up and maintaining dedicated network infrastructure like Cloud Interconnect. Global Availability: Cloud Pub/Sub is available globally and can handle data from multiple regions efficiently."
"You have historical data covering the last three years in BigQuery and a data pipeline that delivers new data to BigQuery daily. You have noticed that when the
Data Science team runs a query filtered on a date column and limited to 30`""90 days of data, the query scans the entire table. You also noticed that your bill is increasing more quickly than you expected. You want to resolve the issue as cost-effectively as possible while maintaining the ability to conduct SQL queries.
What should you do?",Re-create the tables using DDL. Partition the tables by a column containing a TIMESTAMP or DATE Type.,Recommend that the Data Science team export the table to a CSV file on Cloud Storage and use Cloud Datalab to explore the data by reading the files directly.,"Modify your pipeline to maintain the last 30ג€""90 days of data in one table and the longer history in a different table to minimize full table scans over the entire history.",Write an Apache Beam pipeline that creates a BigQuery table per day. Recommend that the Data Science team use wildcards on the table name suffixes to select the data they need.,,,A,111,Partition is the solution for reducing cost and time
"You are creating a new pipeline in Google Cloud to stream IoT data from Cloud Pub/Sub through Cloud Dataflow to BigQuery. While previewing the data, you notice that roughly 2% of the data appears to be corrupt. You need to modify the Cloud Dataflow pipeline to filter out this corrupt data. What should you do?",Add a SideInput that returns a Boolean if the element is corrupt.,Add a ParDo transform in Cloud Dataflow to discard corrupt elements.,Add a Partition transform in Cloud Dataflow to separate valid data from corrupt data.,Add a GroupByKey transform in Cloud Dataflow to group all of the valid data together and discard the rest.,,,B,110,ParDo is used to do transformation and create side output
You have an application that sends banking events to Bigtable cluster-a in us-east. You decide to add cluster-b in us-central1. Cluster-a replicates data to cluster-b. You need to ensure that Bigtable continues to accept read and write requests if one of the clusters becomes unavailable and that requests are routed automatically to the other cluster. What deployment strategy should you use?,Use the default app profile with single-cluster routing.,Use the default app profile with multi-cluster routing.,Create a custom app profile with multi-cluster routing.,Create a custom app profile with single-cluster routing.,,,C,109,"Even the default profile can be single-cluster or multi-cluster ""The settings in an instance's default app profile depend on the number of clusters the instance had when you first created it: If you created the instance with 1 cluster, the default app profile uses single-cluster routing, and it enables single-row transactions. This ensures that adding additional clusters later doesn't change the behavior of your existing applications."""
"You have developed three data processing jobs. One executes a Cloud Dataflow pipeline that transforms data uploaded to Cloud Storage and writes results to
BigQuery. The second ingests data from on-premises servers and uploads it to Cloud Storage. The third is a Cloud Dataflow pipeline that gets information from third-party data providers and uploads the information to Cloud Storage. You need to be able to schedule and monitor the execution of these three workflows and manually execute them when needed. What should you do?",Create a Direct Acyclic Graph in Cloud Composer to schedule and monitor the jobs.,Use Stackdriver Monitoring and set up an alert with a Webhook notification to trigger the jobs.,Develop an App Engine application to schedule and request the status of the jobs using GCP API calls.,Set up cron jobs in a Compute Engine instance to schedule and monitor the pipelines using GCP API calls.,,,A,108,Cloud composer's DAG would manage the dependencies
"You work for a shipping company that uses handheld scanners to read shipping labels. Your company has strict data privacy standards that require scanners to only transmit tracking numbers when events are sent to Kafka topics. A recent software update caused the scanners to accidentally transmit recipients' personally identifiable information (PII) to analytics systems, which violates user privacy rules. You want to quickly build a scalable solution using cloud-native managed services to prevent exposure of PII to the analytics systems. What should you do?",Create an authorized view in BigQuery to restrict access to tables with sensitive data.,Install a third-party data validation tool on Compute Engine virtual machines to check the incoming data for sensitive information.,Use Cloud Logging to analyze the data passed through the total pipeline to identify transactions that may contain sensitive information.,Build a Cloud Function that reads the topics and makes a call to the Cloud Data Loss Prevention (Cloud DLP) API. Use the tagging and confidence levels to either pass or quarantine the data in a bucket for review.,,,D,107,The cloud function with DLP seems the best option
"You are managing a Cloud Dataproc cluster. You need to make a job run faster while minimizing costs, without losing work in progress on your clusters. What should you do?",Increase the cluster size with more non-preemptible workers.,"Increase the cluster size with preemptible worker nodes, and configure them to forcefully decommission.","Increase the cluster size with preemptible worker nodes, and use Cloud Stackdriver to trigger a script to preserve work.","Increase the cluster size with preemptible worker nodes, and configure them to use graceful decommissioning.",,,D,106,All your workers need to be the same kind. Use Graceful Decommissioning for don't lose any data and add more(increase the cluster) preemptible workers because there are more cost-effective .
"You want to automate execution of a multi-step data pipeline running on Google Cloud. The pipeline includes Dataproc and Dataflow jobs that have multiple dependencies on each other. You want to use managed services where possible, and the pipeline will run every day. Which tool should you use?",cron,Cloud Composer,Cloud Scheduler,Workflow Templates on Dataproc,,,B,105,"Cloud Composer is a managed service that allows you to create and run Apache Airflow workflows. Airflow is a workflow management platform that can be used to automate complex data pipelines. It is a good choice for this use case because it is a managed service, which means that Google will take care of the underlying infrastructure. It also supports multiple dependencies, so you can easily schedule a multi-step pipeline"
"Your organization needs to migrate a critical, on-premises MySQL database to Cloud SQL for MySQL. The on-premises database is on a version of MySQL that is supported by Cloud SQL and uses the InnoDB storage engine. You need to migrate the database while preserving transactions and minimizing downtime. What should you do?","1. Use Database Migration Service to connect to your on-premises database, and choose continuous replication.
2. After the on-premises database is migrated, promote the Cloud SQL for MySQL instance, and connect applications to your Cloud SQL instance","1. Build a Cloud Data Fusion pipeline for each table to migrate data from the on-premises MySQL database to Cloud SQL for MySQL.
2. Schedule downtime to run each Cloud Data Fusion pipeline.
3. Verify that the migration was successful.
4. Re-point the applications to the Cloud SQL for MySQL instance.","1. Pause the on-premises applications.
2. Use the mysqldump utility to dump the database content in compressed format.
3. Run gsutil –m to move the dump file to Cloud Storage.
4. Use the Cloud SQL for MySQL import option.
5. After the import operation is complete, re-point the applications to the Cloud SQL for MySQL instance.","1 Pause the on-premises applications.
2. Use the mysqldump utility to dump the database content in CSV format.
3. Run gsutil –m to move the dump file to Cloud Storage.
4. Use the Cloud SQL for MySQL import option.
5. After the import operation is complete, re-point the applications to the Cloud SQL for MySQL instance.",,,A,104,Database Migration Service supports one-time and continuous migrations from source databases to Cloud SQL destination databases.
"You have a data stored in BigQuery. The data in the BigQuery dataset must be highly available. You need to define a storage, backup, and recovery strategy of this data that minimizes cost. How should you configure the BigQuery table that have a recovery point objective (RPO) of 30 days?","Set the BigQuery dataset to be regional. In the event of an emergency, use a point-in-time snapshot to recover the data.","Set the BigQuery dataset to be regional. Create a scheduled query to make copies of the data to tables suffixed with the time of the backup. In the event of an emergency, use the backup copy of the table.","Set the BigQuery dataset to be multi-regional. In the event of an emergency, use a point-in-time snapshot to recover the data.","Set the BigQuery dataset to be multi-regional. Create a scheduled query to make copies of the data to tables suffixed with the time of the backup. In the event of an emergency, use the backup copy of the table.",,,C,103,1. HA -> Multi-region 2. DR -> Snapshot
You need to create a near real-time inventory dashboard that reads the main inventory tables in your BigQuery data warehouse. Historical inventory data is stored as inventory balances by item and location. You have several thousand updates to inventory every hour. You want to maximize performance of the dashboard and ensure that the data is accurate. What should you do?,Leverage BigQuery UPDATE statements to update the inventory balances as they are changing.,Partition the inventory balance table by item to reduce the amount of data scanned with each inventory update.,Use the BigQuery streaming the stream changes into a daily inventory movement table. Calculate balances in a view that joins it to the historical inventory balance table. Update the inventory balance table nightly.,Use the BigQuery bulk loader to batch load inventory changes into a daily inventory movement table. Calculate balances in a view that joins it to the historical inventory balance table. Update the inventory balance table nightly.,,,C,102,"The best approach is to use BigQuery streaming to stream the inventory changes into a daily inventory movement table. Then calculate balances in a view that joins the inventory movement table to the historical inventory balance table. Finally, update the inventory balance table nightly (option C)."
"A data engineer has realized that the data files associated with a Delta table are incredibly small. They want to compact the small files to form larger files to improve performance.

Which keyword can be used to compact the small files?",OPTIMIZE,VACUUM,COMPACTION,REPARTITION,,,A,101,"The OPTIMIZE command is used to compact small files into larger ones, which helps improve the performance of Delta Lake tables. It consolidates small files into fewer larger files to reduce the overhead associated with having many small files. This process is often referred to as ""compaction"" but the specific keyword in Databricks Delta Lake is OPTIMIZE."
"You have a requirement to insert minute-resolution data from 50,000 sensors into a BigQuery table. You expect significant growth in data volume and need the data to be available within 1 minute of ingestion for real-time analysis of aggregated trends. What should you do?",Use bq load to load a batch of sensor data every 60 seconds.,Use a Cloud Dataflow pipeline to stream data into the BigQuery table.,Use the INSERT statement to insert a batch of data every 60 seconds.,Use the MERGE statement to apply updates in batch every 60 seconds.,,,B,100,"“need the data to be available within 1 minute of ingestion for real-time analysis” → low latency requirement → Dataflow streaming The database can either be BQ or BigTable for this kind of requirement in data volume and latency. But it mentioned that the destination has to be BQ, so B."
